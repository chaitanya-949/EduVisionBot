{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publication date of InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output: Thu, 4 Jul 2024 (showing first 1 of 115 entries )\n",
      "Title: InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output\n",
      "Authors of InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output: Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, Jiaqi Wang\n",
      "\n",
      "InternLM-XComposer-2.5: A Versatile Large Vision Language Model\n",
      "Supporting Long-Contextual Input and Output\n",
      "Pan Zhang∗1, Xiaoyi Dong∗1,2, Yuhang Zang∗1, Yuhang Cao1, Rui Qian1,2, Lin Chen1, Qipeng Guo1,\n",
      "Haodong Duan1, Bin Wang1, Linke Ouyang1, Songyang Zhang1, Wenwei Zhang1, Yining Li1,\n",
      "Yang Gao1, Peng Sun1, Xinyue Zhang1, Wei Li1, Jingwen Li1, Wenhai Wang1,2, Hang Yan1,\n",
      "Conghui He3, Xingcheng Zhang3, Kai Chen1, Jifeng Dai4,1, Yu Qiao1, Dahua Lin1,2, Jiaqi Wang1,B\n",
      "1Shanghai Artificial Intelligence Laboratory, 2The Chinese University of Hong Kong,\n",
      "3SenseTime Group, 4Tsinghua University\n",
      "internlm@pjlab.org.cn\n",
      "Abstract\n",
      "We present InternLM-XComposer-2.5 (IXC-2.5), a ver-\n",
      "satile large-vision language model that supports long-\n",
      "contextual input and output.\n",
      "IXC-2.5 excels in various\n",
      "text-image comprehension and composition applications,\n",
      "achieving GPT-4V level capabilities with merely 7B LLM\n",
      "backend. Trained with 24K interleaved image-text contexts,\n",
      "it can seamlessly extend to 96K long contexts via RoPE ex-\n",
      "trapolation.\n",
      "This long-context capability allows IXC-2.5\n",
      "to excel in tasks requiring extensive input and output con-\n",
      "texts.\n",
      "Compared to its previous 2.0 version, InternLM-\n",
      "XComposer-2.5 features three major upgrades in vision-\n",
      "language comprehension: (1) Ultra-High Resolution Un-\n",
      "derstanding, (2) Fine-Grained Video Understanding, and\n",
      "(3) Multi-Turn Multi-Image Dialogue. In addition to com-\n",
      "prehension, IXC-2.5 extends to two compelling applications\n",
      "using extra LoRA parameters for text-image composition:\n",
      "(1) Crafting Webpages and (2) Composing High-Quality\n",
      "Text-Image Articles.\n",
      "IXC-2.5 has been evaluated on 28\n",
      "benchmarks, outperforming existing open-source state-of-\n",
      "the-art models on 16 benchmarks.\n",
      "It also surpasses or\n",
      "competes closely with GPT-4V and Gemini Pro on 16 key\n",
      "tasks. The InternLM-XComposer-2.5 is publicly available\n",
      "at https://github.com/InternLM/InternLM-\n",
      "XComposer.\n",
      "1. Introduction\n",
      "Recent\n",
      "advancements\n",
      "in\n",
      "Large\n",
      "Language\n",
      "Models\n",
      "(LLMs) [29, 55, 111, 121, 146, 147] have sparked in-\n",
      "terest in the development of Large Vision Language\n",
      "Models (LVLMs) [31, 41, 84, 112, 173, 183].\n",
      "Leading\n",
      "* equal contribution. B corresponding author.\n",
      "Figure 1. Overview of InternLM-XComposer-2.5 (IXC-2.5) per-\n",
      "formance on benchmarks in different domains, including Video\n",
      "Benchmarks, Structural High-resolution Benchmarks, Gen-\n",
      "eral Visual QA Benchmarks, Multi-True Multi-Image Bench-\n",
      "mark, and Webpage Crafting Benchmark. IXC-2.5 based on\n",
      "InternLM2-7B [143] matches or even surpasses GPT-4V [112]\n",
      "and Gemini Pro [142] in 15 benchmarks. Please refer to Ta-\n",
      "ble 3, 4, 5 for details.\n",
      "paradigms like GPT-4 [112], Gemini Pro 1.5 [41], and\n",
      "Claude 3 [3] have achieved considerable success and\n",
      "significantly expanded the range of applications for LLMs.\n",
      "Open-source LVLMs are also being rapidly developed and\n",
      "can compete with proprietary APIs in several benchmarks.\n",
      "However, these open-source models still lag behind closed-\n",
      "source leading paradigms in versatility.\n",
      "They lack the\n",
      "1\n",
      "arXiv:2407.03320v1  [cs.CV]  3 Jul 2024\n",
      "ability to perform diverse vision-language comprehension\n",
      "and composition tasks, largely due to limited diversity in\n",
      "training corpus and challenges in managing long-context\n",
      "input and output.\n",
      "To further bridge the gap between proprietary APIs [41,\n",
      "112] and open-sourced Large Vision Language Models, we\n",
      "are introducing InternLM-XComposer-2.5 (IXC-2.5), a ver-\n",
      "satile LVLM supporting long-contextual input and output\n",
      "with diverse comprehension and composition capacities.\n",
      "IXC-2.5 excels in existing open-sourced LVLMs with two\n",
      "advantages. (1) Versatility: IXC-2.5 supports a wide range\n",
      "of tasks related to comprehension and composition, such\n",
      "as free-form text-image conversation, OCR, video under-\n",
      "standing, article composition with illustrations, and web-\n",
      "page crafting. (2) Long-context capabilities in both in-\n",
      "put and output: It is natively trained with 24K interleaved\n",
      "image-text data, whose context window can be extended to\n",
      "96K through positional encoding extrapolation [94], em-\n",
      "powering the long-term human-AI interaction and content\n",
      "creation.\n",
      "Benefiting from the long contextual capability, compared\n",
      "to its previous 2.0 version [33], IXC-2.5 has upgraded three\n",
      "comprehension abilities: (1) Ultra-High Resolution Un-\n",
      "derstanding: IXC-2.5 enhances the dynamic resolution so-\n",
      "lution proposed in IXC2-4KHD [34] with a native 560 ×\n",
      "560 ViT vision encoder, supporting high-resolution images\n",
      "with any aspect ratio.\n",
      "(2) Fine-Grained Video Under-\n",
      "standing: IXC-2.5 treats videos as a ultra-high-resolution\n",
      "composite picture consisting of tens to hundreds of frames,\n",
      "allowing it to capture fine details through dense sampling\n",
      "and higher resolution for each frame.\n",
      "(3) Multi-Turn\n",
      "Multi-Image Dialogue: IXC-2.5 supports free-form multi-\n",
      "turn multi-image dialogue, allowing it to naturally interact\n",
      "with humans in multi-round conversations.\n",
      "Besides comprehension, IXC-2.5 also supports two no-\n",
      "table applications by incorporating extra LoRA parame-\n",
      "ters for text-image composition: (1) Crafting Webpages:\n",
      "IXC-2.5 can be readily applied to create webpages by com-\n",
      "posing source code (HTML, CSS, and JavaScript) follow-\n",
      "ing text-image instructions. (2) Composing High-Quality\n",
      "Text-Image Articles: Compared to IXC-2, IXC-2.5 lever-\n",
      "ages specially designed Chain-of-Thought (CoT) [153] and\n",
      "Direct Preference Optimization (DPO) [124] techniques to\n",
      "significantly enhance the quality of its written content.\n",
      "We evaluated the versatility of InternLM-XComposer-\n",
      "2.5 (IXC-2.5) across a range of twenty-eight benchmarks,\n",
      "including five video benchmarks [38, 42, 71, 88, 181],\n",
      "nine structural high-resolution benchmarks [20, 89, 106–\n",
      "108, 117, 133, 139, 140], twelve general VQA bench-\n",
      "marks [18, 40, 44, 61, 66, 87, 100, 155, 164, 166], one\n",
      "multi-true multi-image benchmark [92], and one webpage\n",
      "crafting benchmark [131].\n",
      "Compared to previous open-\n",
      "source LVLMs, IXC-2.5 achieved state-of-the-art results in\n",
      "16 out of 28 benchmarks based on InternLM2-7B [143]\n",
      "backend. As shown in Figure 1, the performance of IXC-\n",
      "2.5 matches or even surpasses proprietary APIs, e.g., GPT-\n",
      "4V [112] and Gemini Pro [41], in 16 benchmarks.\n",
      "IXC-2.5 web demo now supports audio input and\n",
      "output using open-source tools [123, 179].\n",
      "You may\n",
      "try it at https :/ /huggingface .co / spaces/\n",
      "Willow123/InternLM-XComposer.\n",
      "2. Related Works\n",
      "LVLMs for Text-Image Conversation. Large Language\n",
      "Models (LLMs) [8, 12, 13, 29, 55, 60, 111, 115, 121, 143,\n",
      "146, 147, 168] have received considerable attention be-\n",
      "cause of their impressive performance in language com-\n",
      "prehension and generation.\n",
      "Large vision-language mod-\n",
      "els (LVLMs) [5, 9, 22–24, 31, 34, 35, 41, 68, 78, 112,\n",
      "118, 159, 173, 183] have been developed by integrating\n",
      "LLMs with vision encoders [6, 14, 17, 25, 26, 33, 79, 91,\n",
      "93, 113, 122, 138, 150, 167, 169, 170, 176] to extend the\n",
      "ability to understand vision content, enabling the applica-\n",
      "tion of text-image conversation. Most existing LVLMs are\n",
      "trained for single-image multi-round conversations, while\n",
      "some works [2, 6, 56, 78, 136, 178] have the ability to\n",
      "understand multi-image inputs. However, IXC-2.5 focuses\n",
      "on providing a free-form long-contextual multi-turn multi-\n",
      "image interaction experience [86, 92, 103], which has not\n",
      "been addressed yet.\n",
      "LVLMs for High Resolution Images Analysis. Under-\n",
      "standing high-resolution images has significant potential\n",
      "applications such as OCR and document/chart analysis,\n",
      "which is attracting increased attention in the LVLMs area.\n",
      "In recent works, there are two main strategies to enable\n",
      "high-resolution understanding: (1) High-resolution (HR)\n",
      "visual encoders [47, 74, 102, 141, 151, 177] directly sup-\n",
      "port higher resolution images. (2) Patchification: A high-\n",
      "resolution image is cropped into patches [34, 49, 67, 76, 79,\n",
      "79, 85, 90, 157, 158, 158]. Each patch is processed with a\n",
      "low resolution vision encoder, e.g., CLIP [122] and visual\n",
      "embeddings of patches are further concatenated as inputs\n",
      "for LLM backends. IXC-4KHD [34] scales the supported\n",
      "resolution of open-source LVLMs into 4K and beyond for\n",
      "the first time. IXC-2.5 combines both solutions with a vi-\n",
      "sion encoder trained with a resolution of 560x560 and a\n",
      "dynamic resolution solution proposed in IXC2-4KHD [34],\n",
      "resulting in further improvements.\n",
      "LVLMs for Video Understanding. In addition to image\n",
      "understanding, the LVLMs area has also witnessed emerg-\n",
      "ing efforts in video analysis [37, 39, 70, 88, 110, 134, 135].\n",
      "To handle complex video inputs, existing works use sparse\n",
      "sampling or temporal pooling [52, 77, 101, 104, 162], com-\n",
      "pressed video tokens [57, 69, 73, 120, 154, 172], memory\n",
      "banks [45, 134, 135], and language as a bridge [54, 59, 171]\n",
      "for video understanding. Apart from these video-specific\n",
      "2\n",
      "Chat on 4K-resolution Image\n",
      "Analyze this image in detail\n",
      "The infographic is a visual representation of various facts \n",
      "about Dubai. It begins with a statement about Palm \n",
      "Jumeirah, highlighting it as the largest artificial island \n",
      "visible from space. It then provides a historical context, \n",
      "noting that in 1968, there were only a few cars in Dubai, \n",
      "contrasting this with the current figure of more than 1.5 \n",
      "million vehicles. The infographic also points out that \n",
      "Dubai has the world's largest Gold Chain, with 7 of the \n",
      "top 10 tallest hotels located there. \n",
      "Additionally, it mentions that the crime rate is near 0%, \n",
      "and the income tax rate is also 0%, with 20% of the \n",
      "world's total cranes operating in Dubai. \n",
      "Furthermore, it states that 17% of the population is \n",
      "Emirati, and 83% are immigrants. The Dubai Mall is \n",
      "highlighted as the largest shopping mall in the world, with \n",
      "1200 stores. The infographic also notes that Dubai has no \n",
      "standard address system, with no zip codes, area codes, or \n",
      "postal services. It mentions that the Burj Khalifa is so tall \n",
      "that its residents on top floors need to wait longer to break \n",
      "fast during Ramadan. \n",
      "The infographic also includes information about Dubai's \n",
      "climate-controlled City, with the Royal Suite at Burj Al \n",
      "Arab costing $24,000 per night. Lastly, it notes that the \n",
      "net worth of the four listed billionaires is roughly equal to \n",
      "the GDP of Honduras.\n",
      "The HD image (1200 x 4679)\n",
      "Multi-Turn Multi-Image Dialogue\n",
      "I want to buy a car from the three given cars, analyze their advantages and weaknesses one by one\n",
      "When considering the purchase of a car from the three options provided, it's essential to evaluate each vehicle based on its advantages \n",
      "and weaknesses. The first image shows a Mercedes-Benz SUV, which is known for its luxury, comfort, and reliability. Its strengths \n",
      "include spacious interiors, advanced safety features, and a reputation for durability. However, it may not offer the same performance as \n",
      "the other two vehicles, especially in terms of speed and handling.\n",
      "The second image depicts a Bugatti Veyron, an iconic supercar renowned for its power, speed, and engineering prowess. Its advantages \n",
      "lie in its exceptional performance capabilities, with a top speed exceeding 250 mph and a powerful engine that can deliver over 1,100 \n",
      "horsepower. However, this comes at a significant price, both financially and in terms of maintenance costs. Additionally, while the \n",
      "Veyron offers unparalleled driving experience, it might be less practical for everyday use due to its limited seating capacity and high \n",
      "fuel consumption.\n",
      "The third image shows a BMW i8, a hybrid sports car that combines electric propulsion with a combustion engine. This vehicle's \n",
      "benefits include lower emissions, efficient energy usage, and impressive acceleration times. It also boasts a stylish design and advanced \n",
      "technology features. However, one potential weakness could be the relatively short range compared to fully electric cars, requiring \n",
      "frequent charging stops if used extensively for long-distance travel.\n",
      "How about this car\n",
      "The car in the image is a red Ferrari, which is known for its high performance and luxury. This particular model appears to be a modern \n",
      "iteration of the Ferrari 812 Superfast or a similar high-performance vehicle from the brand. The advantages of this car include its powerful \n",
      "engine, likely delivering over 800 horsepower, making it one of the fastest cars on the road. Its aerodynamic design suggests excellent \n",
      "handling and stability at high speeds. Additionally, as a Ferrari, it comes with a reputation for superior craftsmanship and quality.\n",
      "However, there are also some potential weaknesses to consider. One might argue that the high price tag associated with owning such a \n",
      "vehicle could be a deterrent for many buyers. Maintenance costs can also be quite significant due to the complex nature of Ferrari engines \n",
      "and the specialized parts required. Furthermore, while the car's performance is unmatched by most standards, it may not offer the same \n",
      "level of comfort and practicality as more conventional vehicles, especially when considering long-distance travel or daily commuting.\n",
      "Figure 2. The capabilities of IXC-2.5: Chat on 4K-resolution Image and Multi-Turn Multi-Image Dialogue.\n",
      "3\n",
      "Chat on Video\n",
      "What is the most likely use of this video? \n",
      "The most likely use of this video is as an advertisement for Starbucks. The sequence of events depicted in the video, such \n",
      "as the preparation of a coffee drink, the interaction between customers and baristas, and the closing shot of the Starbucks \n",
      "logo, are characteristic of a promotional piece designed to showcase the brand's products and services. The emphasis on \n",
      "the coffee-making process, the use of the Starbucks app, and the friendly interactions between customers and staff all serve \n",
      "to create a positive image of the brand. Additionally, the final shot of the Starbucks logo with the text \"IT STARTS WITH \n",
      "YOU\" suggests a call to action for viewers to engage with the brand, further supporting the idea that this video is an \n",
      "advertisement aimed at attracting new customers and encouraging existing ones to continue patronizing Starbucks.\n",
      "frames\n",
      "Resume-to-Homepage\n",
      "Generate Code and \n",
      "Render\n",
      "Resume\n",
      "Homepage\n",
      "Some Crop Regions\n",
      "Instruction-aware Webpage Generation\n",
      "Generate Code and \n",
      "Render\n",
      "Generate a website for Research institutions. \n",
      "The name is Shanghai AI lab. Top \n",
      "Navigation Bar is blue. Below left, an \n",
      "image shows the logo of the lab. In the \n",
      "right, there is a passage of text below that \n",
      "describes the mission of the laboratory. \n",
      "There are several images to show the \n",
      "research projects of Shanghai AI lab.\n",
      "Instruction\n",
      "Figure 3. The capabilities of IXC-2.5: Chat on Video, Resume-to-Homepage and Instruction-aware Webpage Generation.\n",
      "4\n",
      "Screenshot-to-Code\n",
      "Generate Code and \n",
      "Render\n",
      "Article Composing\n",
      "Impact and Measures Regarding Global Climate Change\n",
      "The most likely use of this video is as an advertisement for Starbucks. The sequence of events depicted in the video, such as the preparation of a coffee \n",
      "drink, the interaction between customers and baristas, and the closing shot of the Starbucks logo, are characteristic of a promotional piece designed to \n",
      "showcase the brand's products and services. The emphasis on the coffee-making process, the use of the Starbucks app, and the friendly interactions between \n",
      "customers and staff all serve to create a positive image of the brand. Additionally, the final shot of the Starbucks logo with the text \"IT STARTS WITH \n",
      "YOU\" suggests a call to action for viewers to engage with the brand, further supporting the idea that this video is an advertisement aimed at attracting new \n",
      "customers and encouraging existing ones to continue patronizing Starbucks.\n",
      "Generate an article about  the Impact and Measures Regarding Global Climate Change. The outline is as follows: \n",
      "1.\n",
      "Describe the impact of climate change on the natural environment, including rising sea levels and an increase in extreme weather events.\n",
      "2.\n",
      "Analyze the effects of climate change on industries such as agriculture and tourism.\n",
      "3.\n",
      "Explore international efforts to address climate change, such as the Paris Climate Agreement.\n",
      "4.\n",
      "Discuss actions that individuals and communities can take at a personal level.\n",
      "Impact of Climate Change on the Environment\n",
      "One of the most visible impacts of climate change is the rise in sea levels. As temperatures increase, \n",
      "glaciers and ice caps melt, contributing to a rise in sea level that threatens coastal communities around the \n",
      "world. This can lead to saltwater intrusion into freshwater supplies, affecting agriculture and drinking \n",
      "water quality. Additionally, increased precipitation can cause flooding, while droughts can result in crop \n",
      "failure and food shortages.\n",
      "Another significant impact of climate change is the alteration of ecosystems. As temperatures rise, plant \n",
      "and animal species are forced to adapt to new conditions or face extinction. This can lead to a loss of \n",
      "biodiversity and the collapse of entire ecosystems. The warming of oceans also poses a threat to marine \n",
      "life, including coral reefs, which serve as vital habitats for many ocean species.\n",
      "Impact of Climate Change on Industry\n",
      "Climate change has a profound impact on various industries, particularly those dependent on natural \n",
      "resources. Agriculture, for example, is highly vulnerable to changes in temperature and precipitation \n",
      "patterns. Droughts can reduce crop yields, while floods can destroy crops and livestock. This can lead to \n",
      "food shortages and higher food prices, affecting both producers and consumers.\n",
      "The tourism industry is another sector significantly impacted by climate change. Extreme weather events, such as hurricanes and typhoons, can disrupt travel \n",
      "plans and damage infrastructure, leading to a loss of revenue for businesses and jobs. Furthermore, the warming of oceans and the melting of glaciers can \n",
      "alter the aesthetic appeal of beach destinations, reducing their attractiveness to tourists.\n",
      "International Efforts to Address Climate Change\n",
      "In response to the global challenge posed by climate change, countries have come together to form international agreements aimed at mitigating its impact. \n",
      "The Paris Climate Agreement, signed by 195 countries in 2015, is a landmark effort to address climate change. The agreement sets a goal of limiting global \n",
      "warming to well below 2 degrees Celsius above pre-industrial levels and pursuing efforts to limit the temperature increase to 1.5 degrees Celsius.\n",
      "Personal Level Actions\n",
      "While international efforts are crucial in addressing climate change, individuals and communities can also take actions at a personal level. Some measures \n",
      "include:\n",
      "- Reducing energy consumption: This can be achieved by using energy-efficient appliances, turning off lights when not in use, and reducing overall energy \n",
      "usage.\n",
      "- Using alternative transportation: Walking, cycling, carpooling, and using public transportation can help reduce greenhouse gas emissions from vehicles.\n",
      "- Reducing waste: Properly recycling and composting waste can help reduce methane emissions from landfills.\n",
      "- Supporting renewable energy: Investing in solar panels, wind turbines, and other forms of renewable energy can help reduce reliance on fossil fuels.\n",
      "- Planting trees: Trees absorb carbon dioxide and produce oxygen, making them a valuable tool in the fight against climate change.\n",
      "In conclusion, climate change poses a significant threat to the natural environment, industry, and human populations. However, through international efforts \n",
      "and personal actions, it is possible to mitigate its impact and create a more sustainable future.\n",
      "The Paris Agreement recognizes the importance of reducing greenhouse gas \n",
      "emissions and increasing resilience to the impacts of climate change. \n",
      "Countries are required to submit nationally determined contributions \n",
      "(NDCs), outlining their efforts to reduce emissions and adapt to the impacts \n",
      "of climate change. While the implementation of these measures is voluntary, \n",
      "countries are encouraged to increase their ambition over time to meet the \n",
      "goals of the agreement.\n",
      "Figure 4. The capabilities of IXC-2.5: Screenshot-to-Code and Article Composing.\n",
      "5\n",
      "designs, video analysis can also be formulated to under-\n",
      "stand a high-resolution composite picture consisting of\n",
      "sampled video frames [63, 156, 174].\n",
      "Benefiting from\n",
      "the ability to comprehend ultra-high-resolution images and\n",
      "long context, IXC-2.5 exhibits strong performance on vari-\n",
      "ous video benchmarks for LVLMs.\n",
      "Webpage Generation.\n",
      "Pix2Code [10] presents an end-\n",
      "to-end solution for UI-to-code transformation leveraging\n",
      "CNNs and RNNs. This approach contends with the chal-\n",
      "lenges posed by intricate visual encoding and extensive text\n",
      "decoding when applied to real-world UIs. In the sphere of\n",
      "recent advancements, works such as Sightseer [64], DC-\n",
      "Gen [148], and Design2Code [131] have employed large\n",
      "vision-language models trained on synthetic screenshot-\n",
      "HTML paired datasets like WebSight v0.1 or v0.2 [64] to\n",
      "facilitate HTML code generation. Nevertheless, the syn-\n",
      "thesized web page datasets have been critiqued for their\n",
      "simplicity and lack of diversity. These studies generally\n",
      "concentrate on the screenshot/sketch-to-code task. In con-\n",
      "trast, our IXC-2.5 model extends these capabilities to in-\n",
      "clude screenshot-to-code, instruction-aware webpage gen-\n",
      "eration, and resume-to-homepage tasks. IXC-2.5 is trained\n",
      "using a combination of high-quality synthesized and real-\n",
      "world web data. Furthermore, IXC-2.5 is proficient in gen-\n",
      "erating JavaScript code, thereby enabling the development\n",
      "of interactive front-end webpages.\n",
      "Preference Alignment. Reinforcement Learning from Hu-\n",
      "man Feedback (RLHF) [115] and Reinforcement Learning\n",
      "from AI Feedback (RLAIF) [7] have shown great promise\n",
      "in aligning LLMs across various domains, including im-\n",
      "proving logical reasoning and generating helpful and harm-\n",
      "less outputs. The typical approach involves training a re-\n",
      "ward model using human or AI preference data and fine-\n",
      "tuning the LLM to maximize the expected reward function\n",
      "with optimization algorithms like Proximal Policy Opti-\n",
      "mization (PPO) [126]. Alternatively, Direct Preference Op-\n",
      "timization (DPO) [124] and the following works [36, 116]\n",
      "have emerged as leading methods that implicitly repre-\n",
      "sent the reward score and eliminate the need for a sepa-\n",
      "rate reward model. Building on the success of RLHF and\n",
      "RLAIF in LLMs, recent studies have successfully extended\n",
      "RLHF/RLAIF algorithms for multimodal LVLMs [72, 119,\n",
      "163, 180, 182] to reduce hallucination. In this work, we in-\n",
      "vestigate the application of preference alignment techniques\n",
      "to the text-image article composition task, with a focus on\n",
      "generating high-quality and stable response results.\n",
      "3. Method\n",
      "3.1. Model Architecture\n",
      "The model architecture of InternLM-XComposer-2.5 (IXC-\n",
      "2.5 in the following for simplicity) mainly follows the\n",
      "design of InternLM-XComposer2 [33] and InternLM-\n",
      "Encode\n",
      "& Merge\n",
      "Encode\n",
      "& Merge\n",
      "Flatten\n",
      "Resize\n",
      "\\n\n",
      "\\n\n",
      "\\n\n",
      "\\n\n",
      "\\n\n",
      "\\n\n",
      "LLM\n",
      "PLoRA\n",
      "Pease describe the image / video …………\n",
      "Tokenize\n",
      "<IMAGE N>\n",
      "<IMAGE 3>\n",
      "<IMAGE 2>\n",
      "<IMAGE1>\n",
      "<IMAGE N>\n",
      "<IMAGE 3>\n",
      "<IMAGE 2>\n",
      "<IMAGE1>\n",
      "<IMAGE N>\n",
      "<IMAGE 3>\n",
      "<IMAGE 2>\n",
      "<IMAGE1>\n",
      "<IMAGE N>\n",
      "<IMAGE 3>\n",
      "<IMAGE 2>\n",
      "<IMAGE1>\n",
      "<IMAGE N>\n",
      "<IMAGE 3>\n",
      "<IMAGE 2>\n",
      "<IMAGE1>\n",
      "Dynamic \n",
      "Partition\n",
      "<IMAGE N>\n",
      "<IMAGE 3>\n",
      "<IMAGE 2>\n",
      "<IMAGE1>\n",
      "Video\n",
      "Dynamic \n",
      "Partition\n",
      "Image\n",
      "Resize\n",
      "\\n\n",
      "\\n\n",
      "\\n\n",
      "\\n\n",
      "Flatten\n",
      "\\n\n",
      "\\n\n",
      "\\n\n",
      "\\n\n",
      "\\n\n",
      "\\n\n",
      "\\n\n",
      "\\n\n",
      "\\n\n",
      "\\n\n",
      "sp\n",
      "Global \n",
      "View\n",
      "Local \n",
      "View\n",
      "Figure 5. Framework of IXC-2.5 that supports the multi-modal\n",
      "inputs, including text, single/multiple images, and videos.\n",
      "XComposer2-4KHD [34] (IXC2 and IXC2-4KHD for sim-\n",
      "plicity), including a light-weight Vision Encoder Ope-\n",
      "nAI ViT-L/14 [122], Large Language Model InternLM2-\n",
      "7B [13], and Partial LoRA [33] for efficient alignment. We\n",
      "recommend the readers to the IXC2 and IXC2-4KHD pa-\n",
      "pers for more details.\n",
      "3.2. Multi-modal Input\n",
      "Our IXC-2.5 supports diverse input modalities, including\n",
      "text, single/multiple images, and videos.\n",
      "As showin in\n",
      "Figure 5, a Unified Dynamic Image Partition strategy is\n",
      "adopted for both videos and multiple images with any reso-\n",
      "lutions and aspect ratios.\n",
      "Image Processing. We mainly follow the Dynamic Image\n",
      "Partition and Global-Local Format design used in IXC2-\n",
      "4KHD [34] with a few modifications. For the vision en-\n",
      "coder, we reuse the ViT of 490 × 490 resolution used in\n",
      "IXC2 and further increase its resolution to 560 × 560, so\n",
      "that each sub-image has 400 tokens.\n",
      "6\n",
      "For the high-resolution strategy, we unify the different\n",
      "strategies used in the IXC-4KHD into a scaled identity strat-\n",
      "egy. Given a maximum partition number H, the image x\n",
      "with size [h, w] is resized and padded to the new image ˆ\n",
      "x\n",
      "with size [ph × 560, pw × 560]. This process is subject to\n",
      "the following constraints:\n",
      "pw1 × ⌈pw1 × h/w⌉≤H;\n",
      "(1)\n",
      "pw2 = ⌈w ∗s/560⌉\n",
      "(2)\n",
      "pw = min(pw1, pw2) ; ph = ⌈pw × h/w⌉\n",
      "(3)\n",
      "where s is the scale factor, pw and ph represent the number\n",
      "of patches in each row and column, respectively.\n",
      "For multi-image input, we assign an index to each image\n",
      "like <IMAGE i>, i ∈{1, 2, 3, . . .} and format the image\n",
      "and text in an interleaved format.\n",
      "Video Processing. We sample frames from the given video\n",
      "and concatenate them along the short side of the frame,\n",
      "leading to a high-resolution image. The frame index is also\n",
      "written in the image to provide the temporal relation.\n",
      "Audio Processing. IXC-2.5 web demo supports audio input\n",
      "and output using open-source tools. For audio input, we\n",
      "employ Whisper [123] to transcribe audio into text. For\n",
      "audio output, we utilize MeloTTS [179] to convert the text\n",
      "back into audio.\n",
      "3.3. Pre-training\n",
      "During the pre-training phase, the LLM (InternLM2-\n",
      "7B [143]) is frozen while both the vision encoder and Partial\n",
      "LoRA [33] are fine-tuned to align the visual tokens with the\n",
      "LLM. The data used for pre-training is shown in Table 1.\n",
      "In practice, we employ the CLIP ViT-L-14-490 [122]\n",
      "from IXC2 as the vision encoder and further increase its\n",
      "resolution to 560 × 560.\n",
      "For the Unified Dynamic Im-\n",
      "age Partition strategy [34], we set the maximum number\n",
      "H = 12 for the pertaining. For the Partial LoRA [33], we\n",
      "set a rank of 256 for all the linear layers in the LLM de-\n",
      "coder block. Our training process involves a batch size of\n",
      "4096 and spans across 2 epochs. The learning rate linearly\n",
      "increases to 2 × 10−4 within the first 1% of the training\n",
      "steps. Following this, it decreases to 0 according to a cosine\n",
      "decay strategy. To preserve the original knowledge of the\n",
      "vision encoder, we apply a layer-wise learning rate (LLDR)\n",
      "decay strategy [33], and the decay factor is set to 0.90.\n",
      "3.4. Supervised Fine-tuning\n",
      "We fine-tune the model with data listed in Table 2. The\n",
      "maximum number H of the Unified Dynamic Image Par-\n",
      "tition strategy is 24 to handle extremely large images and\n",
      "videos. For video datasets, the IXC-2.5 is trained with large\n",
      "images concatenated by at most 64 frames.\n",
      "The largest\n",
      "training context is set to a 24,000 context window size,\n",
      "where the MMDU [92] dataset can achieve this limitation.\n",
      "In practice, we jointly train all the components with a batch\n",
      "size of 2048 over 4000 steps. Data from multiple sources\n",
      "are sampled in a weighted manner, with the weights based\n",
      "on the number of data from each source. The maximum\n",
      "learning rate is set to 5 × 10−5, and each component has its\n",
      "own unique learning strategy. For the vision encoder, we set\n",
      "the LLDR to 0.9, which aligns with the pretraining strategy.\n",
      "For the LLM, we employ a fixed learning rate scale factor\n",
      "of 0.2. This slows down the update of the LLM, achieving\n",
      "a balance between preserving its original capabilities and\n",
      "aligning it with vision knowledge.\n",
      "3.5. Webpage Generation\n",
      "We enhance the capabilities of the IXC-2.5 to include au-\n",
      "tomated webpage generation. Specifically, the IXC-2.5 is\n",
      "now equipped to autonomously construct web pages, utiliz-\n",
      "ing HTML, CSS, and JavaScript, based on input in the form\n",
      "of a visual screenshot, a set of free-form instructions, or\n",
      "a resume document. Current open-source general-purpose\n",
      "large language models frequently demonstrate suboptimal\n",
      "performance in generating HTML and CSS relative to their\n",
      "proficiency in natural language generation.\n",
      "To address\n",
      "this limitation, we propose training the screenshot-to-code\n",
      "task using extensive datasets from WebSight v0.1/v0.2 [64],\n",
      "and Stack v2 [95]. Subsequently, we fine-tune the model\n",
      "with a smaller, meticulously crafted dataset consisting of\n",
      "instruction-aware webpage generation and personal page\n",
      "generation examples.\n",
      "Screenshot-to-code.\n",
      "In addition to the WebSight [64]\n",
      "datasets, we preprocess the HTML and CSS code from\n",
      "the Stack v2 [95] dataset to facilitate screenshot-to-code\n",
      "training. Initially, we combine the CSS and HTML code\n",
      "into a single file. Subsequently, we remove all comments,\n",
      "JavaScript code, and external links. Furthermore, we elim-\n",
      "inate any CSS styles that are not referenced by the HTML\n",
      "code. We convert all files into screenshots, subsequently\n",
      "discarding those that did not render successfully. The re-\n",
      "maining screenshots are then processed using the IXC2-\n",
      "4KHD [34] model to assess the quality of the web pages.\n",
      "Following the exclusion of low-quality web pages, we re-\n",
      "tained a final set of three remaining about 250,000 high-\n",
      "quality web pages.\n",
      "We conduct training on the LoRA model utilizing the\n",
      "three aforementioned datasets. The LoRA rank is set to\n",
      "512. The training protocol employs a batch size of 512 and\n",
      "is executed over a single epoch. Initially, the learning rate\n",
      "is incremented linearly to 1 × 10−4 within the first 1% of\n",
      "the training iterations. Subsequently, the learning rate de-\n",
      "creases to 0 following a cosine decay schedule.\n",
      "Instruction-aware Webpage Generation.\n",
      "A pivotal at-\n",
      "tribute of large language models lies in their capability to\n",
      "adhere to human instructions. To facilitate web page gen-\n",
      "eration based on freeform instructions, we propose con-\n",
      "7\n",
      "Task\n",
      "Dataset\n",
      "General Semantic Alignment\n",
      "ShareGPT4V-PT [17], COCO [21], Nocaps [1], TextCaps [132], LAION [125], SBU [114], CC 3M [129] ALLaVA [15]\n",
      "World Knowledge Alignment\n",
      "Concept Data [173]\n",
      "Vision Capability Enhancement WanJuan [46], Flicker[160], MMC-Inst[82], RCTW-17[130], CTW[165], LSVT[137], ReCTs[175], ArT[28]\n",
      "Table 1. Datasets used for Pre-Training. The data are collected from diverse sources for the three objectives.\n",
      "Task\n",
      "Dataset\n",
      "Caption\n",
      "ShareGPT4V [17], COCO [21], Nocaps [1]\n",
      "General QA\n",
      "VQAv2 [4], GQA [53], OK-VQA [105]\n",
      "VD [32], RD [16], VSR [81], ALLaVA-QA [15]\n",
      "Multi-Turn QA\n",
      "MMDU [92]\n",
      "Science QA\n",
      "AI2D [61], SQA [98], TQA [62], IconQA [97]\n",
      "Chart QA\n",
      "DVQA [58], ChartQA [106], ChartQA-AUG [106]\n",
      "Math QA\n",
      "MathQA [161], Geometry3K [96], TabMWP [99],\n",
      "CLEVR-MATH [80], Super [75]\n",
      "World Knowledge QA A-OKVQA [127], KVQA [128], ViQuAE [65]\n",
      "OCR QA\n",
      "TextVQA [133], OCR-VQA [109], ST-VQA [11]\n",
      "HD-OCR QA\n",
      "InfoVQA[108], DocVQA [107], TabFact [20],\n",
      "WTQ [117], DeepForm [139], Visual MRC [140]\n",
      "Video\n",
      "ShareGPT4Video [19], ActivityNet [37]\n",
      "Conversation\n",
      "LLaVA-150k [84], LVIS-Instruct4V [149]\n",
      "ShareGPT-en&zh [27], InternLM-Chat [143]\n",
      "Table 2. Datasets used for Supervised Fine-Tuning. We collect\n",
      "data from diverse sources to empower the model with different\n",
      "capabilities.\n",
      "structing data through querying closed-source large lan-\n",
      "guage models. Specifically, we utilize GPT-4 to generate\n",
      "diverse instructions and concepts for web page creation, en-\n",
      "compassing elements such as type, style, and layout. Subse-\n",
      "quently, these instructions are harnessed to query Claude-3-\n",
      "sonnet [3] for the actual web page generation process. This\n",
      "approach results in 18,000 high-quality, instruction-aware\n",
      "samples. Additionally, we employ Tailwind CSS instead of\n",
      "traditional CSS, given its succinct nature.\n",
      "Resume-to-homepage.\n",
      "In addition to instruction-aware\n",
      "webpage generation, we introduce a more practical task.\n",
      "Specifically, given a resume, the model is designed to gen-\n",
      "erate a personal homepage. This homepage not only en-\n",
      "capsulates the information present in the resume but also\n",
      "presents it with a well-structured and visually appealing\n",
      "format, improving both content organization and aesthetic\n",
      "layout. To generate corresponding datasets, we propose an\n",
      "idea-resume-homepage data generation pipeline. Initially,\n",
      "we leverage GPT-4 to produce resume ideas tailored for di-\n",
      "verse personas, such as researchers, students, and engineers.\n",
      "GPT-4 is tasked with generating these resumes in mark-\n",
      "down format based on the provided ideas. Upon obtaining\n",
      "the generated resumes, we then prompt Claude-3-sonnet [3]\n",
      "to create corresponding homepages from these resumes.\n",
      "To enhance the interactivity of these webpages, Claude-3-\n",
      "sonnet is also utilized to generate JavaScript events based\n",
      "on the HTML code. In total, we have constructed a dataset\n",
      "comprising 2,000 samples.\n",
      "Upon constructing the dataset for instruction-aware web-\n",
      "page generation and resume-to-homepage, we subsequently\n",
      "fine-tuned the LoRA model for 10 epochs. All other ex-\n",
      "perimental settings were maintained consistent with those\n",
      "employed during the screenshot-to-code training phase.\n",
      "3.6. Article Composing\n",
      "Generating high-quality text-image articles (e.g., poetry,\n",
      "novels, short stories, and essays) is a crucial capability for\n",
      "AI assistants, with various applications in daily life, includ-\n",
      "ing education and entertainment. Building upon the IXC-\n",
      "2.5 SFT model π in Section 3.4, we enhance creative writ-\n",
      "ing capabilities for generating high-quality text-image arti-\n",
      "cles. However, collecting high-quality text-image articles is\n",
      "a rare and expensive endeavor. Direct fine-tuning on scarce\n",
      "instruction data can lead to unstable responses from LVLMs\n",
      "in most cases. To overcome these challenges, we propose a\n",
      "scalable pipeline that integrates supervised fine-tuning, re-\n",
      "ward modeling, preference data collection, and DPO align-\n",
      "ment for high-quality and stable article generation.\n",
      "Supervised Fine-tuning. We begin with the SFT model π\n",
      "(Section 3.4) and a collection of 5,000 instruction tuning\n",
      "data samples D from IXC2 [33], focused on article writing.\n",
      "Due to the limited scale of the instruction data, we use the\n",
      "SFT model to rewrite the original prompts using the Chain-\n",
      "of-Thought (CoT) technique [152], generating step-by-step\n",
      "prompts to supplement the instruction tuning data as aug-\n",
      "mented data D∗. We observe that the SFT model is more ef-\n",
      "fective in generating long-form responses when using these\n",
      "augmented prompts. We then train the initial model π on\n",
      "the augmented instruction tuning data via LoRA [51] with\n",
      "the rank of 256 and get the model πref to establish a starting\n",
      "point of our alignment pipeline.\n",
      "Preference Data Collection. We use the fine-tuned model\n",
      "πref to generate diverse responses for each prompt in the\n",
      "augmented instruction tuning data D∗, using different ran-\n",
      "dom seeds.\n",
      "This yields a collection of 80,000 prompt-\n",
      "response pairs. Next, we employ the GPT-4o model to label\n",
      "2,000 responses with chosen or rejected decisions and give\n",
      "the reasons, which serve as our reward modeling data. We\n",
      "then train a reward model πrm, sharing the same architecture\n",
      "of πref, on the reward modeling data. The reward model is\n",
      "used to make the chosen or rejected prediction on the re-\n",
      "maining prompt-response pairs. These selected responses\n",
      "8\n",
      "MVBench\n",
      "MLVU\n",
      "MME\n",
      "MMB∗1\n",
      "Temp∗2\n",
      "Doc\n",
      "Chart\n",
      "Info\n",
      "Text\n",
      "OCR\n",
      "WTQ\n",
      "Deep\n",
      "Visual\n",
      "Tab\n",
      "Video\n",
      "Video\n",
      "Compass\n",
      "VQA\n",
      "QA\n",
      "VQA\n",
      "VQA\n",
      "Bench\n",
      "Form\n",
      "MRC\n",
      "Fact\n",
      "Open-Source\n",
      "VideoChat InternVL\n",
      "LIVA\n",
      "InternVL\n",
      "Qwen-VL\n",
      "InternVL\n",
      "InternVL\n",
      "InternVL\n",
      "InternVL GLM-4v DocOwl\n",
      "DocOwl\n",
      "DocOwl\n",
      "DocOwl\n",
      "Previous SOTA\n",
      "2-7B[71] 1.5-26B[26]34B[78]1.5-26B[26]\n",
      "7B[6]\n",
      "1.5-26B[26]1.5-26B[26]1.5-26B[26]1.5-26B[26] 9B[43] 1.5-8B[50]1.5-8B[50]1.5-8B[50]1.5-8B[50]\n",
      "Performance\n",
      "60.4\n",
      "50.4\n",
      "59.0\n",
      "42.0\n",
      "58.4\n",
      "90.9\n",
      "83.8\n",
      "72.5\n",
      "80.6\n",
      "77.6\n",
      "40.6\n",
      "68.8\n",
      "246.4\n",
      "80.2\n",
      "Closed-source API\n",
      "GPT-4V [112]\n",
      "43.5\n",
      "49.2\n",
      "59.9\n",
      "56.0\n",
      "—\n",
      "88.4\n",
      "78.5\n",
      "75.1\n",
      "78.0\n",
      "51.6\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "Gemini-Pro [142]\n",
      "—\n",
      "—\n",
      "75.0\n",
      "49.3\n",
      "70.6\n",
      "88.1\n",
      "74.1\n",
      "75.2\n",
      "74.6\n",
      "68.0\n",
      "—\n",
      "—\n",
      "—\n",
      "—\n",
      "IXC-2.5-7B\n",
      "69.1\n",
      "58.8\n",
      "55.8\n",
      "46.9\n",
      "67.1\n",
      "90.9\n",
      "82.2\n",
      "69.9\n",
      "78.2\n",
      "69.0\n",
      "53.6\n",
      "71.2\n",
      "307.5\n",
      "85.2\n",
      "Table 3. Comparison with closed-source APIs and previous open-source SOTAs on Video Benchmarks and Structural High-resolution\n",
      "Benchmarks. The best results are bold and the second-best results are underlined. ∗1 We scale the score from 0 ∼3 to 0 ∼100 for easier\n",
      "understanding. ∗2 We report the determinism part (MCQA, Y/N, Caption Match) of TempCompass as the evaluation using GPT-3.5 is not\n",
      "stable.\n",
      "MMDU\n",
      "MMStar\n",
      "RealWQAMathVista\n",
      "AI2D\n",
      "MMMU\n",
      "MME\n",
      "MMB\n",
      "MMBCN\n",
      "MMB1.1\n",
      "SEEDI MM-Vet HallB\n",
      "Open-Source\n",
      "LLaVa1.6 InternVL\n",
      "WeMM\n",
      "WeMM\n",
      "InternVL\n",
      "360VL\n",
      "InternVL InternVL1.5InternVL1.5InternVL1.5 WeMM GLM-4v WeMM\n",
      "Previous SOTA\n",
      "8B[83] 1.5-26B[26] 8B[145]\n",
      "8B[145] 1.5-26B[26]70B[144]1.5-26B[26] 1.5-26B[26] 1.5-26B[26] 1.5-26B[26] 8B[145] 14B[43] 8B[145]\n",
      "Performance\n",
      "42.8\n",
      "57.1\n",
      "68.1\n",
      "54.9\n",
      "80.6\n",
      "53.4\n",
      "2,189.6\n",
      "82.3\n",
      "80.7\n",
      "79.7\n",
      "75.9\n",
      "58.0\n",
      "47.5\n",
      "Closed-source API\n",
      "GPT-4V [112]\n",
      "66.3\n",
      "57.1\n",
      "68.0\n",
      "47.8\n",
      "75.5\n",
      "56.8\n",
      "1,926.5\n",
      "81.3\n",
      "80.2\n",
      "79.8\n",
      "69.1\n",
      "56.8\n",
      "46.5\n",
      "Gemini-Pro [142]\n",
      "—\n",
      "42.6\n",
      "64.1\n",
      "45.8\n",
      "70.2\n",
      "47.9\n",
      "1,933.3\n",
      "73.9\n",
      "74.3\n",
      "73.9\n",
      "70.7\n",
      "59.2\n",
      "45.2\n",
      "IXC-2.5-7B\n",
      "56.6\n",
      "59.9\n",
      "67.8\n",
      "63.8\n",
      "81.5\n",
      "42.9\n",
      "2,229.0\n",
      "82.2\n",
      "80.8\n",
      "79.4\n",
      "75.4\n",
      "51.7\n",
      "42.4\n",
      "Table 4. Comparison with closed-source APIs and previous open-source SOTAs on Multi-Turn Multi-Image Dialog and General Visual\n",
      "QA Benchmarks. The best results are bold and the second-best results are underlined.\n",
      "are then used to construct the pair data Dp = {x, yw, yl},\n",
      "while x, yw and yl refer to the prompt, chosen response and\n",
      "rejected response, respectively. Ultimately, we obtain a to-\n",
      "tal of 30,000 preference data Dp for DPO [124] alignment.\n",
      "DPO Alignment. We use the DPO algorithm to update the\n",
      "SFT model πref on target policy from the preference data\n",
      "Dp:\n",
      "LDPO(πθ, πref) = Ex,yw,yl∼Dp\n",
      "[−log σ(β log( πθ(yw|x)\n",
      "πref(yw|x)) −β log( πθ(yl|x)\n",
      "πref(yl|x)))].\n",
      "(4)\n",
      "In practice, we use LoRA with a rank of 256 to get the\n",
      "DPO model πθ. We observe that our model tends to pri-\n",
      "oritize minimizing the likelihood of dis-preferred responses\n",
      "yl over maximizing the likelihood of preferred responses yw\n",
      "to avoid generating inappropriate or low-quality content.\n",
      "In summary, our scalable pipeline consists of three pri-\n",
      "mary components. First, we address the challenge of lim-\n",
      "ited instruction tuning data by re-writing original prompts\n",
      "into augmented prompts.\n",
      "Next, we generate diverse re-\n",
      "sponses using different random seeds, enabling the explo-\n",
      "ration of various creative possibilities. Finally, we apply the\n",
      "DPO algorithm to the chosen and rejected responses to re-\n",
      "fine our model’s performance. Through our pipeline, our\n",
      "model is capable of generating high-quality articles.\n",
      "4. Experiments\n",
      "In this section, we validate the benchmark performance of\n",
      "our InternLM-XComposer-2.5 (IXC-2.5) after supervised\n",
      "fine-tuning.\n",
      "4.1. LVLM Benchmark results.\n",
      "In Table 3 and Table 4, we compare our IXC-2.5 on a list\n",
      "of benchmarks with both closed-source APIs and SOTA\n",
      "open-source LVLMs (with comparable model size). Here\n",
      "we report video understanding results on MVBench [71],\n",
      "MLVU [181], MME-Video [42], MMBench-Video [38],\n",
      "TempCompass [88].\n",
      "For Structural High-resolution\n",
      "understanding,\n",
      "we report results on DocVQA [107],\n",
      "ChartQA [106], InfographicVQA [108], TextVQA [133],\n",
      "OCRBench\n",
      "[89],\n",
      "DeepForm\n",
      "[139],\n",
      "WikiTableQues-\n",
      "tion (WTQ) [117], Visual MRC [140], and TabFact [20].\n",
      "For general visual question answering, we report results\n",
      "on MMStar [18], RealWorldQA[155], MathVista [100],\n",
      "MMMU [166],\n",
      "AI2D [61],\n",
      "MME [40],\n",
      "MMBench\n",
      "(MMB)\n",
      "[87],\n",
      "MMBench-Chinese\n",
      "(MMBCN)\n",
      "[87],\n",
      "MMBench-v1.1\n",
      "(MMBv1.1)\n",
      "[87],\n",
      "SEED-Bench\n",
      "Im-\n",
      "age Part (SEEDI)[66], MM-Vet [164], HallusionBench\n",
      "(HallB) [44].\n",
      "For Multi-True Multi-Image dialogue, we\n",
      "evaluate IXC-2.5 on MMDU [92] benchmark. For webpage\n",
      "crafting, we report a subtask screenshot-to-code [131] since\n",
      "benchmarks for others are not available in the community.\n",
      "The evaluation is mainly conducted on the OpenCom-\n",
      "pass VLMEvalKit [30] for the unified reproduction of the\n",
      "results.\n",
      "Comparison on Video Understanding Benchmarks. As\n",
      "demonstrated in Table 3, IXC-2.5 exhibits competitive per-\n",
      "formance on fine-grained video understanding tasks, out-\n",
      "performing open-source models on 4 of the 5 benchmarks\n",
      "and being on par with Closed-Source APIs.\n",
      "For exam-\n",
      "ple, IXC-2.5 reaches 69.1 on the MVBench, +8.7% higher\n",
      "9\n",
      "Block-Match\n",
      "Text\n",
      "Position\n",
      "Color\n",
      "CLIP\n",
      "Average\n",
      "Closed-source API\n",
      "GPT-4V [112]\n",
      "85.8\n",
      "97.4\n",
      "80.5\n",
      "73.3\n",
      "86.9\n",
      "84.8\n",
      "Gemini-Pro [142]\n",
      "80.2\n",
      "94.6\n",
      "72.3\n",
      "66.2\n",
      "83.9\n",
      "79.4\n",
      "Open-source\n",
      "WebSight VLM-8B [64]\n",
      "55.9\n",
      "86.6\n",
      "77.3\n",
      "79.4\n",
      "86.5\n",
      "77.1\n",
      "CogAgent-Chat-18B [48]\n",
      "7.1\n",
      "18.1\n",
      "13.3\n",
      "13.0\n",
      "75.5\n",
      "25.4\n",
      "Design2Code-18B [131]\n",
      "78.5\n",
      "96.4\n",
      "74.3\n",
      "67.0\n",
      "85.8\n",
      "80.4\n",
      "IXC-2.5-7B\n",
      "81.9\n",
      "95.6\n",
      "80.9\n",
      "80.8\n",
      "86.5\n",
      "85.1\n",
      "Table 5. Screenshot-to-code. Comparison with closed-source APIs and open-source models on Design2Code benchmark. The best results\n",
      "are bold and the second-best results are underlined.\n",
      "than the previous SOTA method VideoChat2-7B and out-\n",
      "performs GPT-4V with +25.6%. For the recent challenging\n",
      "MMBench-Video, IXC-2.5 reaches the SOTA performance\n",
      "on open-source models and performs close to Gemini-Pro.\n",
      "Comparison\n",
      "on\n",
      "Structural\n",
      "High-resolution\n",
      "Bench-\n",
      "marks. Benefiting from the unified image partition strat-\n",
      "egy, IXC-2.5 could handle diverse kinds of images. Ta-\n",
      "ble 3 reports its performance on several structural high-\n",
      "resolution benchmarks. IXC-2.5 with only 7B parameters\n",
      "performs on par with the current large open-source LVLMs\n",
      "and close-source APIs. For example, IXC-2.5 gets 90.9%\n",
      "on the DocVQA test set, the same as InternVL-1.5 which\n",
      "has nearly 4× parameters. For the highly structured form\n",
      "and table understanding tasks, IXC-2.5 outperforms Do-\n",
      "cOwl 1.5-8B with +13.0%, +2.4%, +5.0% on WikiTable-\n",
      "Question, DeepForm and TableFace respectively.\n",
      "Comparison on Multi-Image Multi-Turn Benchmarks.\n",
      "IXC-2.5 is capable of taking multiple images as input and\n",
      "conducting multi-round free-form dialogue based on them.\n",
      "We evaluate it quantitatively on the newly proposed MMDU\n",
      "benchmark [92]. As shown in Table 4, the IXC-2.5 model\n",
      "demonstrates superior performance, outperforming the pre-\n",
      "vious SOTA open-source model by a significant margin of\n",
      "13.8%. This notable improvement highlights the effective-\n",
      "ness of our approach in advancing the capabilities of multi-\n",
      "image and multi-turn understanding.\n",
      "Comparison on General Visual QA Benchmarks. IXC-\n",
      "2.5 is designed as a general LVLM to handle diverse multi-\n",
      "modal tasks. Here we report its performance on general\n",
      "visual QA benchmarks. As shown in Table 4, the IXC-2.5\n",
      "shows superb performance on these benchmarks and on par\n",
      "with current large open-source LVLMs and closed-source\n",
      "APIs. For example, IXC-2.5 gets 59.9% on the challenging\n",
      "MMStar and outperforms GPT-4V and Gemini-Pro. On the\n",
      "RealWorldQA, IXC-2.5 also performs better than Gemini-\n",
      "Pro and close to GPT-4V.\n",
      "Comparison on Screenshot-to-code Benchmark. Table 5\n",
      "presents the comparison results on the Design2Code [131]\n",
      "benchmark that assesses the ability to translate visual de-\n",
      "sign into code implementation. Our IXC-2.5 even surpasses\n",
      "the GPT-4v on average performance, which highlights the\n",
      "potential of IXC-2.5 to excel in bridging the gap between\n",
      "visual design and code implementation.\n",
      "5. Conclusion\n",
      "We have introduced InternLM-XComposer-2.5 (IXC-2.5), a\n",
      "cutting-edge Large Vision-Language Model (LVLM) boast-\n",
      "ing long-contextual input and output capabilities that en-\n",
      "able advanced features such as ultra-high resolution im-\n",
      "age understanding, fine-grained video understanding, multi-\n",
      "turn multi-image dialogue, webpage generation, and article\n",
      "composing. Our comprehensive experiments demonstrate\n",
      "that IXC-2.5 achieves competitive performance, remark-\n",
      "ably, with a relatively modest 7B Large Language Model\n",
      "(LLM) backend.\n",
      "Our model sets out a promising research direction that\n",
      "can extend to a more contextual multi-modal environ-\n",
      "ment, including long-context video understanding (e.g.,\n",
      "long movies) and long-context interaction history, to better\n",
      "assist humans in real-world applications.\n",
      "Acknowledgements: We deeply express our gratitude to\n",
      "Prof. Chao Zhang from Tsinghua University for sugges-\n",
      "tions about audio models and tools.\n",
      "10\n",
      "References\n",
      "[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\n",
      "Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh,\n",
      "Stefan Lee, and Peter Anderson.\n",
      "Nocaps: Novel object\n",
      "captioning at scale. In Proceedings of the IEEE/CVF In-\n",
      "ternational Conference on Computer Vision (ICCV), 2019.\n",
      "8\n",
      "[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\n",
      "toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\n",
      "Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,\n",
      "Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,\n",
      "Sina Samangooei, Marianne Monteiro, Jacob Menick, Se-\n",
      "bastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-\n",
      "hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\n",
      "Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.\n",
      "Flamingo: a visual language model for few-shot learning,\n",
      "2022. 2\n",
      "[3] Anthropic.\n",
      "Claude 3 haiku:\n",
      "our fastest model yet,\n",
      "2024. Available at: https://www.anthropic.com/\n",
      "news/claude-3-haiku. 1, 8\n",
      "[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\n",
      "Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi\n",
      "Parikh. VQA: Visual question answering. In Proceedings of\n",
      "the IEEE/CVF International Conference on Computer Vi-\n",
      "sion (ICCV), 2015. 8\n",
      "[5] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel,\n",
      "Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan\n",
      "Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon\n",
      "Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Worts-\n",
      "man, and Ludwig Schmidt.\n",
      "Openflamingo:\n",
      "An open-\n",
      "source framework for training large autoregressive vision-\n",
      "language models. arXiv.org, 2023. 2\n",
      "[6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\n",
      "Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\n",
      "Zhou. Qwen-VL: A frontier large vision-language model\n",
      "with versatile abilities. arXiv.org, 2023. 2, 9\n",
      "[7] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda\n",
      "Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna\n",
      "Goldie, Azalia Mirhoseini, Cameron McKinnon, et al.\n",
      "Constitutional AI: Harmlessness from ai feedback. arXiv\n",
      "preprint arXiv:2212.08073, 2022. 6\n",
      "[8] Baichuan. Baichuan 2: Open large-scale language models.\n",
      "arXiv.org, 2023. 2\n",
      "[9] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell\n",
      "Nye, Augustus Odena, Arushi Somani, and Sa˘\n",
      "gnak Tas\n",
      "¸ırlar.\n",
      "Introducing our multimodal models, 2023. 2\n",
      "[10] Tony Beltramelli. pix2code: Generating code from a graph-\n",
      "ical user interface screenshot. In Proceedings of the ACM\n",
      "SIGCHI symposium on engineering interactive computing\n",
      "systems, 2018. 6\n",
      "[11] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,\n",
      "Marc\n",
      "¸al Rusinol, Ernest Valveny, CV Jawahar, and Dimos-\n",
      "thenis Karatzas. Scene text visual question answering. In\n",
      "ICCV, 2019. 8\n",
      "[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\n",
      "biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\n",
      "tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\n",
      "Language models are few-shot learners. Advances in Neu-\n",
      "ral Information Processing Systems (NeurIPS), 33:1877–\n",
      "1901, 2020. 2\n",
      "[13] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu\n",
      "Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei\n",
      "Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei,\n",
      "Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia\n",
      "Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang,\n",
      "Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiax-\n",
      "ing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yin-\n",
      "ing Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kai-\n",
      "wen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun\n",
      "Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang\n",
      "Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang,\n",
      "Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng\n",
      "Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang,\n",
      "Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi\n",
      "Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong\n",
      "Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan,\n",
      "Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing\n",
      "Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang,\n",
      "Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang,\n",
      "Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue\n",
      "Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe\n",
      "Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng\n",
      "Qiu, Yu Qiao, and Dahua Lin. Internlm2 technical report.\n",
      "arXiv preprint arXiv:2403.17297, 2024. 2, 6\n",
      "[14] Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, and Ji-\n",
      "aqi Wang. DualFocus: Integrating macro and micro per-\n",
      "spectives in multi-modal large language models.\n",
      "arXiv\n",
      "preprint arXiv:2402.14767, 2024. 2\n",
      "[15] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Juny-\n",
      "ing Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jian-\n",
      "quan Li, Xiang Wan, and Benyou Wang. ALLaVA harness-\n",
      "ing gpt4v-synthesized data for a lite vision-language model.\n",
      "arXiv preprint arXiv:2402.11684, 2024. 8\n",
      "[16] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\n",
      "Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal\n",
      "llm’s referential dialogue magic. arXiv.org, 2023. 8\n",
      "[17] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui\n",
      "He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\n",
      "Improving large multi-modal models with better captions.\n",
      "arXiv preprint arXiv:2311.12793, 2023. 2, 8\n",
      "[18] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang\n",
      "Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,\n",
      "Dahua Lin, and Feng Zhao. Are we on the right way for\n",
      "evaluating large vision-language models?\n",
      "arXiv preprint\n",
      "arXiv:2403.20330, 2024. 2, 9\n",
      "[19] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang,\n",
      "Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin,\n",
      "Zhenyu Tang, et al. ShareGPT4Video: Improving video\n",
      "understanding and generation with better captions. arXiv\n",
      "preprint arXiv:2406.04325, 2024. 8\n",
      "[20] Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\n",
      "Zhang,\n",
      "Hong Wang,\n",
      "Shiyang Li,\n",
      "Xiyou Zhou,\n",
      "and\n",
      "William Yang Wang.\n",
      "TabFact: A large-scale dataset for\n",
      "table-based fact verification. In Proceedings of the Inter-\n",
      "11\n",
      "national Conference on Learning Representations (ICLR),\n",
      "2020. 2, 8, 9\n",
      "[21] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna\n",
      "Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence\n",
      "Zitnick. Microsoft coco captions: Data collection and eval-\n",
      "uation server, 2015. 8\n",
      "[22] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\n",
      "Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz,\n",
      "Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shak-\n",
      "eri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael\n",
      "Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi,\n",
      "Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin\n",
      "Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic,\n",
      "Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas\n",
      "Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner,\n",
      "Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu,\n",
      "Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhos-\n",
      "seini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\n",
      "Radu Soricut. Pali-x: On scaling up a multilingual vision\n",
      "and language model, 2023. 2\n",
      "[23] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,\n",
      "Jialin Wu,\n",
      "Paul Voigtlaender,\n",
      "Basil Mustafa,\n",
      "Sebas-\n",
      "tian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski,\n",
      "Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran\n",
      "Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu\n",
      "Soricut.\n",
      "Pali-3 vision language models: Smaller, faster,\n",
      "stronger, 2023.\n",
      "[24] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio-\n",
      "vanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,\n",
      "Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander\n",
      "Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Has-\n",
      "san Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal,\n",
      "James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini,\n",
      "Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas\n",
      "Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\n",
      "Radu Soricut. Pali: A jointly-scaled multilingual language-\n",
      "image model, 2023. 2\n",
      "[25] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo\n",
      "Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou\n",
      "Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and\n",
      "Jifeng Dai. Internvl: Scaling up vision foundation mod-\n",
      "els and aligning for generic visual-linguistic tasks. arXiv\n",
      "preprint arXiv:2312.14238, 2023. 2\n",
      "[26] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhang-\n",
      "wei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng\n",
      "Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang\n",
      "Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin,\n",
      "Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang,\n",
      "Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min\n",
      "Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao,\n",
      "Jifeng Dai, and Wenhai Wang.\n",
      "How far are we to gpt-\n",
      "4v? closing the gap to commercial multimodal models with\n",
      "open-source suites, 2024. 2, 9\n",
      "[27] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\n",
      "hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\n",
      "Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and\n",
      "Eric P. Xing. Vicuna: An open-source chatbot impressing\n",
      "gpt-4 with 90%* chatgpt quality, 2023. 8\n",
      "[28] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet\n",
      "Ng, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao\n",
      "Zhang, Junyu Han, Errui Ding, et al. Icdar2019 robust read-\n",
      "ing challenge on arbitrary-shaped text-rrc-art. In Interna-\n",
      "tional Conference on Document Analysis and Recognition\n",
      "(ICDAR), 2019. 8\n",
      "[29] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\n",
      "Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\n",
      "Barham, Hyung Won Chung, Charles Sutton, Sebastian\n",
      "Gehrmann, et al. Palm: Scaling language modeling with\n",
      "pathways. arXiv.org, 2022. 1, 2\n",
      "[30] OpenCompass Contributors.\n",
      "Opencompass:\n",
      "A univer-\n",
      "sal evaluation platform for foundation models. https:\n",
      "//github.com/open- compass/opencompass,\n",
      "2023. 9\n",
      "[31] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\n",
      "Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\n",
      "Fung, and Steven Hoi.\n",
      "Instructblip: Towards general-\n",
      "purpose vision-language models with instruction tuning,\n",
      "2023. 1, 2\n",
      "[32] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,\n",
      "Deshraj Yadav, Jos´\n",
      "e M.F. Moura, Devi Parikh, and Dhruv\n",
      "Batra. Visual Dialog. In Proceedings of the IEEE Confer-\n",
      "ence on Computer Vision and Pattern Recognition (CVPR),\n",
      "2017. 8\n",
      "[33] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,\n",
      "Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang,\n",
      "Haodong Duan, Maosong Cao, Wenwei Zhang, Yining\n",
      "Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jing-\n",
      "wen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu\n",
      "Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2:\n",
      "Mastering free-form text-image composition and compre-\n",
      "hension in vision-language large model.\n",
      "arXiv preprint\n",
      "arXiv:2401.16420, 2024. 2, 6, 7, 8\n",
      "[34] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin\n",
      "Wang, Linke Ouyang, Songyang Zhang, Haodong Duan,\n",
      "Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe\n",
      "Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang,\n",
      "Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu\n",
      "Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2-\n",
      "4khd: A pioneering large vision-language model handling\n",
      "resolutions from 336 pixels to 4k hd.\n",
      "arXiv preprint\n",
      "arXiv:2404.06512, 2024. 2, 6, 7\n",
      "[35] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,\n",
      "Aakanksha Chowdhery,\n",
      "Brian Ichter,\n",
      "Ayzaan Wahid,\n",
      "Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\n",
      "Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\n",
      "worth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,\n",
      "Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,\n",
      "and Pete Florence. Palm-e: An embodied multimodal lan-\n",
      "guage model. In arXiv preprint arXiv:2303.03378, 2023.\n",
      "2\n",
      "[36] Kawin Ethayarajh,\n",
      "Winnie Xu,\n",
      "Niklas Muennighoff,\n",
      "Dan Jurafsky, and Douwe Kiela.\n",
      "KTO: Model align-\n",
      "ment as prospect theoretic optimization.\n",
      "arXiv preprint\n",
      "arXiv:2402.01306, 2024. 6\n",
      "[37] Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia\n",
      "and Juan Carlos Niebles. ActivityNet: A large-scale video\n",
      "12\n",
      "benchmark for human activity understanding. In Proceed-\n",
      "ings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition (CVPR), 2015. 2, 8\n",
      "[38] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao,\n",
      "Yining Li, Dahua Lin, and Kai Chen. MMBench-Video: A\n",
      "long-form multi-shot benchmark for holistic video under-\n",
      "standing. arXiv preprint arXiv:2406.14515, 2024. 2, 9\n",
      "[39] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao,\n",
      "Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: A\n",
      "long-form multi-shot benchmark for holistic video under-\n",
      "standing, 2024. 2\n",
      "[40] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\n",
      "Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-\n",
      "rui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-\n",
      "grong Ji. Mme: A comprehensive evaluation benchmark\n",
      "for multimodal large language models.\n",
      "arXiv preprint\n",
      "arXiv:2306.13394, 2023. 2, 9\n",
      "[41] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang,\n",
      "Zhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang\n",
      "Shen, Mengdan Zhang, Peixian Chen, Sirui Zhao, Shao-\n",
      "hui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hong-\n",
      "sheng Li, and Xing Sun. A challenger to gpt-4v? early\n",
      "explorations of gemini in visual expertise. arXiv preprint\n",
      "arXiv:2312.12436, 2023. 1, 2\n",
      "[42] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai\n",
      "Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang\n",
      "Shen, Mengdan Zhang, et al. Video-MME: The first-ever\n",
      "comprehensive evaluation benchmark of multi-modal llms\n",
      "in video analysis. arXiv preprint arXiv:2405.21075, 2024.\n",
      "2, 9\n",
      "[43] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui\n",
      "Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao,\n",
      "Hanyu Lai, et al. ChatGLM: A family of large language\n",
      "models from glm-130b to glm-4 all tools. arXiv preprint\n",
      "arXiv:2406.12793, 2024. 9\n",
      "[44] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian,\n",
      "Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen,\n",
      "Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi\n",
      "Zhou. Hallusionbench: An advanced diagnostic suite for\n",
      "entangled language hallucination & visual illusion in large\n",
      "vision-language models, 2023. 2, 9\n",
      "[45] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xue-\n",
      "fei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam\n",
      "Lim.\n",
      "Ma-lmm:\n",
      "Memory-augmented large multimodal\n",
      "model for long-term video understanding. arXiv preprint\n",
      "arXiv:2404.05726, 2024. 2\n",
      "[46] Conghui He, Zhenjiang Jin, Chaoxi Xu, Jiantao Qiu, Bin\n",
      "Wang, Wei Li, Hang Yan, Jiaqi Wang, and Da Lin. Wan-\n",
      "juan: A comprehensive multimodal dataset for advancing\n",
      "english and chinese large models. ArXiv, abs/2308.10755,\n",
      "2023. 8\n",
      "[47] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu,\n",
      "Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao\n",
      "Dong, Ming Ding, et al.\n",
      "Cogagent: A visual language\n",
      "model for gui agents.\n",
      "arXiv preprint arXiv:2312.08914,\n",
      "2023. 2\n",
      "[48] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu,\n",
      "Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao\n",
      "Dong, Ming Ding, et al.\n",
      "CogAgent: A visual language\n",
      "model for gui agents.\n",
      "In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition\n",
      "(CVPR), 2024. 10\n",
      "[49] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang\n",
      "Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei\n",
      "Huang, et al. mplug-docowl 1.5: Unified structure learn-\n",
      "ing for ocr-free document understanding. arXiv preprint\n",
      "arXiv:2403.12895, 2024. 2\n",
      "[50] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang\n",
      "Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang,\n",
      "et al.\n",
      "mPLUG-DocOwl 1.5:\n",
      "Unified structure learn-\n",
      "ing for ocr-free document understanding. arXiv preprint\n",
      "arXiv:2403.12895, 2024. 9\n",
      "[51] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\n",
      "Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\n",
      "LoRA: Low-rank adaptation of large language models. In\n",
      "Proceedings of the International Conference on Learning\n",
      "Representations (ICLR), 2022. 8\n",
      "[52] Suyuan Huang, Haoxin Zhang, Yan Gao, Yao Hu, and\n",
      "Zengchang Qin. From image to video, what do we need in\n",
      "multimodal llms? arXiv preprint arXiv:2404.11865, 2024.\n",
      "2\n",
      "[53] Drew A Hudson and Christopher D Manning. Gqa: A new\n",
      "dataset for real-world visual reasoning and compositional\n",
      "question answering. Conference on Computer Vision and\n",
      "Pattern Recognition (CVPR), 2019. 8\n",
      "[54] Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Na-\n",
      "garajan, Lorenzo Torresani, and Gedas Bertasius. Video\n",
      "recap: Recursive captioning of hour-long videos.\n",
      "arXiv\n",
      "preprint arXiv:2402.13250, 2024. 2\n",
      "[55] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,\n",
      "Chris Bamford, Devendra Singh Chaplot, Diego de las\n",
      "Casas, Florian Bressand, Gianna Lengyel, Guillaume Lam-\n",
      "ple, Lucile Saulnier, L´\n",
      "elio Renard Lavaud, Marie-Anne\n",
      "Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,\n",
      "Thomas Wang, Timoth´\n",
      "ee Lacroix, and William El Sayed.\n",
      "Mistral 7b, 2023. 1, 2\n",
      "[56] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku,\n",
      "Qian Liu, and Wenhu Chen.\n",
      "Mantis: Interleaved multi-\n",
      "image instruction tuning, 2024. 2\n",
      "[57] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun\n",
      "Cao, and Li Yuan. Chat-univi: Unified visual representa-\n",
      "tion empowers large language models with image and video\n",
      "understanding. arXiv preprint arXiv:2311.08046, 2023. 2\n",
      "[58] Kushal Kafle, Brian Price, Scott Cohen, and Christopher\n",
      "Kanan. DVQA: Understanding data visualizations via ques-\n",
      "tion answering. In Proceedings of the IEEE/CVF Confer-\n",
      "ence on Computer Vision and Pattern Recognition (CVPR),\n",
      "2018. 8\n",
      "[59] Kumara Kahatapitiya, Kanchana Ranasinghe, Jongwoo\n",
      "Park, and Michael S Ryoo. Language repository for long\n",
      "video understanding.\n",
      "arXiv preprint arXiv:2403.14622,\n",
      "2024. 2\n",
      "[60] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\n",
      "Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\n",
      "Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for\n",
      "13\n",
      "neural language models. arXiv preprint arXiv:2001.08361,\n",
      "2020. 2\n",
      "[61] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon\n",
      "Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is\n",
      "worth a dozen images.\n",
      "In Proceedings of the European\n",
      "Conference on Computer Vision (ECCV), 2016. 2, 8, 9\n",
      "[62] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk,\n",
      "Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are\n",
      "you smarter than a sixth grader? textbook question answer-\n",
      "ing for multimodal machine comprehension. In Proceed-\n",
      "ings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition (CVPR), 2017. 8\n",
      "[63] Wonkyun Kim, Changin Choi, Wonseok Lee, and Wonjong\n",
      "Rhee. An image grid can be worth a video: Zero-shot video\n",
      "question answering using a vlm, 2024. 6\n",
      "[64] Hugo Laurenc\n",
      "¸on, L´\n",
      "eo Tronchon, and Victor Sanh. Unlock-\n",
      "ing the conversion of web screenshots into html code with\n",
      "the websight dataset.\n",
      "arXiv preprint arXiv:2403.09029,\n",
      "2024. 6, 7, 10\n",
      "[65] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herv´\n",
      "e\n",
      "Le Borgne, Romaric Besanc\n",
      "¸on, Jos´\n",
      "e G Moreno, and Jes´\n",
      "us\n",
      "Lov´\n",
      "on Melgarejo. Viquae, a dataset for knowledge-based\n",
      "visual question answering about named entities. In Pro-\n",
      "ceedings of the 45th International ACM SIGIR Conference\n",
      "on Research and Development in Information Retrieval,\n",
      "pages 3108–3120, 2022. 8\n",
      "[66] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\n",
      "iao Ge, and Ying Shan. Seed-bench: Benchmarking multi-\n",
      "modal llms with generative comprehension, 2023. 2, 9\n",
      "[67] Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang,\n",
      "Fanyi Pu, and Ziwei Liu. Otterhd: A high-resolution multi-\n",
      "modality model, 2023. 2\n",
      "[68] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\n",
      "Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model\n",
      "with in-context instruction tuning. arXiv.org, 2023. 2\n",
      "[69] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wen-\n",
      "hai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu\n",
      "Qiao. Videochat: Chat-centric video understanding. arXiv\n",
      "preprint arXiv:2305.06355, 2023. 2\n",
      "[70] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,\n",
      "Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al.\n",
      "Mvbench:\n",
      "A comprehensive multi-modal video under-\n",
      "standing benchmark.\n",
      "arXiv preprint arXiv:2311.17005,\n",
      "2023. 2\n",
      "[71] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,\n",
      "Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al.\n",
      "Mvbench:\n",
      "A comprehensive multi-modal video under-\n",
      "standing benchmark.\n",
      "In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition\n",
      "(CVPR), 2024. 2, 9\n",
      "[72] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang,\n",
      "Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng\n",
      "Kong. Silkie: Preference distillation for large visual lan-\n",
      "guage models. arXiv preprint arXiv:2312.10665, 2023. 6\n",
      "[73] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An\n",
      "image is worth 2 tokens in large language models. arXiv\n",
      "preprint arXiv:2311.17043, 2023. 2\n",
      "[74] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng\n",
      "Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya\n",
      "Jia. Mini-Gemini: Mining the potential of multi-modality\n",
      "vision language models. arXiv preprint arXiv:2403.18814,\n",
      "2024. 2\n",
      "[75] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam\n",
      "Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan L\n",
      "Yuille. Super-clevr: A virtual benchmark to diagnose do-\n",
      "main robustness in visual reasoning.\n",
      "In Proceedings of\n",
      "the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition (CVPR), 2023. 8\n",
      "[76] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang,\n",
      "Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai.\n",
      "Monkey: Image resolution and text label are important\n",
      "things for large multi-modal models.\n",
      "arXiv preprint\n",
      "arXiv:2311.06607, 2023. 2\n",
      "[77] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and\n",
      "Li Yuan.\n",
      "Video-llava: Learning united visual represen-\n",
      "tation by alignment before projection.\n",
      "arXiv preprint\n",
      "arXiv:2311.10122, 2023. 2\n",
      "[78] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov,\n",
      "Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi,\n",
      "and Song Han. Vila: On pre-training for visual language\n",
      "models, 2024. 2, 9\n",
      "[79] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian\n",
      "Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin\n",
      "Chen, et al. Sphinx: The joint mixing of weights, tasks, and\n",
      "visual embeddings for multi-modal large language models.\n",
      "arXiv preprint arXiv:2311.07575, 2023. 2\n",
      "[80] Adam Dahlgren Lindstr¨\n",
      "om and Savitha Sam Abra-\n",
      "ham.\n",
      "Clevr-math:\n",
      "A dataset for compositional lan-\n",
      "guage, visual and mathematical reasoning. arXiv preprint\n",
      "arXiv:2208.05358, 2022. 8\n",
      "[81] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier.\n",
      "Visual spatial reasoning. Transactions of the Association\n",
      "for Computational Linguistics (TACL), 2023. 8\n",
      "[82] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen,\n",
      "Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong\n",
      "Yu.\n",
      "Mmc:\n",
      "Advancing multimodal chart understand-\n",
      "ing with large-scale instruction tuning.\n",
      "arXiv preprint\n",
      "arXiv:2311.10774, 2023. 8\n",
      "[83] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\n",
      "Improved baselines with visual instruction tuning. arXiv\n",
      "preprint arXiv:2310.03744, 2023. 9\n",
      "[84] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\n",
      "Lee. Visual instruction tuning. arXiv.org, 2023. 1, 8\n",
      "[85] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\n",
      "Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im-\n",
      "proved reasoning, ocr, and world knowledge, 2024. 2\n",
      "[86] Shuo Liu, Kaining Ying, Hao Zhang, Yue Yang, Yuqi Lin,\n",
      "Tianle Zhang, Chuanhao Li, Yu Qiao, Ping Luo, Wenqi\n",
      "Shao, et al. Convbench: A multi-turn conversation eval-\n",
      "uation benchmark with hierarchical capability for large\n",
      "vision-language models. arXiv preprint arXiv:2403.20194,\n",
      "2024. 2\n",
      "[87] Yuan Liu,\n",
      "Haodong Duan,\n",
      "Yuanhan Zhang,\n",
      "Bo Li,\n",
      "Songyang Zhnag, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\n",
      "14\n",
      "Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mm-\n",
      "bench: Is your multi-modal model an all-around player?\n",
      "arXiv:2307.06281, 2023. 2, 9\n",
      "[88] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai\n",
      "Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Temp-\n",
      "Compass: Do video llms really understand videos? arXiv\n",
      "preprint arXiv:2403.00476, 2024. 2, 9\n",
      "[89] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng\n",
      "Yin, Cheng lin Liu, Lianwen Jin, and Xiang Bai. On the\n",
      "hidden mystery of ocr in large multimodal models, 2024. 2,\n",
      "9\n",
      "[90] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma,\n",
      "Shuo Zhang, and Xiang Bai.\n",
      "Textmonkey: An ocr-free\n",
      "large multimodal model for understanding document. arXiv\n",
      "preprint arXiv:2403.04473, 2024. 2\n",
      "[91] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\n",
      "enhofer, Trevor Darrell, and Saining Xie. A convnet for the\n",
      "2020s, 2022. 2\n",
      "[92] Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong,\n",
      "Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua\n",
      "Lin, et al. MMDU: A multi-turn multi-image dialog un-\n",
      "derstanding benchmark and instruction-tuning dataset for\n",
      "lvlms. arXiv preprint arXiv:2406.11833, 2024. 2, 7, 8, 9,\n",
      "10\n",
      "[93] Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xi-\n",
      "aoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang.\n",
      "RAR: Retrieving and ranking augmented mllms for visual\n",
      "recognition. arXiv preprint arXiv:2403.13805, 2024. 2\n",
      "[94] LocalLLaMA. Dynamically scaled rope further increases\n",
      "performance of long context llama with zero fine-tuning,\n",
      "2023. 2\n",
      "[95] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico\n",
      "Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang,\n",
      "Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder\n",
      "2 and the stack v2: The next generation. arXiv preprint\n",
      "arXiv:2402.19173, 2024. 7\n",
      "[96] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan\n",
      "Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: In-\n",
      "terpretable geometry problem solving with formal language\n",
      "and symbolic reasoning. In The 59th Annual Meeting of the\n",
      "Association for Computational Linguistics (ACL), 2021. 8\n",
      "[97] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao,\n",
      "Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu.\n",
      "Iconqa: A new benchmark for abstract diagram under-\n",
      "standing and visual language reasoning.\n",
      "arXiv preprint\n",
      "arXiv:2110.13214, 2021. 8\n",
      "[98] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\n",
      "Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark,\n",
      "and Ashwin Kalyan.\n",
      "Learn to explain: Multimodal rea-\n",
      "soning via thought chains for science question answer-\n",
      "ing. In Advances in Neural Information Processing Systems\n",
      "(NeurIPS), 2022. 8\n",
      "[99] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-\n",
      "Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin\n",
      "Kalyan. Dynamic prompt learning via policy gradient for\n",
      "semi-structured mathematical reasoning.\n",
      "arXiv preprint\n",
      "arXiv:2209.14610, 2022. 8\n",
      "[100] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-\n",
      "yuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang,\n",
      "Michel Galley, and Jianfeng Gao. Mathvista: Evaluating\n",
      "mathematical reasoning of foundation models in visual con-\n",
      "texts. In International Conference on Learning Represen-\n",
      "tations (ICLR), 2024. 2, 9\n",
      "[101] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong,\n",
      "Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu\n",
      "Wei.\n",
      "Valley: Video assistant with large language model\n",
      "enhanced ability. arXiv preprint arXiv:2306.07207, 2023.\n",
      "2\n",
      "[102] Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shum-\n",
      "ing Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang,\n",
      "Li Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha\n",
      "Zhang, and Furu Wei. Kosmos-2.5: A multimodal literate\n",
      "model, 2023. 2\n",
      "[103] Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu\n",
      "Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi\n",
      "Dong, Pan Zhang, Jiang Yu-Gang Pan, Liangming, Jiaqi\n",
      "Wang, Yixin Cao, and Aixin Sun. MMLongBench-Doc:\n",
      "Benchmarking long-context document understanding with\n",
      "visualizations. arXiv preprint arXiv:2407.01523, 2024. 2\n",
      "[104] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and\n",
      "Fahad Shahbaz Khan.\n",
      "Video-chatgpt: Towards detailed\n",
      "video understanding via large vision and language models.\n",
      "arXiv preprint arXiv:2306.05424, 2023. 2\n",
      "[105] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\n",
      "Roozbeh Mottaghi. Ok-vqa: A visual question answering\n",
      "benchmark requiring external knowledge. In Proceedings\n",
      "of the IEEE/CVF Conference on Computer Vision and Pat-\n",
      "tern Recognition (CVPR), pages 3195–3204, 2019. 8\n",
      "[106] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,\n",
      "and Enamul Hoque. Chartqa: A benchmark for question\n",
      "answering about charts with visual and logical reasoning.\n",
      "arXiv preprint arXiv:2203.10244, 2022. 2, 8, 9\n",
      "[107] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.\n",
      "Docvqa: A dataset for vqa on document images. In Proc. of\n",
      "the IEEE Winter Conference on Applications of Computer\n",
      "Vision (WACV), 2021. 8, 9\n",
      "[108] Minesh Mathew, Viraj Bagal, Rub`\n",
      "en Tito, Dimosthenis\n",
      "Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa.\n",
      "In Proc. of the IEEE Winter Conference on Applications of\n",
      "Computer Vision (WACV), 2022. 2, 8, 9\n",
      "[109] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\n",
      "Anirban Chakraborty. OCR-VQA: Visual question answer-\n",
      "ing by reading text in images. In International Conference\n",
      "on Document Analysis and Recognition (ICDAR), 2019. 8\n",
      "[110] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui,\n",
      "Lu Yuan, Dongdong Chen, and Li Yuan.\n",
      "Video-bench:\n",
      "A comprehensive benchmark and toolkit for evaluat-\n",
      "ing video-based large language models.\n",
      "arXiv preprint\n",
      "arXiv:2311.16103, 2023. 2\n",
      "[111] OpenAI.\n",
      "Chatgpt.\n",
      "https://openai.com/blog/\n",
      "chatgpt, 2022. 1, 2\n",
      "[112] OpenAI. Gpt-4 technical report, 2023. 1, 2, 9, 10\n",
      "[113] Maxime Oquab, Timoth´\n",
      "ee Darcet, Th´\n",
      "eo Moutakanni, Huy\n",
      "Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\n",
      "15\n",
      "Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,\n",
      "Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Rus-\n",
      "sell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra,\n",
      "Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu\n",
      "Xu, Herv´\n",
      "e Jegou, Julien Mairal, Patrick Labatut, Armand\n",
      "Joulin, and Piotr Bojanowski. Dinov2: Learning robust vi-\n",
      "sual features without supervision, 2024. 2\n",
      "[114] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\n",
      "Im2text: Describing images using 1 million captioned pho-\n",
      "tographs. In Advances in Neural Information Processing\n",
      "Systems (NeurIPS), 2011. 8\n",
      "[115] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\n",
      "roll Wainwright, Pamela Mishkin, Chong Zhang, Sand-\n",
      "hini Agarwal, Katarina Slama, Alex Ray, et al. Training\n",
      "language models to follow instructions with human feed-\n",
      "back. Advances in Neural Information Processing Systems\n",
      "(NeurIPS), 2022. 2, 6\n",
      "[116] Arka Pal,\n",
      "Deep Karkhanis,\n",
      "Samuel Dooley,\n",
      "Manley\n",
      "Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing\n",
      "failure modes of preference optimisation with dpo-positive.\n",
      "arXiv preprint arXiv:2402.13228, 2024. 6\n",
      "[117] Panupong Pasupat and Percy Liang. Compositional seman-\n",
      "tic parsing on semi-structured tables. In The Annual Meet-\n",
      "ing of the Association for Computational Linguistics (ACL),\n",
      "2015. 2, 8, 9\n",
      "[118] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\n",
      "Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding\n",
      "multimodal large language models to the world. arXiv.org,\n",
      "2023. 2\n",
      "[119] Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Run-\n",
      "tao Liu, Rui Pan, and Tong Zhang. Strengthening multi-\n",
      "modal large language model with bootstrapped preference\n",
      "optimization. arXiv preprint arXiv:2403.08730, 2024. 6\n",
      "[120] Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuan-\n",
      "grui Ding, Dahua Lin, and Jiaqi Wang.\n",
      "Streaming long\n",
      "video understanding with large language models, 2024. 2\n",
      "[121] Qwen.\n",
      "Introducing Qwen-7B: Open foundation and\n",
      "human-aligned models (of the state-of-the-arts), 2023. 1,\n",
      "2\n",
      "[122] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\n",
      "Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\n",
      "Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\n",
      "ing transferable visual models from natural language super-\n",
      "vision. In Proceedings of the International Conference on\n",
      "Machine learning (ICML), 2021. 2, 6, 7\n",
      "[123] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,\n",
      "Christine McLeavey, and Ilya Sutskever.\n",
      "Robust speech\n",
      "recognition via large-scale weak supervision. arxiv 2022.\n",
      "arXiv preprint arXiv:2212.04356, 10, 2022. 2, 7\n",
      "[124] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-\n",
      "pher D Manning, Stefano Ermon, and Chelsea Finn. Direct\n",
      "preference optimization: Your language model is secretly a\n",
      "reward model. In Advances in Neural Information Process-\n",
      "ing Systems (NeurIPS), 2024. 2, 6, 9\n",
      "[125] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\n",
      "Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\n",
      "Coombes, Jenia Jitsev, and Aran Komatsuzaki.\n",
      "Laion-\n",
      "400m: Open dataset of clip-filtered 400 million image-text\n",
      "pairs. arXiv preprint arXiv:2111.02114, 2021. 8\n",
      "[126] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-\n",
      "ford, and Oleg Klimov. Proximal policy optimization algo-\n",
      "rithms. arXiv preprint arXiv:1707.06347, 2017. 6\n",
      "[127] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\n",
      "Kenneth Marino, and Roozbeh Mottaghi.\n",
      "A-okvqa: A\n",
      "benchmark for visual question answering using world\n",
      "knowledge.\n",
      "In Proceedings of the European Conference\n",
      "on Computer Vision (ECCV), 2022. 8\n",
      "[128] Sanket Shah,\n",
      "Anand Mishra,\n",
      "Naganand Yadati,\n",
      "and\n",
      "Partha Pratim Talukdar.\n",
      "Kvqa: Knowledge-aware visual\n",
      "question answering. In Proceedings of the Conference on\n",
      "Artificial Intelligence (AAAI), 2019. 8\n",
      "[129] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\n",
      "Soricut. Conceptual captions: A cleaned, hypernymed, im-\n",
      "age alt-text dataset for automatic image captioning. In The\n",
      "Annual Meeting of the Association for Computational Lin-\n",
      "guistics (ACL), 2018. 8\n",
      "[130] Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang,\n",
      "Pei Xu, Linyan Cui, Serge Belongie, Shijian Lu, and Xiang\n",
      "Bai. Icdar2017 competition on reading chinese text in the\n",
      "wild (rctw-17). In International Conference on Document\n",
      "Analysis and Recognition (ICDAR), 2017. 8\n",
      "[131] Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo\n",
      "Liu, and Diyi Yang.\n",
      "Design2code:\n",
      "How far are we\n",
      "from automating front-end engineering?\n",
      "arXiv preprint\n",
      "arXiv:2403.03163, 2024. 2, 6, 9, 10\n",
      "[132] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\n",
      "Amanpreet Singh. Textcaps: a dataset for image captioning\n",
      "with reading comprehension. In Proceedings of the Euro-\n",
      "pean Conference on Computer Vision (ECCV), 2020. 8\n",
      "[133] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\n",
      "Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\n",
      "Rohrbach. Towards vqa models that can read. In Proceed-\n",
      "ings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition (CVPR), 2019. 2, 8, 9\n",
      "[134] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng\n",
      "Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye,\n",
      "Yan Lu, Jenq-Neng Hwang, et al.\n",
      "Moviechat:\n",
      "From\n",
      "dense token to sparse memory for long video understand-\n",
      "ing. arXiv preprint arXiv:2307.16449, 2023. 2\n",
      "[135] Enxin Song, Wenhao Chai, Tian Ye, Jenq-Neng Hwang, Xi\n",
      "Li, and Gaoang Wang. Moviechat+: Question-aware sparse\n",
      "memory for long video question answering. arXiv preprint\n",
      "arXiv:2404.17176, 2024. 2\n",
      "[136] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiy-\n",
      "ing Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao,\n",
      "Jingjing Liu, Tiejun Huang, and Xinlong Wang. Genera-\n",
      "tive multimodal models are in-context learners, 2024. 2\n",
      "[137] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu,\n",
      "Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jing-\n",
      "tuo Liu, Dimosthenis Karatzas, et al. Icdar 2019 competi-\n",
      "tion on large-scale street view text with partial labeling-rrc-\n",
      "lsvt.\n",
      "In International Conference on Document Analysis\n",
      "and Recognition (ICDAR), 2019. 8\n",
      "[138] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang,\n",
      "Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang.\n",
      "16\n",
      "Alpha-CLIP: A clip model focusing on wherever you want.\n",
      "In Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition (CVPR), 2024. 2\n",
      "[139] S. Svetlichnaya. DeepForm: Understand structured docu-\n",
      "ments at scale., 2020. 2, 8, 9\n",
      "[140] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida.\n",
      "Vi-\n",
      "sualMRC: Machine reading comprehension on document\n",
      "images. In Proceedings of the Conference on Artificial In-\n",
      "telligence (AAAI), 2021. 2, 8, 9\n",
      "[141] Jingqun Tang,\n",
      "Chunhui Lin,\n",
      "Zhen Zhao,\n",
      "Shu Wei,\n",
      "Binghong Wu, Qi Liu, Hao Feng, Yang Li, Siqi Wang, Lei\n",
      "Liao, Wei Shi, Yuliang Liu, Hao Liu, Yuan Xie, Xiang Bai,\n",
      "and Can Huang. Textsquare: Scaling up text-centric visual\n",
      "instruction tuning, 2024. 2\n",
      "[142] Gemini Team. Gemini: A family of highly capable multi-\n",
      "modal models, 2023. 1, 9, 10\n",
      "[143] InternLM Team. Internlm: A multilingual language model\n",
      "with progressively enhanced capabilities.\n",
      "https://\n",
      "github.com/InternLM/InternLM, 2023. 1, 2, 7,\n",
      "8\n",
      "[144] 360VL Team. 360vl, 2024. 9\n",
      "[145] WeMM Team. Wemm, 2024. 9\n",
      "[146] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\n",
      "Martinet, Marie-Anne Lachaux, Timoth´\n",
      "ee Lacroix, Bap-\n",
      "tiste Rozi`\n",
      "ere, Naman Goyal, Eric Hambro, Faisal Azhar,\n",
      "et al. Llama: Open and efficient foundation language mod-\n",
      "els. arXiv.org, 2023. 1, 2\n",
      "[147] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\n",
      "Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\n",
      "Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\n",
      "Llama 2: Open foundation and fine-tuned chat models,\n",
      "2023. 1, 2\n",
      "[148] Yuxuan Wan, Chaozheng Wang, Yi Dong, Wenxuan Wang,\n",
      "Shuqing Li, Yintong Huo, and Michael R Lyu. Automat-\n",
      "ically generating ui code from screenshot: A divide-and-\n",
      "conquer-based approach. arXiv preprint arXiv:2406.16386,\n",
      "2024. 6\n",
      "[149] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan\n",
      "Wu, and Yu-Gang Jiang. To see is to believe: Prompting\n",
      "gpt-4v for better visual instruction tuning. arXiv preprint\n",
      "arXiv:2311.07574, 2023. 8\n",
      "[150] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji\n",
      "Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\n",
      "Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming\n",
      "Ding, and Jie Tang. Cogvlm: Visual expert for pretrained\n",
      "language models, 2023. 2\n",
      "[151] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao,\n",
      "Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and\n",
      "Xiangyu Zhang.\n",
      "Vary:\n",
      "Scaling up the vision vocab-\n",
      "ulary for large vision-language models.\n",
      "arXiv preprint\n",
      "arXiv:2312.06109, 2023. 2\n",
      "[152] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n",
      "Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\n",
      "Chain-of-thought prompting elicits reasoning in large lan-\n",
      "guage models. In Advances in Neural Information Process-\n",
      "ing Systems (NeurIPS), 2022. 8\n",
      "[153] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n",
      "Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny\n",
      "Zhou.\n",
      "Chain-of-thought prompting elicits reasoning in\n",
      "large language models, 2023. 2\n",
      "[154] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang,\n",
      "and Bohan Zhuang.\n",
      "Longvlm: Efficient long video un-\n",
      "derstanding via large language models.\n",
      "arXiv preprint\n",
      "arXiv:2404.03384, 2024. 2\n",
      "[155] XAI. Grok-1.5 vision preview. 2024. 2, 9\n",
      "[156] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong\n",
      "Ng, and Jiashi Feng. Pllava : Parameter-free llava extension\n",
      "from images to videos for video dense captioning, 2024. 6\n",
      "[157] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin\n",
      "Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong\n",
      "Sun, and Gao Huang. Llava-uhd: an lmm perceiving any\n",
      "aspect ratio and high-resolution images.\n",
      "arXiv preprint\n",
      "arXiv:2403.11703, 2024. 2\n",
      "[158] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming\n",
      "Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji\n",
      "Zhang, et al. Ureader: Universal ocr-free visually-situated\n",
      "language understanding with multimodal large language\n",
      "model. arXiv preprint arXiv:2310.05126, 2023. 2\n",
      "[159] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\n",
      "Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\n",
      "Yaya Shi, et al.\n",
      "mplug-owl: Modularization empowers\n",
      "large language models with multimodality. arXiv.org, 2023.\n",
      "2\n",
      "[160] Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-\n",
      "enmaier. From image descriptions to visual denotations:\n",
      "New similarity metrics for semantic inference over event\n",
      "descriptions. Transactions of the Association for Computa-\n",
      "tional Linguistics (TACL), 2014. 8\n",
      "[161] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengy-\n",
      "ing Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian\n",
      "Weller, and Weiyang Liu. Metamath: Bootstrap your own\n",
      "mathematical questions for large language models. arXiv\n",
      "preprint arXiv:2309.12284, 2023. 8\n",
      "[162] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal.\n",
      "Self-chained image-language model for video localization\n",
      "and question answering. In Advances in Neural Information\n",
      "Processing Systems (NeurIPS), 2024. 2\n",
      "[163] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng\n",
      "Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng,\n",
      "Maosong Sun, et al. RLHF-V: Towards trustworthy mllms\n",
      "via behavior alignment from fine-grained correctional hu-\n",
      "man feedback. In Proceedings of the IEEE/CVF Confer-\n",
      "ence on Computer Vision and Pattern Recognition (CVPR),\n",
      "2024. 6\n",
      "[164] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\n",
      "Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\n",
      "Mm-vet:\n",
      "Evaluating large multimodal models for inte-\n",
      "grated capabilities. arXiv preprint arXiv:2308.02490, 2023.\n",
      "2, 9\n",
      "[165] Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Tai-Jiang\n",
      "Mu, and Shi-Min Hu. A large chinese text dataset in the\n",
      "wild. Journal of Computer Science and Technology, 34(3):\n",
      "509–521, 2019. 8\n",
      "17\n",
      "[166] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\n",
      "Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,\n",
      "Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin\n",
      "Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu\n",
      "Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and\n",
      "Wenhu Chen. Mmmu: A massive multi-discipline multi-\n",
      "modal understanding and reasoning benchmark for expert\n",
      "agi. arXiv preprint arXiv:2311.16502, 2023. 2, 9\n",
      "[167] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and\n",
      "Chen Change Loy.\n",
      "Contextual object detection with\n",
      "multimodal large language models.\n",
      "arXiv preprint\n",
      "arXiv:2305.18279, 2023. 2\n",
      "[168] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu\n",
      "Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,\n",
      "Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong\n",
      "Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao\n",
      "Dong, and Jie Tang. GLM-130b: An open bilingual pre-\n",
      "trained model. In Proceedings of the International Confer-\n",
      "ence on Learning Representations (ICLR), 2023. 2\n",
      "[169] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\n",
      "Lucas Beyer. Sigmoid loss for language image pre-training,\n",
      "2023. 2\n",
      "[170] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang,\n",
      "and Jiaqi Wang. Long-CLIP: Unlocking the long-text capa-\n",
      "bility of clip. arXiv preprint arXiv:2403.15378, 2024. 2\n",
      "[171] Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang,\n",
      "Shoubin Yu, Mohit Bansal, and Gedas Bertasius. A simple\n",
      "llm framework for long-range video question-answering.\n",
      "arXiv preprint arXiv:2312.17235, 2023. 2\n",
      "[172] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\n",
      "instruction-tuned audio-visual language model for video\n",
      "understanding. arXiv preprint arXiv:2306.02858, 2023. 2\n",
      "[173] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu,\n",
      "Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang\n",
      "Zhang, Haodong Duan, Hang Yan, et al.\n",
      "Internlm-\n",
      "xcomposer: A vision-language large model for advanced\n",
      "text-image comprehension and composition. arXiv preprint\n",
      "arXiv:2309.15112, 2023. 1, 2, 8\n",
      "[174] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng,\n",
      "Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan,\n",
      "Chunyuan Li, and Ziwei Liu. Long context transfer from\n",
      "language to vision. arXiv preprint arXiv:2406.16852, 2024.\n",
      "6\n",
      "[175] Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan\n",
      "Li, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao,\n",
      "Mingkun Yang, et al. Icdar 2019 robust reading challenge\n",
      "on reading chinese text on signboard. In International Con-\n",
      "ference on Document Analysis and Recognition (ICDAR),\n",
      "2019. 8\n",
      "[176] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong\n",
      "Wu, Shunping Ji, Chen Change Loy, and Shuicheng Yan.\n",
      "Omg-llava: Bridging image-level, object-level, pixel-level\n",
      "reasoning and understanding, 2024. 2\n",
      "[177] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang,\n",
      "Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-\n",
      "hd: Diving into high-resolution large multimodal models,\n",
      "2024. 2\n",
      "[178] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai\n",
      "An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han,\n",
      "and Baobao Chang. Mmicl: Empowering vision-language\n",
      "model with multi-modal in-context learning.\n",
      "arXiv.org,\n",
      "2023. 2\n",
      "[179] Wenliang Zhao, Xumin Yu, and Zengyi Qin. Melotts: High-\n",
      "quality multi-lingual multi-accent text-to-speech, 2023. 2,\n",
      "7\n",
      "[180] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Ji-\n",
      "aqi Wang, and Conghui He. Beyond hallucinations: En-\n",
      "hancing lvlms through hallucination-aware direct prefer-\n",
      "ence optimization. arXiv preprint arXiv:2311.16839, 2023.\n",
      "6\n",
      "[181] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao,\n",
      "Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang,\n",
      "and Zheng Liu.\n",
      "MLVU: A comprehensive benchmark\n",
      "for multi-task long video understanding.\n",
      "arXiv preprint\n",
      "arXiv:2406.04264, 2024. 2, 9\n",
      "[182] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea\n",
      "Finn, and Huaxiu Yao. Aligning modalities in vision large\n",
      "language models via preference fine-tuning. arXiv preprint\n",
      "arXiv:2402.11411, 2024. 6\n",
      "[183] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\n",
      "Mohamed Elhoseiny.\n",
      "Minigpt-4:\n",
      "Enhancing vision-\n",
      "language understanding with advanced large language\n",
      "models. arXiv.org, 2023. 1, 2\n",
      "18\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# URL of the webpage containing PDF links\n",
    "webpage_url = 'https://arxiv.org/list/cs.CV/recent?show=1'\n",
    "\n",
    "# Fetch webpage content\n",
    "response = requests.get(webpage_url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract the publication date\n",
    "date_element = soup.find('h3')\n",
    "publication_date = date_element.get_text(strip=True) if date_element else 'Unknown date'\n",
    "\n",
    "# Extract all titles, authors, and PDF URLs\n",
    "base_url = 'https://arxiv.org'\n",
    "papers = []\n",
    "\n",
    "# Find all entries\n",
    "entries = soup.find_all('dl')\n",
    "\n",
    "for entry in entries:\n",
    "    titles = entry.find_all('div', class_='list-title mathjax')\n",
    "    authors = entry.find_all('div', class_='list-authors')\n",
    "    pdf_links = entry.find_all('a', title='Download PDF')\n",
    "\n",
    "    for title, author, pdf_link in zip(titles, authors, pdf_links):\n",
    "        paper_title = title.get_text(strip=True).replace('Title:', '').strip()\n",
    "        paper_authors = [a.get_text(strip=True) for a in author.find_all('a')]\n",
    "        paper_pdf_url = base_url + pdf_link['href']\n",
    "        papers.append({\n",
    "            'title': paper_title,\n",
    "            'authors': paper_authors,\n",
    "            'pdf_url': paper_pdf_url,\n",
    "            'date': publication_date\n",
    "        })\n",
    "\n",
    "def extract_text_from_pdf(paper):\n",
    "    text = f\"publication date of {paper['title']}: {paper['date']}\\nTitle: {paper['title']}\\nAuthors of {paper['title']}: {', '.join(paper['authors'])}\\n\\n\"\n",
    "    try:\n",
    "        # Download PDF or directly process it\n",
    "        response = requests.get(paper['pdf_url'], stream=True)\n",
    "        document = fitz.open(stream=response.content, filetype=\"pdf\")\n",
    "\n",
    "        for page_num in range(len(document)):\n",
    "            page = document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "\n",
    "    except Exception as e:\n",
    "        text += f\"Error processing PDF at {paper['pdf_url']}: {e}\"\n",
    "\n",
    "    return text\n",
    "\n",
    "# Extract text from each PDF and print it along with title, authors, and date\n",
    "for paper in papers:\n",
    "    extracted_text = extract_text_from_pdf(paper)\n",
    "    print(extracted_text)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "extracted_texts=[extracted_text]\n",
    "# Split text into manageable chunks/documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    \n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200, # Adjust overlap size based on requirements\n",
    ")\n",
    "\n",
    "documents = []\n",
    "for text in extracted_texts:\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    documents.extend(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "# Initialize the embeddings model from HuggingFace\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "# Create a FAISS vector store from documents and embeddings\n",
    "vector = FAISS.from_texts(documents, embeddings)\n",
    "# Create a retriever from the vector store\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "# Initialize the Ollama language model\n",
    "llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# Initialize the output parser for string outputs\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# Define the system instruction for reformulating the user's question\n",
    "\n",
    "instruction_to_system = \"\"\"\n",
    "Given a chat history and the latest user question \n",
    "which might reference context in the chat history, formulate a standalone question \n",
    "which can be understood without the chat history. Do NOT answer the question, \n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "# Create a prompt template for reformulating questions\n",
    "question_maker_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", instruction_to_system),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Define a chain that reformulates the question if needed\n",
    "question_chain = question_maker_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the question-answering assistant\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, provide a summary of the context. Do not generate your answer.\\\n",
    "{context}\"\"\"\n",
    "\n",
    "# Create a prompt template for the question-answering task\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine if question needs reformulation based on chat history\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return question_chain\n",
    "    else:\n",
    "        return input[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# Create a retriever chain to fetch relevant context for the question\n",
    "retriever_chain = RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever #| format_docs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Retrieval-Augmented Generation (RAG) chain\n",
    "rag_chain = (\n",
    "    retriever_chain\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, the publication date of InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output is:\n",
      "\n",
      "Thu, 4 Jul 2024\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = \"publication date of InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output\"\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "#Invoke the RAG chain with the question and chat history, and update chat history with responses\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"authors \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the answer to your question is:\n",
      "\n",
      "The publication date of InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output is Thu, 4 Jul 2024.\n",
      "\n",
      "Additionally, according to the context, the authors of this document are:\n",
      "\n",
      "Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, Jiaqi Wang\n"
     ]
    }
   ],
   "source": [
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I can summarize this paper as follows:\n",
      "\n",
      "The paper introduces InternLM-XComposer-2.5 (IXC-2.5), a versatile large vision language model that supports long-contextual input and output. The authors highlight two advantages of IXC-2.5: its versatility in supporting various tasks related to comprehension and composition, such as free-form text-image conversation, OCR, video understanding, article composition with illustrations, and webpage crafting; and its ability to handle long-term human-AI interaction and content creation through its long-contextual capabilities.\n",
      "\n",
      "The paper also presents the evaluation results of IXC-2.5 on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. Additionally, it compares IXC-2.5's performance to GPT-4V and Gemini Pro on 16 key tasks, showing that IXC-2.5 matches or surpasses these models in many cases.\n",
      "\n",
      "The authors conclude by emphasizing the potential of open-source LVLMs like IXC-2.5 to bridge the gap between proprietary APIs and open-sourced large vision language models, enabling more diverse and effective applications for AI-powered content creation and interaction.\n"
     ]
    }
   ],
   "source": [
    "question = \"great summarize this paper\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Function to handle the chat interaction\n",
    "def chat_complete(message, state):\n",
    "    if state is None:\n",
    "        state = []\n",
    "    ai_msg = rag_chain.invoke({\"question\": message, \"chat_history\": state})\n",
    "    state.append({\"role\": \"user\", \"content\": message})\n",
    "    state.append({\"role\": \"assistant\", \"content\": ai_msg})\n",
    "    response = [(msg[\"content\"], state[i+1][\"content\"]) for i, msg in enumerate(state) if msg[\"role\"] == \"user\"]\n",
    "    return response, state\n",
    "\n",
    "# Define the Gradio interface\n",
    "with gr.Blocks() as block:\n",
    "    gr.Markdown(\"\"\"<h1><center> EduVisionBot </center></h1>\"\"\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    message = gr.Textbox(placeholder=\"Type your Message.........\")\n",
    "    state = gr.State([])\n",
    "    submit = gr.Button(\"SEND\")\n",
    "    \n",
    "    submit.click(chat_complete, inputs=[message, state], outputs=[chatbot, state])\n",
    "block.launch(debug=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
