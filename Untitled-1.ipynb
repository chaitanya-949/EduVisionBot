{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# URL of the webpage containing PDF links\n",
    "webpage_url = 'https://arxiv.org/list/cs.CV/recent?show=1'# Here we can adjust how many pdfs to extract for now i just kept it as 1\n",
    "\n",
    "# Fetch webpage content\n",
    "response = requests.get(webpage_url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract all PDF URLs\n",
    "pdf_urls = []\n",
    "base_url = 'https://arxiv.org'\n",
    "\n",
    "# Find all 'a' tags with title 'Download PDF' and extract href attributes\n",
    "for a_tag in soup.find_all('a', title='Download PDF'):\n",
    "    pdf_link = a_tag['href']\n",
    "    pdf_urls.append(base_url + pdf_link)\n",
    "\n",
    "def extract_text_from_pdf(pdf_url):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        #  PDF  processing\n",
    "        response = requests.get(pdf_url, stream=True)\n",
    "        document = fitz.open(stream=response.content, filetype=\"pdf\")\n",
    "        \n",
    "        for page_num in range(len(document)):\n",
    "            page = document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF at {pdf_url}: {e}\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Extract text from each PDF and print it\n",
    "for pdf_url in pdf_urls:\n",
    "    extracted_text = extract_text_from_pdf(pdf_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "extracted_texts=[extracted_text]\n",
    "# Split text into manageable chunks/documents\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",  # Use newline as the separator for paragraphs\n",
    "    chunk_size=1000,  # Adjust chunk size based on  requirements\n",
    "    chunk_overlap=200  # Adjust overlap size based on requirements\n",
    ")\n",
    "\n",
    "documents = []\n",
    "for text in extracted_texts:\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    documents.extend(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "# Initialize the embeddings model from HuggingFace\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "# Create a FAISS vector store from documents and embeddings\n",
    "vector = FAISS.from_texts(documents, embeddings)\n",
    "# Create a retriever from the vector store\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "# Initialize the Ollama language model\n",
    "llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# Initialize the output parser for string outputs\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# Define the system instruction for reformulating the user's question\n",
    "\n",
    "instruction_to_system = \"\"\"\n",
    "Given a chat history and the latest user question \n",
    "which might reference context in the chat history, formulate a standalone question \n",
    "which can be understood without the chat history. Do NOT answer the question, \n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "# Create a prompt template for reformulating questions\n",
    "question_maker_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", instruction_to_system),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Define a chain that reformulates the question if needed\n",
    "question_chain = question_maker_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the question-answering assistant\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, provide a summary of the context. Do not generate your answer.\\\n",
    "{context}\"\"\"\n",
    "\n",
    "# Create a prompt template for the question-answering task\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine if question needs reformulation based on chat history\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return question_chain\n",
    "    else:\n",
    "        return input[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# Create a retriever chain to fetch relevant context for the question\n",
    "retriever_chain = RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever #| format_docs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Retrieval-Augmented Generation (RAG) chain\n",
    "rag_chain = (\n",
    "    retriever_chain\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it appears that the authors of this paper have not been explicitly mentioned. The text does not include a list of authors or their affiliations.\n"
     ]
    }
   ],
   "source": [
    "question = \"there are some authors for this particular paper please analyze and find who are they?\"\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "chat_history = []\n",
    "#Invoke the RAG chain with the question and chat history, and update chat history with responses\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"not specific authors there are few persons please find them\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the context:\n",
      "\n",
      "The provided context is from a research paper discussing various topics such as ward modeling, preference data collection, and DPO alignment for high-quality article generation. The paper also mentions instruction-aware webpage generation, chatbots, and other AI-related concepts.\n",
      "\n",
      "There are no specific authors mentioned in the provided text.\n"
     ]
    }
   ],
   "source": [
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the context, Supervised Fine-tuning (SFT) is a model that begins with an initial model π and uses instruction tuning data from IXC2 [33] focused on article writing. The SFT model rewrites original prompts using the Chain-of-Thought (CoT) technique [152], generating step-by-step prompts to supplement the instruction tuning data as augmented data D∗.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"yes you are correct ok lemme know what is supervised fine tuning??\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Function to handle the chat interaction\n",
    "def chat_complete(message, state):\n",
    "    if state is None:\n",
    "        state = []\n",
    "    ai_msg = rag_chain.invoke({\"question\": message, \"chat_history\": state})\n",
    "    state.append({\"role\": \"user\", \"content\": message})\n",
    "    state.append({\"role\": \"assistant\", \"content\": ai_msg})\n",
    "    response = [(msg[\"content\"], state[i+1][\"content\"]) for i, msg in enumerate(state) if msg[\"role\"] == \"user\"]\n",
    "    return response, state\n",
    "\n",
    "# Define the Gradio interface\n",
    "with gr.Blocks() as block:\n",
    "    gr.Markdown(\"\"\"<h1><center> EduVisionBot </center></h1>\"\"\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    message = gr.Textbox(placeholder=\"Type your Message.........\")\n",
    "    state = gr.State([])\n",
    "    submit = gr.Button(\"SEND\")\n",
    "    \n",
    "    submit.click(chat_complete, inputs=[message, state], outputs=[chatbot, state])\n",
    "block.launch(debug=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
