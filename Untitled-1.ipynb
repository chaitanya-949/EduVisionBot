{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publication date of BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations is Thu, 4 Jul 2024 (continued, showing 1 of 115 entries )\n",
      "Title BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations\n",
      "Authors of BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations Zhantao Yang, Ruili Feng, Keyu Yan, Huangji Wang, Zhicai Wang, Shangwen Zhu, Han Zhang, Jie Xiao, Pingyu Wu, Kai Zhu, Jixuan Chen, Chen-Wei Xie, Chaojie Mao, Yue Yang, Hongyang Zhang, Yu Liu, Fan Cheng\n",
      "\n",
      "BACON: Supercharge Your VLM with Bag-of-Concept Graph\n",
      "to Mitigate Hallucinations\n",
      "Zhantao Yang1,2‚ãÜ, Ruili Feng2‚ãÜ‚ãÑ, Keyu Yan2, Huangji Wang1, Zhicai Wang2\n",
      "Shangwen Zhu1, Han Zhang1,2, Jie Xiao2, Pingyu Wu2, Kai Zhu2, Jixuan Chen2\n",
      "Chen-Wei Xie2, Chaojie Mao2, Yue Yang3, Hongyang Zhang4, Yu Liu2, Fan Cheng1‚Ä†\n",
      "1Shanghai Jiao Tong University, 2Alibaba group\n",
      "3University of Pennsylvania, 4University of Waterloo\n",
      "https://ztyang23.github.io/bacon-page\n",
      "Abstract\n",
      "This paper presents Bag-of-Concept Graph\n",
      "(BACON) to gift models with limited linguistic\n",
      "abilities to taste the privilege of Vision Lan-\n",
      "guage Models (VLMs) and reduce hallucina-\n",
      "tions in the downstream tasks such as detection,\n",
      "visual question answering (VQA), and image\n",
      "generation. Since the visual scenes in phys-\n",
      "ical worlds are structured with complex rela-\n",
      "tions between objects, BACON breaks down\n",
      "annotations into basic minimum elements and\n",
      "presents them in a graph structure. Element-\n",
      "wise style enables easy understanding, and\n",
      "structural composition liberates difficult locat-\n",
      "ing.\n",
      "Careful prompt design births the BA-\n",
      "CON captions with the help of public-available\n",
      "VLMs and segmentation methods. In this way,\n",
      "we gather a dataset with 100K annotated im-\n",
      "ages, which endow VLMs with remarkable ca-\n",
      "pabilities, such as accurately generating BA-\n",
      "CON, transforming prompts into BACON for-\n",
      "mat, envisioning scenarios in the style of BA-\n",
      "CON, and dynamically modifying elements\n",
      "within BACON through interactive dialogue and\n",
      "more.\n",
      "Wide representative experiments, in-\n",
      "cluding detection, VQA, and image generation\n",
      "tasks, tell BACON as a lifeline to achieve previ-\n",
      "ous out-of-reach tasks or excel in their current\n",
      "cutting-edge solutions.\n",
      "1\n",
      "Introduction\n",
      "A single image can tell long stories, weaving\n",
      "tales as those told across myriad pages, akin\n",
      "to the masterful strokes seen in ‚ÄúThe Last Sup-\n",
      "per‚Äù (Pitre, 2015) or the vibrant dynamism of ‚ÄúLib-\n",
      "erty Leading the People‚Äù (Chaulagain, 2018). Yet\n",
      "emerging multi-modality tasks feed on describ-\n",
      "ing images with language, which is bound to be\n",
      "lengthy and convoluted. Though Vision Language\n",
      "Models (VLMs) (OpenAI, 2023) can now easily\n",
      "‚Ä† Corresponding author, ‚ãÜEqual contribution, ‚ãÑProject\n",
      "leader\n",
      "Part I: Overall description\n",
      "Part1.1: Style - color painting with a dynamic \n",
      "and dramatic composition.\n",
      "Part1.2: Theme - historical event, specifically \n",
      "a depiction of a revolution or uprising.\n",
      "Part1.3: Global description of background - \n",
      "The painting is chaotic and filled with a \n",
      "multitude of figures, suggesting a crowded and \n",
      "tumultuous setting, ‚Ä¶\n",
      "Part1.4: Global description of foreground - \n",
      "The painting is dominated by a central female \n",
      "figure, who stands out prominently against the \n",
      "crowded and violent backdrop, ...\n",
      "Image\n",
      "BACON\n",
      "Part II: Object List\n",
      "<Woman>,<flag>,<Crowd>,<Man1>, \n",
      "<Man2>,<Man3>, ‚Ä¶\n",
      "An example: <Woman>\n",
      "- Category1: Living \n",
      "- Category2: Foreground\n",
      "- Description: The woman's arms are raised \n",
      "high, holding a flag, She is dressed in a white \n",
      "robe and appears to be leading the crowd, ‚Ä¶\n",
      "- Color: white robe\n",
      "Part III: Relationships\n",
      "Man2\n",
      "Crowd\n",
      "Woman\n",
      "Man3\n",
      "Man1\n",
      "leads\n",
      "Building\n",
      "Child\n",
      "Fallen figure 1\n",
      "Fallen figure 2\n",
      "stands behind\n",
      "is positioned\n",
      "to the left of\n",
      "is positioned\n",
      "to the right of\n",
      "is lying among\n",
      "is lying among\n",
      "surrounds\n",
      "is above\n",
      "is behind\n",
      "Flag\n",
      "is holding\n",
      "Figure 1: BACON representation of an image, including\n",
      "overall description, object list, and relationships.\n",
      "produce those detailed annotations, downstream\n",
      "multi-modality applications may not easily process\n",
      "them‚Äîunderstanding long and intricate context is\n",
      "still a privilege of VLMs and is far beyond the\n",
      "ability of most popular multi-modality methods,\n",
      "like those ResNet-based models (He et al., 2016a;\n",
      "Huang et al., 2017; He et al., 2016b), segmentation\n",
      "methods (Kirillov et al., 2023; Liu et al., 2023b;\n",
      "Ke et al., 2024), and Contrastive Language-Image\n",
      "Pretraining methods (CLIPs) (Radford et al., 2021;\n",
      "Lee et al., 2022; Li et al., 2021). Thus, popular\n",
      "multi-modality methods usually suffer from model\n",
      "hallucinations, where predictions from these meth-\n",
      "ods are inconsistent with input prompts. For exam-\n",
      "ple, off-the-shelf image generative models such as\n",
      "DALL-E 3 and SDXL cannot even count apples cor-\n",
      "rectly and do not understand position relationships\n",
      "between objects, generating inconsistent images\n",
      "with human-provided captions (see Figure 5).\n",
      "Challenges stem from two main issues: the\n",
      "complexity of long phrases and the difficulty in\n",
      "pinpointing specific information. Smaller mod-\n",
      "1\n",
      "arXiv:2407.03314v1  [cs.CV]  3 Jul 2024\n",
      "‚ë† Point question answering\n",
      "‚ë° Image generation\n",
      "In an exploding nebula, there is \n",
      "an erupting volcano with a \n",
      "cyberpunk style building to the \n",
      "left of the volcano. In the \n",
      "bottom right corner of the scene \n",
      "is a green river flowing with \n",
      "venom. There is a yellow duck \n",
      "on the river, and to the left of \n",
      "the river stands a crowned \n",
      "potato man. On the right bank \n",
      "of the river, there is a miniature \n",
      "version of the village.\n",
      "Captioner\n",
      "‚ùÑ\n",
      "üåã\n",
      "LLaVA\n",
      "What color is this table? \n",
      "Question\n",
      "bbox: \n",
      "[320, 0, 440, 70]\n",
      "Target region\n",
      "Obj1. Apple 1. \n",
      "  \n",
      "Description: The apple is of ‚ÄòFuji‚Äô‚Ä¶\n",
      "Color: red.\n",
      "Obj2. Apple Tray 1. \n",
      "Description: The apple tray holds ‚Ä¶\n",
      "Color: a mix of red and yellow.\n",
      "Obj3. Table 2. \n",
      "   \n",
      "Description: The table provides ...\n",
      "Color: brown.\n",
      "Large IOU\n",
      "Brown\n",
      "Answer\n",
      "SDXL\n",
      "BACON\n",
      "Wooden Structure\n",
      "Sign\n",
      "Apple\n",
      " Tray\n",
      "Person\n",
      "Display Board\n",
      "in front of\n",
      "is placed on\n",
      "is reaching towards\n",
      "supports\n",
      "serves as a\n",
      "backdrop for\n",
      "‚ë¢ Open-vocabulary scene graph generation\n",
      "BACON\n",
      "BACON\n",
      "Figure 2: Schematic diagram of multiple exemplary downstream tasks can benefit from BACON. Specifically,\n",
      "BACON can (1) enable VLMs to carry out the point question answering task previously beyond their scope; (2)\n",
      "assist text-to-image generative models such as SDXL in creating intricate images with higher precision as demanded\n",
      "by prompts; and (3) execute open-vocabulary scene graph generation tasks that were not feasible for other VLMs.\n",
      "els like CLIPs and Segment Anything Models\n",
      "(SAMs) (Kirillov et al., 2023), constrained by their\n",
      "training data and resources, grasp only word-level\n",
      "or phase-level language and struggle to train ex-\n",
      "tensive attention mechanisms for detailed context\n",
      "analysis. Thus, a critical question emerges:\n",
      "Shall the meticulous breakdown of VLM annota-\n",
      "tions into clearly distinguishable and comprehensi-\n",
      "ble elements significantly mitigate model hallucina-\n",
      "tions and elevate the performance of downstream\n",
      "tasks?\n",
      "Luckily, the answer is yes. This paper proposes\n",
      "BACON‚Äîa neat and efficient approach for VLMs\n",
      "to mitigate hallucinations and boost downstream\n",
      "tasks. BACON breaks down annotations into ba-\n",
      "sic elements (like objects, style of picture, rela-\n",
      "tionships, etc.) within an image, which can be\n",
      "easily understood by various methods. After that,\n",
      "In-Context Learning (ICL) (Brown et al., 2020)\n",
      "techniques are deployed to organize the annotation\n",
      "into a graph, with nodes representing fundamental\n",
      "visual elements and edges illustrating their connec-\n",
      "tions. Illustrated in Figure 1, BACON composes\n",
      "of three key parts: 1) an overall description cap-\n",
      "turing the image‚Äôs overall style, theme, and key\n",
      "features; 2) a detailed object list with labels and\n",
      "descriptions for every item in the image; 3) the re-\n",
      "lationships between these objects. The structure of\n",
      "BACON allows models to flexibly extract and use\n",
      "the desired parts to comprehend complex informa-\n",
      "tion and accomplish tasks previously beyond their\n",
      "reach (see detailed examples as Figure 2).\n",
      "In conclusion, the contribution of this work can\n",
      "be summarized as follows. 1) We introduce BA-\n",
      "CON, a rich and digestible caption method for\n",
      "boosting multi-modality tasks; 2) We collect a high-\n",
      "quality BACON-Dataset using human labor and\n",
      "advanced VLM models; the dataset and BACON-\n",
      "Captioner trained on it will be soon open-source; 3)\n",
      "The proposed method boosts wide and representa-\n",
      "tive downstream tasks remarkably, demonstrating\n",
      "significant potential in promoting future research.\n",
      "2\n",
      "Bag-of-Concept Graph\n",
      "In this section, we first explain the design of Bag-of-\n",
      "Concept Graph in Section 2.1, and then introduce\n",
      "how to get a BACON representation from an image\n",
      "in Section 2.2.\n",
      "2.1\n",
      "Design of BACON\n",
      "BACON aims to tackle the issues faced by smaller\n",
      "downstream models in understanding long phrases\n",
      "and in locating specific pieces of information. For\n",
      "the former, BACON deconstructs the image an-\n",
      "notations into basic elements, ensuring that even\n",
      "smaller downstream models can fully comprehend\n",
      "them. Subsequently, BACON employs a specific\n",
      "graph structure to amalgamate these elements, en-\n",
      "suring each element appears in a designated spot,\n",
      "allowing smaller downstream models to query and\n",
      "retrieve them easily. Specifically, the real world can\n",
      "often be explicitly represented using a scene graph\n",
      "composed of objects and their relationships (Miller,\n",
      "1995; Doddington et al., 2004; Krishna et al., 2017;\n",
      "2\n",
      "man\n",
      "GroundingDINO\n",
      "+CLIP\n",
      "+CLIP&LLaVA\n",
      "‚ë°Grounding\n",
      "DINO\n",
      "üåã\n",
      "‚ë¢LLaVA\n",
      "(b) Ablation study\n",
      "(a) Detailed method of enhancing BACON\n",
      "‚ë†BACON\n",
      "‚ë£CLIP\n",
      "mIOU\n",
      "man\n",
      "Object description: \n",
      "He is wearing a hat, ‚Ä¶\n",
      "Object list\n",
      "Man \n",
      "1\n",
      "Man \n",
      "2\n",
      "Man \n",
      "3\n",
      "Sofa SUV ‚Ä¶\n",
      "The caption of this image is\n",
      "He is wearing a hat, ‚Ä¶, \n",
      "does it correct?\n",
      "Figure 3: (a) Detailed method for graph grounding. The method contains four steps: 1) Extracting BACON from\n",
      "images using GPT-4V or BACON-Captioner; 2) Getting candidate regions using Grounding DINO given the name\n",
      "of the object; 3) Using LLaVA to discard blatant incorrect regions; 4) Select the region whose image feature\n",
      "matches the text feature of object description the most by CLIP. (b) Ablation study of method in (a), exploring the\n",
      "improvement of introducing CLIP and LLaVA, where the experiment is conducted on BACON benchmark.\n",
      "Lu et al., 2016; Xu et al., 2017; Johnson et al., 2015,\n",
      "2018). Inspired by this insight, we adopt the struc-\n",
      "ture of a graph to deconstruct and reassemble basic\n",
      "elements. When selecting these basic elements, we\n",
      "have deliberately considered those that may ben-\n",
      "efit downstream tasks (listed in Appendix A.2.1).\n",
      "Finally, we have obtained BACON as shown in Fig-\n",
      "ure 1.\n",
      "Surprisingly, we discover that such a design\n",
      "also significantly increases the number of cor-\n",
      "rect objects included in captions (see the higher\n",
      "recall score of BACON compared to GPT-4V in Sec-\n",
      "tion 4.2.2). We believe this is due to the structured\n",
      "object list emphasizing the distinction of separated\n",
      "entities, urging VLMs to separate complex objects\n",
      "into more detailed components.\n",
      "2.2\n",
      "Formulation of BACON\n",
      "Given an image I, we aim to induce a structural\n",
      "representation G = (D, O, R, B), where D is the\n",
      "textual description, O is the list of objects in the\n",
      "image, with R denotes their relationships and B\n",
      "as their bounding box positions. In practice, we\n",
      "optimize the construction of G in two stages: (1)\n",
      "Graph Construction (Section 2.3), which utilizes\n",
      "VLMs to generate the graph elements (D, O, R)\n",
      "from the image, and (2) Graph Grounding (Sec-\n",
      "tion 2.4) aligns the graph elements with the bound-\n",
      "ing boxes (B) extracted by a grounding model.\n",
      "2.3\n",
      "Graph construction\n",
      "Deconstructing annotations.\n",
      "BACON assists\n",
      "downstream models in understanding complex\n",
      "texts by decomposing the annotations of VLMs into\n",
      "basic elements and then combining them according\n",
      "to a specific structure. Our approach is divided\n",
      "into two key parts. Firstly, we design a reversible\n",
      "transformation method that converts BACON into a\n",
      "string format understandable by VLMs. Illustrative\n",
      "examples are provided in Appendix A.2.2. Then,\n",
      "we employ the ICL technique to teach VLMs to\n",
      "output following the string format we designed. We\n",
      "find a few simplified examples sufficient and exe-\n",
      "cute the ICL learning process in one conversational\n",
      "exchange. In practice, we use GPT-4V as the VLM\n",
      "and provide the final instruction in Appendix A.2.2.\n",
      "Based on this approach, we develop the BACON\n",
      "dataset, detailed in Section 3.\n",
      "BACON-Captioner. Obtaining BACON by GPT-\n",
      "4V is reliable but expensive. Therefore, we opt\n",
      "to fine-tune a 13B LLaVA model on the BACON\n",
      "dataset to serve as a specialized captioner, with\n",
      "training details in Section A.4.1. Our goal for this\n",
      "captioner is to match GPT-4V‚Äôs capability in gen-\n",
      "erating BACON. We gather BACON outputs from\n",
      "BACON-Captioner and GPT-4V on a test set, an-\n",
      "alyzing the categories each detects and the root\n",
      "nouns and verbs of their outputs. The resulting dis-\n",
      "tributions, displayed in Figure 9), show a high de-\n",
      "gree of similarity between them. Furthermore, the\n",
      "precision and recall score calculated by manual an-\n",
      "notation (the metrics are detailed as Section 4.2.2)\n",
      "show BACON-Captioner achieve 91% of precision\n",
      "score and 90% of recall score of that of GPT-4V.\n",
      "Consequently, BACON-Captioner is a viable alter-\n",
      "native to GPT-4V for producing BACON and helps\n",
      "us extend BACON dataset.\n",
      "Beyond generating Bacon from images, we sur-\n",
      "3\n",
      "prisingly find that the trained captioner is also adept\n",
      "at performing additional useful tasks without fine-\n",
      "tuning, such as interactively modifying the items\n",
      "of BACON, transforming prompts into BACON for-\n",
      "mat, envisioning scenarios in the style of BACON.\n",
      "2.4\n",
      "Graph grounding\n",
      "Space information is important for representing an\n",
      "image. While VLMs have some level of ground-\n",
      "ing, specialized models like Grounding DINO (Liu\n",
      "et al., 2023b) significantly outperform them. There-\n",
      "fore, we did not pursue the option of VLMs in\n",
      "Section 2.3 delivering location details, but rather,\n",
      "we intend to leverage dedicated grounding mod-\n",
      "els for this purpose. Fortunately, BACON‚Äôs struc-\n",
      "ture provides a list of objects required by ground-\n",
      "ing models, enabling the combination of advanced\n",
      "VLMs for detailing and top-tier grounding models\n",
      "for precise localization within BACON.\n",
      "Although Grounding DINO provides accurate\n",
      "object positions, names alone fall short of distin-\n",
      "guishing objects within the same category. Here,\n",
      "BACON‚Äôs detailed node descriptions come into play,\n",
      "allowing for precise region identification when\n",
      "used in conjunction with CLIP. Moreover, we en-\n",
      "hance grounding accuracy by first applying LLaVA\n",
      "to filter out incorrect bounding boxes before pro-\n",
      "ceeding with the CLIP step. We conducted an ab-\n",
      "lation study on the BACON benchmark (with de-\n",
      "tails in Section 3), and the findings, presented in\n",
      "Figure 3 (b), confirm the benefits of incorporating\n",
      "CLIP and LLaVA into our approach. See Figure 3\n",
      "(a) for an illustration of this process.\n",
      "3\n",
      "BACON dataset\n",
      "In this section, we introduce the BACON dataset\n",
      "proposed in this paper. BACON dataset is composed\n",
      "of two parts, the training set and the test benchmark,\n",
      "which share different collection methods.\n",
      "Training set. For the training set, we employ the\n",
      "method as detailed in Section 2.3 to collect 110k\n",
      "BACON-image pairs. Then, we engage in a thor-\n",
      "ough manual re-annotation process to eliminate\n",
      "ambiguities and incorrectness (see details in Ap-\n",
      "pendix A.3). The end product is a refined dataset\n",
      "of 100k high-quality image-BACON pairs. Notably,\n",
      "BACON‚Äôs structure greatly streamlines the annota-\n",
      "tion process. By breaking down GPT-4V‚Äôs output\n",
      "into manageable sections, we can assign specific\n",
      "segments to different annotators, which concen-\n",
      "trates their efforts and enhances productivity. Of-\n",
      "Table 1: Comparison of open-vocabulary object\n",
      "detection among BACON, Grounding DINO, open-\n",
      "vocabulary object detection models, and grounding cap-\n",
      "tion models on BACON benchmark. GD represents\n",
      "Grounding DINO. We have calculated error bars for\n",
      "models that exhibit randomness.\n",
      "Method\n",
      "AP50(‚Üë)\n",
      "Recall(‚Üë)\n",
      "mIOU(‚Üë)\n",
      "OV-DQUO\n",
      "4.7%\n",
      "10.7%\n",
      "66.5%\n",
      "DE-VIT\n",
      "19.3%\n",
      "23.8%\n",
      "76.8%\n",
      "GD\n",
      "33.1¬±2.5%\n",
      "20.2¬±0.1%\n",
      "75.7¬±0.1%\n",
      "Next-Chat\n",
      "29.1¬±0.1%\n",
      "7.7¬±0.1%\n",
      "67.1¬±0.0%\n",
      "Kosmos-2\n",
      "34.2¬±4.8%\n",
      "13.3¬±2.4%\n",
      "76.1¬±0.4%\n",
      "GLaMM\n",
      "34.3%\n",
      "19.8%\n",
      "79.6%\n",
      "BACON\n",
      "37.7 ¬± 0.9% 35.9 ¬± 0.7% 79.9 ¬± 0.1%\n",
      "ten, their tasks involve simple verifications like\n",
      "checking colors or relationships. Furthermore, the\n",
      "comprehensive nature of BACON generally spares\n",
      "annotators from generating new information, allow-\n",
      "ing them to focus on editing or deleting existing\n",
      "content‚Äîtasks that are typically less complex.\n",
      "Test benchmark. Despite the existence of graph\n",
      "datasets like Visual-Genome (VG) (Krishna et al.,\n",
      "2017) and PSG (Yang et al., 2022), there is still\n",
      "no dataset that concurrently offers open-vocabulary\n",
      "capabilities, detailed object attributes, and a com-\n",
      "prehensive overall description. To address this defi-\n",
      "ciency, we develop the BACON benchmark.\n",
      "Despite GPT-4V‚Äôs advanced capabilities, it may\n",
      "miss objects in images. For the test benchmark,\n",
      "aiming for the utmost accuracy, we utilize a rigor-\n",
      "ous method that relies heavily on human annota-\n",
      "tors in contrast to the training set. This method, as\n",
      "shown in Figure 11, starts by using SAM (Kirillov\n",
      "et al., 2023) to separate each object in an image.\n",
      "Next, it applies VLMs to describe all objects, list\n",
      "their attributes, and explain how these objects in-\n",
      "teract with each other. Finally, given the object list,\n",
      "VLM provides the overall description of the image.\n",
      "We leave the details in the appendix. Thanks to\n",
      "SAM‚Äôs excellent performance, it is rare to miss\n",
      "objects in the image. The challenge comes with\n",
      "VLM‚Äôs difficulty in recognizing objects given their\n",
      "masked images, which is still hard even for ad-\n",
      "vanced VLMs like GPT-4V (OpenAI, 2023). This\n",
      "difficulty leads to a lot of manual annotation to\n",
      "correct mistakes. The high cost of manual correc-\n",
      "tions is the reason why this method is not applied\n",
      "to create our training set. Finally, we annotated a\n",
      "test benchmark containing around 3k images, 27k\n",
      "objects, and 148k relationships.\n",
      "4\n",
      "GLaMM\n",
      "Kosmos-2\n",
      "Next-Chat\n",
      "BACON\n",
      "PointQA\n",
      "*\n",
      "v7w\n",
      "GLaMM\n",
      "Kosmos-2\n",
      "Next-Chat\n",
      "BACON\n",
      "random\n",
      "*\n",
      "Accuracy \n",
      "Accuracy \n",
      "40\n",
      "0\n",
      "45\n",
      "0\n",
      "(a) PointQA task\n",
      "(b) PointingQA task\n",
      "Figure 4: Quantitative comparison on (a) PointQA\n",
      "and (b) PointingQA between BACON and baselines.\n",
      "Table 2:\n",
      "Comparison on open-vocabulary scene\n",
      "graph generation task between BACON and multiple\n",
      "baselines on VG dataset and BACON benchmark. The\n",
      "number of correct predictions is used as the metric (‚Üë).\n",
      "Method\n",
      "Visual-Genome BACON dataset\n",
      "PSGFormer\n",
      "1.45K\n",
      "0.21K\n",
      "PSGTR\n",
      "0.33K\n",
      "0.07K\n",
      "IMP\n",
      "3.16K\n",
      "1.28K\n",
      "Gps-net\n",
      "3.13K\n",
      "1.16K\n",
      "Motifs\n",
      "3.05K\n",
      "1.25K\n",
      "VCTree\n",
      "3.11K\n",
      "1.24K\n",
      "BACON\n",
      "11.45K\n",
      "1.44K\n",
      "4\n",
      "Experiments\n",
      "In this section, we show that BACON can be ap-\n",
      "plied to help multiple downstream tasks by flexibly\n",
      "utilizing desired parts of information and the re-\n",
      "markable capabilities of BACON-Captioner. First,\n",
      "in Section 4.1, we show BACON can help a wide\n",
      "range of downstream tasks by flexibly utilizing de-\n",
      "sired parts of BACON. Then, for completeness, we\n",
      "discuss some special cases in Section 4.2 where\n",
      "BACON can be directly utilized without any special\n",
      "operations. Next, in Section 4.3, we show some\n",
      "impressive capabilities of BACON-Captioner. Fi-\n",
      "nally, in Section 4.4, we show BACON can be easily\n",
      "generalized to video dense-captioning task.\n",
      "4.1\n",
      "Downstream tasks benefiting from BACON\n",
      "The adaptable nature of BACON‚Äôs structure en-\n",
      "hances models‚Äô comprehension of complex text\n",
      "and empowers them to undertake tasks previ-\n",
      "ously beyond their reach. We conduct evaluations\n",
      "across five downstream tasks, including object de-\n",
      "tection (Section 4.1.1), point question answering\n",
      "(PointQA) (Section 4.1.2), Pointing question an-\n",
      "swering (PointingQA) (Section 4.1.3), scene graph\n",
      "generation (SGG) (Section 4.1.4), and image gen-\n",
      "eration (Section 4.1.5).\n",
      "Table 3:\n",
      "Quantitative comparison of VQA task\n",
      "between BACON and multiple VLM-based baselines,\n",
      "where the input image of the QA model is replaced by\n",
      "its caption to evaluate the performance of the captioner.\n",
      "The metric is the accuracy of answering questions (‚Üë).\n",
      "Method\n",
      "NLVR2 OK-VQA VQAv1 VQAv2\n",
      "LLaVA\n",
      "56.3 %\n",
      "30.9 %\n",
      "50.0 %\n",
      "64.1 %\n",
      "ShareGPT-4V\n",
      "57.5 %\n",
      "31.4 %\n",
      "50.7 %\n",
      "65.4 %\n",
      "Qwen-VL-max\n",
      "56.8 %\n",
      "29.0 %\n",
      "46.0 %\n",
      "65.6 %\n",
      "BACON\n",
      "59.1 %\n",
      "32.1 %\n",
      "52.6 %\n",
      "66.4 %\n",
      "4.1.1\n",
      "Open-vocabulary object detection\n",
      "Grounding DINO struggles to differentiate between\n",
      "multiple objects of the same category during open-\n",
      "vocabulary object detection (OVD) task (Exam-\n",
      "ples are shown in Appendix A.4.2).\n",
      "However,\n",
      "thanks to the structural features of BACON, the\n",
      "approach described in Section 2.4 can be lever-\n",
      "aged to address this issue, significantly enhancing\n",
      "Grounding DINO‚Äôs performance in OVD task. We\n",
      "compare the performance with grounding caption\n",
      "models, including GLaMM (Rasheed et al., 2024),\n",
      "Kosmos-2 (Peng et al., 2023), Next-Chat (Zhang\n",
      "et al., 2023), and two of the SOTA dedicated OVD\n",
      "models, OV-DQUO (Wang et al., 2024a) and DE-\n",
      "VIT (Zhang et al., 2024) as baselines on BACON\n",
      "benchmark, with AP50, recall and mIOU used as\n",
      "the metrics. Results in Table 1 show that BACON\n",
      "enables grounding models to perform OVD task\n",
      "and outperform all evaluated baselines.\n",
      "4.1.2\n",
      "Point question answering\n",
      "Point Question Answering (PointQA) (Mani et al.,\n",
      "2020) aims to answer region-related questions ac-\n",
      "cording to an input image. Inspired by the set-\n",
      "ting of zero-shot visual question answering (ZS-\n",
      "VQA) (Guo et al., 2023), we replace the input of\n",
      "the QA model from images to their captions to\n",
      "evaluate the performance of captioners. Intuitively,\n",
      "if a fixed QA model can correctly answer more\n",
      "questions using a given caption, it suggests that the\n",
      "caption contains more accurate information, indi-\n",
      "cating a superior captioning performance.\n",
      "This task is extremely difficult for VLMs like\n",
      "LLaVA because they can‚Äôt match parts of the cap-\n",
      "tion to image regions. But BACON aids LLaVA\n",
      "in finding relevant descriptions for a given area\n",
      "(we‚Äôve detailed the method in Appendix A.4.3). We\n",
      "use grounding caption models, including GLaMM,\n",
      "Kosmos-2, and Next-Chat, as baselines. Besides,\n",
      "we apply LLaVA as the fixed QA model and uti-\n",
      "lize the QA accuracy as the metric. Results on\n",
      "5\n",
      "Table 4: Accuracy in depicting objects (Ao) and\n",
      "relationships (Ar) in images generated from text\n",
      "prompts, as evaluated by human. We compare SDXL\n",
      "enhanced by BACON with SDXL and DALL-E 3.\n",
      "Method\n",
      "Ao(‚Üë)\n",
      "Ar(‚Üë)\n",
      "SDXL\n",
      "59.20¬±3.96%\n",
      "41.45¬±3.46%\n",
      "DALL-E 3\n",
      "90.05¬±4.17%\n",
      "71.60¬±3.39%\n",
      "BACON + SDXL 95.20 ¬± 1.13% 76.65 ¬± 0.92%\n",
      "LookTwice-QA dataset (Mani et al., 2020) shown\n",
      "in Figure 4 (a) indicate BACON can help VLMs\n",
      "associate descriptions with regions to perform\n",
      "PointQA tasks and surpass all baselines.\n",
      "4.1.3\n",
      "Pointing question answering\n",
      "Pointing Question Answering (PointingQA) (Zhu\n",
      "et al., 2016) asks models to choose the correct re-\n",
      "gion from various options based on a given ques-\n",
      "tion, using only the input image. Following the\n",
      "settings in Section 4.1.2, we replace the input\n",
      "from images to corresponding captions to eval-\n",
      "uate the performance of captioners. BACON can\n",
      "help LLaVA with the task, which is originally be-\n",
      "yond its capabilities (details in Appendix A.4.4).\n",
      "Again, we chose the grounding caption models,\n",
      "including GLaMM, Kosmos-2, and Next-Chat, as\n",
      "baselines and used the accuracy of selecting regions\n",
      "as the metric. Results on Visual-7W dataset (Zhu\n",
      "et al., 2016) in Figure 4 (b) show that BACON can\n",
      "help LLaVA find the correct region according to\n",
      "the requirement and significantly outperform the\n",
      "baselines, whose performances are even worse than\n",
      "randomly selecting.\n",
      "4.1.4\n",
      "Scene graph generation\n",
      "The scene graph generation (SGG) task focuses\n",
      "on identifying (subject-predicate-object) triplets\n",
      "in images. Traditional SGG models are trained\n",
      "to classify items in the triplet within a finite set\n",
      "of categories, which may not be suitable for real-\n",
      "world applications. However, the open-vocabulary\n",
      "(OV-SGG) setting, closer to the real situation, is\n",
      "extremely hard for those SGG models. Fortunately,\n",
      "BACON-Captioner can perform the OV-SGG task\n",
      "without any fine-tuning as BACON provides the\n",
      "relationships part. We compare the performance\n",
      "of OV-SGG between BACON-Captioner with mul-\n",
      "tiple specialized SGG approaches including Mo-\n",
      "tifs (Zellers et al., 2018), GPS-Net (Lin et al.,\n",
      "2020), VCTree (Tang et al., 2019), PSGTR, PS-\n",
      "GFormer (Yang et al., 2022), and IMP (Xu et al.,\n",
      "2017) on two datasets, VG (Krishna et al., 2017)\n",
      "(see details in Appendix A.4.5) and BACON bench-\n",
      "mark. To calculate the open-vocabulary metrics,\n",
      "we apply CLIP similarity to decide whether a pre-\n",
      "diction is correct. The number of correct predic-\n",
      "tions is used as the evaluation metric (as detailed\n",
      "in Appendix A.4.5). The results shown in Table 2\n",
      "demonstrate that BACON can perform the OV-SGG\n",
      "task, which is out-of-reach for VLMs and substan-\n",
      "tially outperforms purpose-built SGG models.\n",
      "4.1.5\n",
      "Image generation\n",
      "Advanced text-to-image models like SDXL (Podell\n",
      "et al., 2023) struggle to follow complex text\n",
      "prompts and accurately generate images. Fortu-\n",
      "nately, BACON allows generative models to sep-\n",
      "arately generate the background and different ob-\n",
      "jects and then merge them, thus splitting the chal-\n",
      "lenge into several easy parts (Detailed method can\n",
      "be found in Appendix A.4.6). We show two exam-\n",
      "ples in Figure 5 with more instances available in\n",
      "Appendix A.4.6. To assess the correlation between\n",
      "the text prompts and the generated images, we con-\n",
      "duct a user study involving 10 human annotators\n",
      "and 100 samples. They are tasked with counting\n",
      "the significant objects and relationships mentioned\n",
      "in the text prompts and those accurately generated\n",
      "in the images. Utilizing the manual annotation, we\n",
      "compute the accuracy metrics for objects (Ao) and\n",
      "relationships (Ar), with the details in Table 4. The\n",
      "results demonstrate that BACON significantly en-\n",
      "hances SDXL‚Äôs ability to understand and follow\n",
      "complex prompts. Remarkably, it enables SDXL\n",
      "to surpass DALL-E 3 in faithfully reproducing the\n",
      "details specified in the text descriptions.\n",
      "4.2\n",
      "Tasks directly using BACON\n",
      "In this section, for completeness, we discuss down-\n",
      "stream tasks where BACON can be used directly\n",
      "without special operations.\n",
      "4.2.1\n",
      "Visual question answering\n",
      "Following the setting of ZS-VQA and Section 4.1.2,\n",
      "we replace the input of VQA from images to\n",
      "their captions to evaluate the performance of\n",
      "the captioner.\n",
      "We compare BACON-Captioner\n",
      "with multiple VLM-based captioners including\n",
      "LLaVA (Liu et al., 2023a), Qwen-VL-max (Bai\n",
      "et al., 2023), and the distillation version on caption\n",
      "task of GPT-4V, ShareGPT-4V (Chen et al., 2023a)\n",
      "on multiple commonly used VQA detasets includ-\n",
      "ing NLVR2 (Suhr et al., 2018), VQAv1 (Antol\n",
      "et al., 2015), VQAv2 (Goyal et al., 2017), and OK-\n",
      "6\n",
      "Caption\n",
      "BACON + SDXL\n",
      "DALL-E 3\n",
      "SDXL\n",
      "At the center of the screen is a man wearing a black suit and a red tie, riding on the \n",
      "back of a dinosaur. The dinosaur stands on the green grass, opening its mouth to \n",
      "reveal its sharp teeth. In the bottom right corner of the screen, a photographer wearing \n",
      "a black coat squats, focusing on capturing this unique scene with a camera. The mid \n",
      "shot features two trees with sparse leaves. In the background, there are two colorful \n",
      "hot air balloons flying in the sky. One is black, located in the upper right corner of the \n",
      "screen, and the other is colorful, located slightly higher in the middle of the screen\n",
      "The full moon hangs high on the left side of the sky, with stars dotted around it. The \n",
      "background on the left side of the picture is a magnificent castle, standing on a distant \n",
      "hill. In the bottom left corner of the photo, two people are standing hand in hand on a \n",
      "rock by the river. Men are on the left, women are on the right, and the river winds \n",
      "from their feet towards the distance. On the right side of the picture is a male angel \n",
      "holding a trumpet and spreading his wings. Above the angel, there is a glowing flying \n",
      "saucer suspended in the air.\n",
      "Figure 5: Comparative examples of image generation reveal that BACON enhances advanced generative models\n",
      "like SDXL. SDXL and DALL-E 3 struggle with complex text and fail to produce corresponding images. Remarkably,\n",
      "BACON not only elevates SDXL‚Äôs image quality but also markedly boosts its comprehension of intricate instructions,\n",
      "enabling it to surpass DALL-E 3 in terms of accurately generating images aligning with textual directives.\n",
      "Cloud\n",
      "sky\n",
      "Boy\n",
      "Building\n",
      "Girl\n",
      "is above\n",
      "standing next to\n",
      "located behind\n",
      "is looking towards\n",
      "is above\n",
      "is above\n",
      "is scattered in\n",
      "Light beam\n",
      "cuts across\n",
      "Cloud\n",
      "sky\n",
      "Boy\n",
      "Building\n",
      "Robot\n",
      "is above\n",
      "standing next to\n",
      "located behind\n",
      "is looking towards\n",
      "is above\n",
      "is above\n",
      "is scattered in\n",
      "Hot air balloon\n",
      "is floating in\n",
      "Captioner\n",
      "‚ùÑ\n",
      "Edit item:\n",
      "1. Remove Light beam\n",
      "2. Change girl to robot\n",
      "3. Add hot air balloon\n",
      "Original image\n",
      "BACON\n",
      "Generated image from \n",
      "Modified BACON\n",
      "Modified BACON\n",
      "Generated image from BACON\n",
      "Figure 6: An example of interactively modifying BACON using BACON-Captioner.\n",
      "ShareGPT4v\n",
      "GPT-4V\n",
      "LLaVA\n",
      "Qwen-max\n",
      "Others\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "BACON\n",
      "Figure 7: Win rate of pairwise comparisons between\n",
      "BACON-Captioner and other VLM-based captioners.\n",
      "VQA (Marino et al., 2019). For evaluation mea-\n",
      "surement, we follow the official evaluation metrics\n",
      "to measure the performance. As shown in Table 3,\n",
      "BACON outperforms all baselines.\n",
      "4.2.2\n",
      "Precision & recall and user study\n",
      "To assess the performance of the BACON-Captioner,\n",
      "we conduct a user preference user study and exam-\n",
      "ine the precision and recall score by manual annota-\n",
      "tion. We compare BACON-Captioner with various\n",
      "VLM-based captioners, including LLaVA, Qwen-\n",
      "VL-max, ShareGPT-4V, and GPT-4V, by analyzing\n",
      "captions produced for a randomly sampled set of\n",
      "200 images from the MSCOCO dataset (Lin et al.,\n",
      "2014). We engage 10 human annotators for manual\n",
      "labeling. For the precision and recall scores, we\n",
      "first extract all important nouns existing in the cap-\n",
      "Table 5: Precision & recall score calculated by man-\n",
      "ual annotation between BACON-Captioner and other\n",
      "VLM-based captioners.\n",
      "Method\n",
      "Precision\n",
      "Recall\n",
      "LLaVA\n",
      "36.35¬±1.48%\n",
      "59.15¬±4.74%\n",
      "ShareGPT-4V\n",
      "23.20¬±3.82%\n",
      "55.30¬±2.12%\n",
      "Qwen-VL-max\n",
      "35.20¬±5.94%\n",
      "57.50¬±1.98%\n",
      "GTP4v\n",
      "21.5¬±0.71%\n",
      "70.55¬± 13.36%\n",
      "BACON\n",
      "56.23 ¬± 4.23% 82.83 ¬± 8.33%\n",
      "tions (see details in Appendix A.4.7) and then ask\n",
      "annotators to count the number of objects in the im-\n",
      "age and the number of correct predictions in the ex-\n",
      "tracted nouns. Then, the precision and recall score\n",
      "can be calculated. In the user preference study,\n",
      "annotators select their preferred annotation in pair-\n",
      "wise comparisons, ensuring structural aspects are\n",
      "neutralized to prevent any biases. The outcomes,\n",
      "as shown in Figure 7 and Table 5, indicate BACON\n",
      "outperforms all comparisons in general, notably\n",
      "predicting more correct objects than multiple\n",
      "popular VLMs even containing GPT-4V.\n",
      "4.3\n",
      "Additional capabilities of captioner\n",
      "Beyond obtaining BACON from images, the trained\n",
      "captioner is also adept at performing additional use-\n",
      "ful tasks, including interactively editing BACON,\n",
      "7\n",
      "Relationship info graph:\n",
      "Relationship info graph:\n",
      "Relationship info graph:\n",
      "Relationship info graph:\n",
      "floor\n",
      "curtain\n",
      "candelabra(1)\n",
      "man(1)\n",
      "Example 1: man(1) \n",
      " \n",
      "Category 1: \n",
      "Living \n",
      "  \n",
      "Category 2: \n",
      "Foreground\n",
      "Description: The man1 is covered with a patterned shirt‚Ä¶. and he stands with a hat and a relaxed \n",
      "posture. \n",
      "Color: black vest and pants, white shoes. \n",
      "Example 2: candelabra(1) Category 1: Inanimate   \n",
      "Category 2: \n",
      "Foreground\n",
      "Description: The candelabra stands to the left of the man, featuring a base and stem that holds \n",
      "several candles that are lit. ‚Ä¶ It provides a warm glow to the immediate area.\n",
      "Color: gold stem and base, white candles with yellow flames.\n",
      "Example 1: man(1) \n",
      " \n",
      "Category 1: \n",
      "Living \n",
      "  \n",
      "Category 2: \n",
      "Foreground\n",
      "Description: The man1 is covered with a patterned shirt‚Ä¶. and he stands with a hat and a relaxed \n",
      "posture. His right hand supported the brim of his hat.\n",
      "Color: black vest and pants, white shoes.\n",
      "Example 2: candelabra(1) Category 1: Inanimate   \n",
      "Category 2: \n",
      "Foreground\n",
      "Description: The candelabra stands to the left of the man, featuring a base and stem that holds \n",
      "several candles that are lit. ‚Ä¶ It provides a warm glow to the immediate area.\n",
      "Color: gold stem and base, white candles with yellow flames.\n",
      "Example 1: man(1) \n",
      " \n",
      "Category 1: \n",
      "Living \n",
      "  \n",
      "Category 2: \n",
      "Foreground\n",
      "Description: The man1 is covered with a patterned shirt‚Ä¶. and he stands with a hat and a relaxed \n",
      "posture. His right hand supported the brim of his hat. He clasped his hat in his left arm.\n",
      "Color: black vest and pants, white shoes. white shirt, red tie, and white waistcoat.\n",
      "Example 2: man(2) \n",
      " \n",
      "Category 1: \n",
      "Living \n",
      "  \n",
      "Category 2: \n",
      "Foreground\n",
      "Description: Man2 has short hair and is dressed in a dark suit with a vest and white pants‚Ä¶. His \n",
      "feet are in black shoes.\n",
      "Color: dark suit, white dress shirt, black tie, black shoes.\n",
      "Example 1: man(1) \n",
      " \n",
      "Category 1: \n",
      "Living \n",
      "  \n",
      "Category 2: \n",
      "Foreground\n",
      "Description: The man1 is covered with a patterned shirt ‚Ä¶. and he stands with a hat and a \n",
      "relaxed posture. He clasped his hat in his left arm.\n",
      "Color: black vest and pants, white shoes. white shirt, red tie, and white waistcoat.\n",
      "Example 2: candelabra(1) Category 1: Inanimate   \n",
      "Category 2: \n",
      "Foreground\n",
      "Description: The candelabra stands on the stage, holding several candles that are lit. ‚Ä¶ It is made \n",
      "of metal with a design that curves outward at the top\n",
      "Color: gold stem and base, white candles with yellow flames.\n",
      "floor\n",
      "curtain\n",
      "candelabra(1)\n",
      "man(1)\n",
      "candelabra(2)\n",
      "man(2)\n",
      "floor\n",
      "curtain\n",
      "candelabra(1)\n",
      "man(1)\n",
      "candelabra(2)\n",
      "man(2)\n",
      "floor\n",
      "curtain\n",
      "candelabra(1)\n",
      "man(1)\n",
      "candelabra(2)\n",
      "man(2)\n",
      "l New item compared to last frame\n",
      "l Changed item compared to last frame\n",
      "l Deleted item compared to last frame\n",
      "Relationship info graph:\n",
      "candelabra(2)\n",
      "floor\n",
      "man(2)\n",
      "curtain\n",
      "man(1)\n",
      "candelabra(1)\n",
      "hang  behind\n",
      "stand on\n",
      "place to the left\n",
      "reflect\n",
      "near\n",
      "Relationship info graph:\n",
      "candelabra(2)\n",
      "floor\n",
      "man(2)\n",
      "curtain\n",
      "man(1)\n",
      "candelabra(1)\n",
      "pair with\n",
      "stand next to\n",
      "near\n",
      "Relationship info graph:\n",
      "candelabra(2)\n",
      "floor\n",
      "man(2)\n",
      "curtain\n",
      "man(1)\n",
      "candelabra(1)\n",
      "stand near\n",
      "pair with\n",
      "hang  behind\n",
      "gesture  toward\n",
      "stand next to\n",
      "near\n",
      "Relationship info graph:\n",
      "reflect\n",
      "candelabra(2)\n",
      "floor\n",
      "man(2)\n",
      "curtain\n",
      "man(1)\n",
      "candelabra(1)\n",
      "stand on\n",
      "Frame1\n",
      "Frame2\n",
      "Frame3\n",
      "Frame4\n",
      "man1\n",
      "man1\n",
      "man1\n",
      "man1man2\n",
      "man2\n",
      "man2\n",
      "curtain\n",
      "curtain\n",
      "candelabra1\n",
      "candelabra1\n",
      "candelabra1\n",
      "candelabra2\n",
      "candelabra2\n",
      "candelabra2\n",
      "floor\n",
      "floor\n",
      "floor\n",
      "floor\n",
      "Figure 8: An example of BACON on video captioning, which includes three components: an overall description,\n",
      "an object list, and their relationships, each dynamically evolving over time. With respect to a prior frame, updates\n",
      "are color-coded: new elements in green, removed in red, altered in gold, and persistent ones in black. BACON thus\n",
      "adeptly captures the temporal changes and salient details of each video frame, while its structured nature potentially\n",
      "aids in downstream model comprehension.\n",
      "transforming ordinary prompts into BACON format,\n",
      "and planning positions of objects in BACON. First,\n",
      "as shown in Figure 6, BACON-Captioner allows us\n",
      "to interactively edit the BACON, and thus affect the\n",
      "generation of images.\n",
      "Subsequently and remarkably, without any fine-\n",
      "tuning, BACON-Captioner can transform an ordi-\n",
      "nary prompt into a BACON. For short prompts, it\n",
      "can add details to create a BACON, and for longer,\n",
      "it can organize the given information into a BACON.\n",
      "Moreover, the BACON-Captioner can arrange the\n",
      "positions of objects of the BACON. We provide ex-\n",
      "amples of both expanding and organizing prompts\n",
      "in Appendix A.4.8. We quantitatively compare\n",
      "BACON-Captioner‚Äôs capability of planning with\n",
      "LayoutGPT (Feng et al., 2024) on the MSCOCO\n",
      "dataset (Lin et al., 2014) and BACON datasets, us-\n",
      "ing the mIOU, precision and recall metrics (Feng\n",
      "et al., 2024) as detailed in Appendix A.4.8. Results\n",
      "in Table 6 indicate that BACON-Captioner performs\n",
      "better than LayoutGPT on both evaluated datasets.\n",
      "4.4\n",
      "BACON on video captioning\n",
      "While BACON is primarily developed for image\n",
      "data, it can be extended to create structured cap-\n",
      "Table 6: Comparison of plan task between BACON and\n",
      "LayoutGPT (Feng et al., 2024) on both MSCOCO (Lin\n",
      "et al., 2014) and BACON benchmark.\n",
      "Dataset\n",
      "Method\n",
      "Precision\n",
      "Recall\n",
      "mIOU\n",
      "MSCOCO LayoutGPT\n",
      "70.12%\n",
      "39.74%\n",
      "4.07%\n",
      "BACON\n",
      "71.18%\n",
      "41.84%\n",
      "6.83%\n",
      "Bacon\n",
      "LayoutGPT\n",
      "50.79%\n",
      "29.16%\n",
      "9.12%\n",
      "Dataset\n",
      "BACON\n",
      "51.66%\n",
      "47.06% 18.39%\n",
      "tions for videos with the help of additional tech-\n",
      "niques that address the temporal dimension of\n",
      "video content. The principal challenge of adapting\n",
      "BACON for videos is distinguishing between ele-\n",
      "ments that change over time and those that remain\n",
      "constant. To resolve this, we employ a tracking\n",
      "method (Cheng et al., 2023) to identify and match\n",
      "the same object across different frames. Further-\n",
      "more, we use T5 (Raffel et al., 2020) as the text\n",
      "encoder to compare descriptions of the same ob-\n",
      "ject or scene segment across frames. Parts of the\n",
      "text with high similarity scores are considered sta-\n",
      "ble, while those with low similarity are deemed\n",
      "to have changed. An example shown in Figure 8\n",
      "demonstrates this approach efficiently captures the\n",
      "continuity and evolution of video content, provid-\n",
      "8\n",
      "ing a coherent and descriptive narration. Additional\n",
      "examples are available in Appendix A.4.9.\n",
      "5\n",
      "Conclusion\n",
      "In this paper, we introduce BACON to address the\n",
      "difficulty of downstream models in understanding\n",
      "complex texts by breaking down complex annota-\n",
      "tions into basic minimum elements and presenting\n",
      "them in a graph structure. We propose a novel\n",
      "method for obtaining the structural representation\n",
      "from images and constructing a dataset of 100k\n",
      "scale based on it. A captioner is trained on this\n",
      "dataset with multiple remarkably useful capabili-\n",
      "ties. Extensive experiments demonstrate that our\n",
      "method can effectively assist downstream models\n",
      "in accomplishing tasks they previously could not\n",
      "achieve or excel in their current cutting-edge solu-\n",
      "tions.\n",
      "6\n",
      "Limitations\n",
      "This paper introduces a method designed to as-\n",
      "sist smaller models in comprehending complex\n",
      "texts and to facilitate their integration with VLMs,\n",
      "achieving remarkable performances across multiple\n",
      "benchmarks. However, despite these achievements,\n",
      "our approach still faces certain limitations. Firstly,\n",
      "given the absence of a fully automated method\n",
      "that guarantees reliable quality, our data collec-\n",
      "tion process still necessitates human annotation\n",
      "involvement. Secondly, due to cost and resource\n",
      "constraints, the captioner‚Äôs localization capabilities\n",
      "remain insufficient, necessitating the combination\n",
      "of a grounding model to obtain high-quality posi-\n",
      "tional information.\n",
      "References\n",
      "Peter Anderson, Xiaodong He, Chris Buehler, Damien\n",
      "Teney, Mark Johnson, Stephen Gould, and Lei Zhang.\n",
      "2018. Bottom-up and top-down attention for image\n",
      "captioning and visual question answering. In IEEE\n",
      "Conf. Comput. Vis. Pattern Recog.\n",
      "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\n",
      "garet Mitchell, Dhruv Batra, C Lawrence Zitnick, and\n",
      "Devi Parikh. 2015. VQA: Visual question answering.\n",
      "In Int. Conf. Comput. Vis.\n",
      "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\n",
      "Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\n",
      "and Jingren Zhou. 2023. Qwen-VL: A frontier large\n",
      "vision-language model with versatile abilities. arXiv\n",
      "preprint arXiv:2308.12966.\n",
      "James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jian-\n",
      "feng Wang, Linjie Li, Long Ouyang, Juntang Zhuang,\n",
      "Joyce Lee, Yufei Guo, et al. 2023. Improving image\n",
      "generation with better captions. Computer Science.\n",
      "https://cdn. openai. com/papers/dall-e-3. pdf.\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, et al. 2020. Language models are few-shot\n",
      "learners. Adv. Neural Inform. Process. Syst.\n",
      "Yashoda Chaulagain. 2018. Visual position and juxta-\n",
      "position: an analytical study of liberty leading the\n",
      "people and moon-woman cuts the circle. Tribhuvan\n",
      "University Journal.\n",
      "Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Con-\n",
      "ghui He, Jiaqi Wang, Feng Zhao, and Dahua\n",
      "Lin. 2023a. ShareGPT4V: Improving large multi-\n",
      "modal models with better captions. arXiv preprint\n",
      "arXiv:2311.12793.\n",
      "Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli\n",
      "Zhao, and Hengshuang Zhao. 2024. Anydoor: Zero-\n",
      "shot object-level image customization. In Proceed-\n",
      "ings of the IEEE/CVF Conference on Computer Vi-\n",
      "sion and Pattern Recognition, pages 6593‚Äì6602.\n",
      "Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo\n",
      "Chen, Sen Xing, Zhong Muyan, Qinglong Zhang,\n",
      "Xizhou Zhu, Lewei Lu, et al. 2023b.\n",
      "InternVL:\n",
      "Scaling up vision foundation models and aligning\n",
      "for generic visual-linguistic tasks. arXiv preprint\n",
      "arXiv:2312.14238.\n",
      "Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexan-\n",
      "der Schwing, and Joon-Young Lee. 2023. Tracking\n",
      "anything with decoupled video segmentation. In Int.\n",
      "Conf. Comput. Vis.\n",
      "George R Doddington, Alexis Mitchell, Mark A Przy-\n",
      "bocki, Lance A Ramshaw, Stephanie M Strassel, and\n",
      "Ralph M Weischedel. 2004. The automatic content\n",
      "extraction (ace) program-tasks, data, and evaluation.\n",
      "In Lrec.\n",
      "Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani,\n",
      "Arjun Akula, Xuehai He, Sugato Basu, Xin Eric\n",
      "Wang, and William Yang Wang. 2024. Layoutgpt:\n",
      "Compositional visual planning and generation with\n",
      "large language models. Adv. Neural Inform. Process.\n",
      "Syst.\n",
      "Andrea Frome, Greg S Corrado, Jon Shlens, Samy Ben-\n",
      "gio, Jeff Dean, Marc‚ÄôAurelio Ranzato, and Tomas\n",
      "Mikolov. 2013. Devise: A deep visual-semantic em-\n",
      "bedding model. Adv. Neural Inform. Process. Syst.\n",
      "Hanan Gani, Shariq Farooq Bhat, Muzammal Naseer,\n",
      "Salman Khan, and Peter Wonka. 2024.\n",
      "LLM\n",
      "blueprint: Enabling text-to-image generation with\n",
      "complex and detailed prompts. Int. Conf. Learn. Rep-\n",
      "resent.\n",
      "9\n",
      "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\n",
      "Batra, and Devi Parikh. 2017. Making the V in VQA\n",
      "matter: Elevating the role of image understanding in\n",
      "visual question answering. In IEEE Conf. Comput.\n",
      "Vis. Pattern Recog.\n",
      "Jiaxian Guo,\n",
      "Junnan Li,\n",
      "Dongxu Li,\n",
      "Anthony\n",
      "Meng Huat Tiong, Boyang Li, Dacheng Tao, and\n",
      "Steven Hoi. 2023. From images to textual prompts:\n",
      "Zero-shot visual question answering with frozen\n",
      "large language models. In IEEE Conf. Comput. Vis.\n",
      "Pattern Recog.\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\n",
      "Sun. 2016a. Deep residual learning for image recog-\n",
      "nition. In IEEE Conf. Comput. Vis. Pattern Recog.\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\n",
      "Sun. 2016b. Identity mappings in deep residual net-\n",
      "works. In Eur. Conf. Comput. Vis.\n",
      "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\n",
      "Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\n",
      "and Weizhu Chen. 2021.\n",
      "Lora: Low-rank adap-\n",
      "tation of large language models.\n",
      "arXiv preprint\n",
      "arXiv:2106.09685.\n",
      "Gao Huang, Zhuang Liu, Laurens Van Der Maaten,\n",
      "and Kilian Q Weinberger. 2017. Densely connected\n",
      "convolutional networks. In IEEE Conf. Comput. Vis.\n",
      "Pattern Recog.\n",
      "Justin Johnson, Agrim Gupta, and Li Fei-Fei. 2018.\n",
      "Image generation from scene graphs. In IEEE Conf.\n",
      "Comput. Vis. Pattern Recog.\n",
      "Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia\n",
      "Li, David Shamma, Michael Bernstein, and Li Fei-\n",
      "Fei. 2015. Image retrieval using scene graphs. In\n",
      "IEEE Conf. Comput. Vis. Pattern Recog.\n",
      "Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,\n",
      "and Tamara Berg. 2014. Referitgame: Referring to\n",
      "objects in photographs of natural scenes. In Proceed-\n",
      "ings of the 2014 conference on empirical methods in\n",
      "natural language processing (EMNLP), pages 787‚Äì\n",
      "798.\n",
      "Lei Ke, Mingqiao Ye, Martin Danelljan, Yu-Wing Tai,\n",
      "Chi-Keung Tang, Fisher Yu, et al. 2024. Segment\n",
      "anything in high quality. Adv. Neural Inform. Process.\n",
      "Syst.\n",
      "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\n",
      "Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,\n",
      "Spencer Whitehead, Alexander C Berg, Wan-Yen\n",
      "Lo, et al. 2023. Segment anything. arXiv preprint\n",
      "arXiv:2304.02643.\n",
      "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\n",
      "son, Kenji Hata, Joshua Kravitz, Stephanie Chen,\n",
      "Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n",
      "2017. Visual genome: Connecting language and vi-\n",
      "sion using crowdsourced dense image annotations.\n",
      "Int. J. Comput. Vis.\n",
      "Janghyeon Lee, Jongsuk Kim, Hyounguk Shon, Bum-\n",
      "soo Kim, Seung Hwan Kim, Honglak Lee, and Junmo\n",
      "Kim. 2022. Uniclip: Unified framework for con-\n",
      "trastive language-image pre-training. Adv. Neural\n",
      "Inform. Process. Syst.\n",
      "Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui,\n",
      "Wanli Ouyang, Jing Shao, Fengwei Yu, and Jun-\n",
      "jie Yan. 2021. Supervision exists everywhere: A\n",
      "data efficient contrastive language-image pre-training\n",
      "paradigm. arXiv preprint arXiv:2110.05208.\n",
      "Tsung-Yi Lin, Michael Maire, Serge Belongie, James\n",
      "Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,\n",
      "and C Lawrence Zitnick. 2014. Microsoft COCO:\n",
      "Common objects in context. In Eur. Conf. Comput.\n",
      "Vis.\n",
      "Xin Lin, Changxing Ding, Jinquan Zeng, and Dacheng\n",
      "Tao. 2020. GPS-Net: Graph property sensing net-\n",
      "work for scene graph generation. In IEEE Conf. Com-\n",
      "put. Vis. Pattern Recog.\n",
      "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\n",
      "Lee. 2023a. Visual instruction tuning. Adv. Neural\n",
      "Inform. Process. Syst.\n",
      "Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\n",
      "Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang\n",
      "Su, Jun Zhu, et al. 2023b. Grounding Dino: Mar-\n",
      "rying Dino with grounded pre-training for open-set\n",
      "object detection. arXiv preprint arXiv:2303.05499.\n",
      "Cewu Lu, Ranjay Krishna, Michael Bernstein, and\n",
      "Li Fei-Fei. 2016. Visual relationship detection with\n",
      "language priors. In Eur. Conf. Comput. Vis.\n",
      "Arjun Mani, Nobline Yoo, Will Hinthorn, and Olga Rus-\n",
      "sakovsky. 2020. Point and ask: Incorporating point-\n",
      "ing into visual question answering. arXiv preprint\n",
      "arXiv:2011.13681.\n",
      "Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\n",
      "Camburu, Alan L Yuille, and Kevin Murphy. 2016.\n",
      "Generation and comprehension of unambiguous ob-\n",
      "ject descriptions. In IEEE Conf. Comput. Vis. Pattern\n",
      "Recog.\n",
      "Kenneth Marino, Mohammad Rastegari, Ali Farhadi,\n",
      "and Roozbeh Mottaghi. 2019. OK-VQA: A visual\n",
      "question answering benchmark requiring external\n",
      "knowledge.\n",
      "In IEEE Conf. Comput. Vis. Pattern\n",
      "Recog.\n",
      "Chenlin Meng, Yutong He, Yang Song, Jiaming Song,\n",
      "Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. 2021.\n",
      "SDEdit: Guided image synthesis and editing with\n",
      "stochastic differential equations.\n",
      "arXiv preprint\n",
      "arXiv:2108.01073.\n",
      "George A Miller. 1995. Wordnet: a lexical database for\n",
      "english. Communications of the ACM.\n",
      "OpenAI. 2023.\n",
      "GPT-4V(ision) technical work and\n",
      "authors.\n",
      "https://openai.com/contributions/\n",
      "gpt-4v/.\n",
      "10\n",
      "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shao-\n",
      "han Huang, Shuming Ma, Qixiang Ye, and Furu Wei.\n",
      "2023. Grounding multimodal large language models\n",
      "to the world. In Int. Conf. Learn. Represent.\n",
      "Brant Pitre. 2015. Jesus and the last supper.\n",
      "Dustin Podell, Zion English, Kyle Lacey, Andreas\n",
      "Blattmann, Tim Dockhorn, Jonas M√ºller, Joe Penna,\n",
      "and Robin Rombach. 2023. SDXL: Improving latent\n",
      "diffusion models for high-resolution image synthesis.\n",
      "arXiv preprint arXiv:2307.01952.\n",
      "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\n",
      "Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\n",
      "try, Amanda Askell, Pamela Mishkin, Jack Clark,\n",
      "et al. 2021. Learning transferable visual models from\n",
      "natural language supervision. In Int. Conf. Mach.\n",
      "Learn.\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\n",
      "Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\n",
      "Wei Li, and Peter J Liu. 2020. Exploring the limits\n",
      "of transfer learning with a unified text-to-text trans-\n",
      "former. J. Mach. Learn. Res.\n",
      "Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Ab-\n",
      "delrahman Shaker, Salman Khan, Hisham Cholakkal,\n",
      "Rao M Anwer, Erix Xing, Ming-Hsuan Yang, and\n",
      "Fahad S Khan. 2024. Glamm: Pixel grounding large\n",
      "multimodal model. IEEE Conf. Comput. Vis. Pattern\n",
      "Recog.\n",
      "Robin Rombach, Andreas Blattmann, Dominik Lorenz,\n",
      "Patrick Esser, and Bj√∂rn Ommer. 2022.\n",
      "High-\n",
      "resolution image synthesis with latent diffusion mod-\n",
      "els. In IEEE Conf. Comput. Vis. Pattern Recog.\n",
      "Vishnu Sarukkai, Linden Li, Arden Ma, Christopher R√©,\n",
      "and Kayvon Fatahalian. 2024. Collage diffusion. In\n",
      "IEEE Winter Conf. Appl. Comput. Vis., pages 4208‚Äì\n",
      "4217.\n",
      "Piyush Sharma, Nan Ding, Sebastian Goodman, and\n",
      "Radu Soricut. 2018. Conceptual captions: A cleaned,\n",
      "hypernymed, image alt-text dataset for automatic im-\n",
      "age captioning. In Proceedings of the 56th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics.\n",
      "Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\n",
      "Huajun Bai, and Yoav Artzi. 2018. A corpus for\n",
      "reasoning about natural language grounded in pho-\n",
      "tographs. arXiv preprint arXiv:1811.00491.\n",
      "Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan\n",
      "Luo, and Wei Liu. 2019. Learning to compose dy-\n",
      "namic tree structures for visual contexts. In IEEE\n",
      "Conf. Comput. Vis. Pattern Recog.\n",
      "Oriol Vinyals, Alexander Toshev, Samy Bengio, and\n",
      "Dumitru Erhan. 2015. Show and tell: A neural im-\n",
      "age caption generator. In IEEE Conf. Comput. Vis.\n",
      "Pattern Recog.\n",
      "Junjie Wang, Bin Chen, Bin Kang, Yulin Li, YiChi Chen,\n",
      "Weizhi Xian, and Huifeng Chang. 2024a. Ov-dquo:\n",
      "Open-vocabulary detr with denoising text query train-\n",
      "ing and open-world unknown objects supervision.\n",
      "arXiv preprint arXiv:2405.17913.\n",
      "Zhenyu Wang, Enze Xie, Aoxue Li, Zhongdao Wang,\n",
      "Xihui Liu, and Zhenguo Li. 2024b. Divide and con-\n",
      "quer: Language models can plan and self-correct\n",
      "for compositional text-to-image generation. arXiv\n",
      "preprint arXiv:2401.15688.\n",
      "Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-\n",
      "Fei. 2017. Scene graph generation by iterative mes-\n",
      "sage passing. In IEEE Conf. Comput. Vis. Pattern\n",
      "Recog., pages 5410‚Äì5419.\n",
      "Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou,\n",
      "Wayne Zhang, and Ziwei Liu. 2022. Panoptic scene\n",
      "graph generation. In Eur. Conf. Comput. Vis.\n",
      "Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu,\n",
      "Stefano Ermon, and Bin Cui. 2024. Mastering text-to-\n",
      "image diffusion: Recaptioning, planning, and gener-\n",
      "ating with multimodal LLM. Int. Conf. Mach. Learn.\n",
      "Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin\n",
      "Choi. 2018. Neural motifs: Scene graph parsing with\n",
      "global context. In IEEE Conf. Comput. Vis. Pattern\n",
      "Recog., pages 5831‚Äì5840.\n",
      "Ao Zhang, Liming Zhao, Chen-Wei Xie, Yun Zheng,\n",
      "Wei Ji, and Tat-Seng Chua. 2023. Next-chat: An\n",
      "LLM for chat, detection and segmentation. arXiv\n",
      "preprint arXiv:2311.04498.\n",
      "Xinyu Zhang, Yuting Wang, and Abdeslam Boularias.\n",
      "2024. Detect everything with few examples. arXiv\n",
      "preprint arXiv:2309.12969.\n",
      "Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-\n",
      "Fei. 2016. Visual7w: Grounded question answer-\n",
      "ing in images. In IEEE Conf. Comput. Vis. Pattern\n",
      "Recog.\n",
      "11\n",
      "A\n",
      "Appendix\n",
      "The appendix is organized into five distinct sections.\n",
      "It begins by presenting a comprehensive overview\n",
      "of related work in Appendix A.1. This is followed\n",
      "by an in-depth discussion of the methodology in\n",
      "Appendix A.2. The next section, Appendix A.3,\n",
      "focuses on the aspects of human annotation within\n",
      "the dataset collection process for BACON. Ap-\n",
      "pendix A.4 then provides a thorough explanation of\n",
      "the experimental setup, detailing the metrics used\n",
      "and the methodology for implementing BACON in\n",
      "solving downstream tasks. Moreover, this section\n",
      "includes additional experimental findings.\n",
      "A.1\n",
      "Related works\n",
      "Image description Transforming images into tex-\n",
      "tual descriptions is a quintessential task, with\n",
      "many downstream applications relying on accu-\n",
      "rate text-based representations of visual data (Rad-\n",
      "ford et al., 2021; Antol et al., 2015; Podell et al.,\n",
      "2023; Betker et al., 2023; Liu et al., 2023b; Frome\n",
      "et al., 2013). Traditional methods for image cap-\n",
      "tioning usually suffer the problem of lacking de-\n",
      "tails (Anderson et al., 2018; Mao et al., 2016;\n",
      "Kazemzadeh et al., 2014; Sharma et al., 2018;\n",
      "Vinyals et al., 2015). Recently, VLM-based models\n",
      "including GPT-4V (OpenAI, 2023), LLaVA (Liu\n",
      "et al., 2023a), ShareGPT-4V (Chen et al., 2023a),\n",
      "InternVL (Chen et al., 2023b), Qwen-VL (Bai et al.,\n",
      "2023) and so on, can describe images in very detail\n",
      "and output very long captions, which have been\n",
      "proved helpful in the downstream fields, such as\n",
      "image generation (Betker et al., 2023).\n",
      "Decomposing description However, overly com-\n",
      "plex texts pose significant challenges to the com-\n",
      "prehension abilities of downstream models. Take\n",
      "the domain of text-to-image generation as an exam-\n",
      "ple, where representative models like SDXL (Rom-\n",
      "bach et al., 2022) struggle to accurately generate\n",
      "images based on detailed textual guidance. This\n",
      "has spurred a plethora of efforts aimed at decom-\n",
      "posing and reassembling texts to enhance mod-\n",
      "els‚Äô fidelity to intricate instructions (Yang et al.,\n",
      "2024; Wang et al., 2024b; Gani et al., 2024). There\n",
      "are also works trying to describe the image in a\n",
      "graph structure, including grounding caption mod-\n",
      "els (Peng et al., 2023; Zhang et al., 2023; Rasheed\n",
      "et al., 2024) and scene graph generation (SGG)\n",
      "models (Xu et al., 2017; Zellers et al., 2018; Lin\n",
      "et al., 2020; Tang et al., 2019; Yang et al., 2022).\n",
      "A.2\n",
      "Supplementary of BACON\n",
      "In this section, we will delve into the details of\n",
      "the BACON method as a supplement to the main\n",
      "text. We will organize this section following the\n",
      "structure of the main text, including additional in-\n",
      "sights into the design philosophy behind BACON in\n",
      "Appendix A.2.1, as well as showcasing complete\n",
      "examples. In Appendix A.2.2, we will present the\n",
      "details and examples of the graph construction.\n",
      "A.2.1\n",
      "Design of BACON\n",
      "As mentioned in the main text, when breaking\n",
      "complex annotations down into basic elements, we\n",
      "specifically considered information that could be\n",
      "beneficial for downstream tasks as these basic ele-\n",
      "ments. We then listed these basic elements along-\n",
      "side the potential domains that might benefit from\n",
      "them, as illustrated in Section 6. Besides, we pro-\n",
      "vide several complete examples of BACON in Fig-\n",
      "ures 13 and 14, which are omitted in the main paper\n",
      "to save space.\n",
      "A.2.2\n",
      "Graph construction\n",
      "In the main paper, we introduced the method of\n",
      "constructing graphs using VLMs but omitted some\n",
      "details due to space constraints. The method en-\n",
      "compasses two key steps. The first is to design a\n",
      "string format along with a reversible conversion\n",
      "method that can transform BACON into the string\n",
      "format understandable by VLMs, while also ensur-\n",
      "ing that this string format can be converted back\n",
      "to BACON without loss. The second step involves\n",
      "utilizing the ICL technique to teach the VLMs to\n",
      "describe images using our designed string format.\n",
      "We will now introduce these two steps in detail.\n",
      "BACON in string format We translate the graph\n",
      "structure of BACON into a string format inter-\n",
      "pretable by VLMs (we use GPT-4V in practice),\n",
      "demarcating various sections with unique symbols,\n",
      "where an example is shown in Figure 15. Specifi-\n",
      "cally, we label main titles with %% and subtitles\n",
      "with &&. When listing objects, we enclose ex-\n",
      "tra details like category, description, and color in\n",
      "brackets (). Each detail is separated by a semi-\n",
      "colon \";\". We mark the name of an object with\n",
      "<>. During the description of relationships, we\n",
      "use <> for showing objects and [] for the predicate.\n",
      "Additionally, we use <> to highlight important ob-\n",
      "jects within the object, serving multiple purposes.\n",
      "One such function is to post-process the GPT-4V\n",
      "output results. This involves removing foreground\n",
      "information from the background description by\n",
      "12\n",
      "Part of information\n",
      "Potential areas of benefit\n",
      "Style\n",
      "Art classification, Style transfer, Aesthetic evaluations\n",
      "Theme\n",
      "Recommendation systems, Thematic analysis\n",
      "Background\n",
      "Scene reconstruction, Virtual reality\n",
      "Foreground\n",
      "Focus analysis, Subject tracking, Interactive applications\n",
      "Object name\n",
      "Object detection, Labeling, Database organization, Language-based image retrieval\n",
      "Object category\n",
      "Semantic segmentation, Hierarchical classification\n",
      "Life status\n",
      "Behavior Prediction, human-computer interaction, Living identification\n",
      "Depth cues\n",
      "Depth estimation, Layered generation, Compositing\n",
      "Object description\n",
      "Fine-grained recognition, region captioning, region multi-modal understanding, Image generation\n",
      "Object color\n",
      "Color-based retrieval, Fashion industry applications, Image coloring\n",
      "Relationships\n",
      "Scene understanding, Action recognition\n",
      "Table 7: List of different important parts of information and the potential fields that may benefit from them. Depth\n",
      "cues represent the foreground-background information of objects.\n",
      "deleting sentences where the foreground objects\n",
      "appear, or similarly, eliminating background infor-\n",
      "mation from the foreground description. By using\n",
      "these special symbols to separate different sections,\n",
      "we can effortlessly organize the string format of\n",
      "BACON into a dictionary using regular expressions.\n",
      "This makes it easy for downstream tasks to extract\n",
      "various pieces of information without any hassle.\n",
      "Instruction for GPT-4V to obtain BACON Then,\n",
      "we discuss the method of applying ICL technique\n",
      "to obtain BACON via VLMs (we use GPT-4V in\n",
      "practice) in detail. As described in Section 2.3,\n",
      "we discover that GPT-4V does not require exhaus-\n",
      "tive examples to master the desired format. We\n",
      "simply need to insert a few important examples in\n",
      "the right spots within the instruction, which then\n",
      "play a key role. You can see the final instruction in\n",
      "Figure 16, where we‚Äôve highlighted the critical ex-\n",
      "amples in orange. Among the examples used, some\n",
      "are specific and others are more general. We‚Äôve ob-\n",
      "served that for straightforward structural elements,\n",
      "general examples are quite effective. For instance,\n",
      "just a few lines, like ‚Äôlines 3-4‚Äô or ‚Äôlines 8-9‚Äô, can\n",
      "adequately indicate the use of special symbols in\n",
      "a section, eliminating the need for a full-fledged\n",
      "example. In lines 21-22, we present a general ex-\n",
      "ample that clearly delineates the structure of each\n",
      "object, which significantly minimizes GPT-4V‚Äôs\n",
      "errors. To keep object details easy to grasp, we use\n",
      "a general example lines 23-24, which are sufficient\n",
      "for producing simple sentences. Regarding lines\n",
      "27-28, a general example is enough to instruct GPT-\n",
      "4V on the basic pattern for depicting relationships.\n",
      "Lastly, a general example set out in line 29 aids\n",
      "in preventing GPT-4V from repeatedly generating\n",
      "two-way relationship pairs.\n",
      "However, our high demands on the content and\n",
      "structure are extremely hard even for GPT-4V.\n",
      "Therefore, GPT-4V sometimes gets details wrong,\n",
      "like missing special symbols, even when we use\n",
      "general examples. That‚Äôs why we need to use spe-\n",
      "cific examples to make sure GPT-4V really gets\n",
      "the structure. Take numbering items in the same\n",
      "category, for instance, we introduce a specific ex-\n",
      "ample in lines 14-15. Without this example, GPT-\n",
      "4V tends to forget to number the items correctly,\n",
      "even though we‚Äôve already required it in lines 13-\n",
      "14. Also, we noticed GPT-4V does well with the\n",
      "format of the first section but often slips up with the\n",
      "second and third parts, which complicates turning\n",
      "the data into a dictionary. By providing only one\n",
      "clear example for these sections, GPT-4V is much\n",
      "more likely to produce the right structure. The\n",
      "ICL technique has helped ensure that nearly all of\n",
      "the 110k data entries we‚Äôve gathered are formatted\n",
      "correctly and can be translated into a dictionary\n",
      "format.\n",
      "A.3\n",
      "Supplementary of BACON dataset\n",
      "In this section, we begin by examining the output\n",
      "distribution of BACON-Captioner, as detailed in\n",
      "Appendix A.3.1. Subsequently, we introduce the\n",
      "details of human annotation, covering both the col-\n",
      "lection of training data and the establishment of\n",
      "test benchmarks.\n",
      "A.3.1\n",
      "Output distribution of\n",
      "BACON-Captioner\n",
      "We show the analysis of the root words and cate-\n",
      "gories detected in the outputs of BACON-Captioner,\n",
      "which can be seen in Figure 9. The result clearly\n",
      "shows that the output pattern of BACON-Captioner\n",
      "is very close to that of GPT-4V. Notably, there‚Äôs a\n",
      "100% overlap in the top 100 frequent nouns, 99%\n",
      "for verbs, and 97% for categories detected by GPT-\n",
      "4V and BACON-captioner. This similarity confirms\n",
      "13\n",
      "Root verbs ‚Äì BACON-Captioner\n",
      "Root verbs ‚Äì GPT-4V\n",
      "Root nouns ‚Äì GPT-4V\n",
      "Root nouns ‚Äì BACON-Captioner\n",
      "Detected categories ‚Äì BACON-Captioner\n",
      "Detected categories ‚Äì GPT-4V\n",
      "Figure 9: Analyzing the root words and detected categories in BACON‚Äôs output on testset: We compare the root\n",
      "words and detected categories generated by BACON-Captioner and GPT-4V, with certain sections magnified for\n",
      "clearer visualization. The results reveal that the output distribution of BACON closely resembles that of GPT-4V.\n",
      "that BACON-Captioner can effectively take over\n",
      "from GPT-4V in generating BACON from images\n",
      "and extend our BACON dataset.\n",
      "A.3.2\n",
      "Human annotation\n",
      "As we‚Äôve mentioned in Sections 2.3 and 3, creating\n",
      "the BACON dataset‚Äôs training and test sets involves\n",
      "human annotations.\n",
      "Collecting training data. In the process of col-\n",
      "lecting training data, BACON significantly reduces\n",
      "the workload of annotation. It breaks down the\n",
      "complex descriptions into basic elements, for many\n",
      "of which annotators simply need to make a straight-\n",
      "forward judgment of right or wrong, a task that\n",
      "is remarkably simple. For large pieces of infor-\n",
      "mation such as background or foreground descrip-\n",
      "tions, annotators are asked to separately determine\n",
      "if each sentence is correct according to the image.\n",
      "Besides, the annotators are asked to add objects\n",
      "missed by GPT-4V. In this process, the structure\n",
      "we designed for objects can help annotators sim-\n",
      "plify the description process. They only need to fill\n",
      "in the corresponding information according to the\n",
      "structure.\n",
      "Collecting test benchmark. In the method of col-\n",
      "lecting the test set of BACON, annotators are in-\n",
      "volved in four parts. For the first part, they are\n",
      "expected to correct the result returned by VLMs to\n",
      "recognize the object name given the masked image.\n",
      "In the second and third parts, annotators are asked\n",
      "Table 8: Complete hyper-parameters of training BA-\n",
      "CON-Captioner.\n",
      "Hyper-parameter Value Hyper-parameter\n",
      "Value\n",
      "Lora rank\n",
      "128\n",
      "Learning rate\n",
      "2√ó10‚àí4\n",
      "Epochs\n",
      "3\n",
      "Warmup ratio\n",
      "0.03\n",
      "Batch size\n",
      "16\n",
      "Max length\n",
      "2048\n",
      "to separately determine if each sentence is correct.\n",
      "They don‚Äôt have to add objects as Segment any-\n",
      "thing (SAM) (Kirillov et al., 2023) in this method\n",
      "has ensured that there will be no omissions. At the\n",
      "last stage, they have to determine if a relationship is\n",
      "correct and add an important relationship omitted\n",
      "by VLMs.\n",
      "A.4\n",
      "Supplementary of experiments\n",
      "In this section, we provide supplementary expla-\n",
      "nations for the experimental details omitted in the\n",
      "main text (Section 4), including the training de-\n",
      "tails of BACON-Captioner, the specific manner in\n",
      "which BACON aids downstream tasks, the exact\n",
      "calculation methods for metrics, and any special\n",
      "processing applied to the datasets. We will organize\n",
      "this section following the structure of the main text\n",
      "(Section 4) to facilitate readers in quickly locating\n",
      "the corresponding section for each experiment.\n",
      "14\n",
      "What color is this table? \n",
      "Question\n",
      "bbox: [320, 0, 440, 70]\n",
      "Bounding box\n",
      "üåã\n",
      "LLaVA\n",
      "BACON\n",
      "Obj1. Apple(1).     Bbox: green\n",
      "Category: Living. Foreground.  \n",
      "Description: The apple is of ‚ÄòFuji‚Äô‚Ä¶\n",
      "Color: red.\n",
      "Obj2. Apple Tray(4). Bbox: blue\n",
      "Category: Inanimate. Foreground.  \n",
      "Description: The apple tray holds ‚Ä¶\n",
      "Color: a mix of red and yellow.\n",
      "Obj3. Table(2).     Bbox: yellow\n",
      "Category: Inanimate. Background.  \n",
      "Description: The table provides ...\n",
      "Color: brown.\n",
      "Large IOU\n",
      "Brown\n",
      "Answer\n",
      "Which one contains a cut apple in it?\n",
      "Question\n",
      "Candidates:\n",
      "Text\n",
      "Encoder\n",
      "Obj1. Apple(1) Score: 0.614\n",
      "Obj2. Apple(2) Score: 0.915\n",
      "‚Ä¶(Other objects with score)\n",
      "BACON\n",
      "Obj1. Apple(1) IOU: 0.293\n",
      "Obj2. Apple(2) IOU: 0.347\n",
      "‚Ä¶(Other objects with IOUs)\n",
      "‚Ä¶(Other Candidates with weights)\n",
      "Candidate 4   Weight: 0. 7772\n",
      "‚Ä¶(Other Candidates with weights)\n",
      "ùëì\n",
      "ùëì=\n",
      "‚àë!\"#‚àà!\"#%&'( ùëÜùëêùëúùëüùëí!\"# ‚àóùêºùëÇùëà!\"#\n",
      "‚àë!\"#‚àà!\"#%&'( ùêºùëÇùëà!\"#\n",
      "Candidate 4 \n",
      "Answer\n",
      "argmax weight\n",
      "(1)\n",
      "(2)\n",
      "(3)\n",
      "(4)\n",
      "Question\n",
      "Obj1. Apple(1). \n",
      "Description: The \n",
      "apple is of ‚ÄòFuji‚Äô  \n",
      "with a red skin‚Ä¶\n",
      "Obj2. Apple(2).\n",
      "Description: The \n",
      "apple is of ‚ÄòFuji‚Äô \n",
      "with a red skin. It is \n",
      "cut‚Ä¶\n",
      "(a) PointQA task\n",
      "(b) PointingQA task\n",
      "Figure 10: An illustrative diagram depicting how BACON aids downstream models in executing PointQA\n",
      "and PointingQA tasks. In (a) the PointQA task, a list of objects and their corresponding descriptions provided by\n",
      "BACON are utilized. The description of the object with the large overlap with the target region is used to represent\n",
      "the description of that region; this regional description is then fed into a QA model to answer questions related to\n",
      "the region. In (b) the PointingQA task, object descriptions provided by BACON are used to calculate similarity\n",
      "scores with the input question, generating scores for each object. Based on the overlap between object positions and\n",
      "candidate regions, a weighted sum of all object scores is computed to assign scores to candidate regions; the region\n",
      "with the highest score is then selected as the prediction.\n",
      "A.4.1\n",
      "Details of training BACON-Captioner\n",
      "BACON-Captioner is fine-tuned on a pre-trained\n",
      "13B LLaVA model using Low-Rank Adaptation\n",
      "(LoRA) (Hu et al., 2021) technique, where the num-\n",
      "ber of parameters of LoRA adapter is around 0.5B.\n",
      "We follow the default settings of LLaVA-lora fine-\n",
      "tuning, where the detailed hyper-parameters are\n",
      "in Table 8. The captioner is trained on NVIDIA\n",
      "A100 GPUs, taking around 100 GPU hours. We‚Äôve\n",
      "done this in hopes of making it easier for others to\n",
      "reproduce BACON-Captioner.\n",
      "A.4.2\n",
      "Open-vocabulary object detection\n",
      "Although Grounding DINO can carry out open-\n",
      "vocabulary object detection task, it still faces some\n",
      "issues. There are primarily two problems. First, the\n",
      "core step of Grounding DINO requires a noun as in-\n",
      "put to locate the position of that noun in the image.\n",
      "Moreover, it introduces methods to extract a se-\n",
      "ries of nouns from a sentence description, enabling\n",
      "it to perform object detection tasks. However, the\n",
      "method of extracting nouns can sometimes err, lead-\n",
      "ing to Grounding DINO producing some bizarre\n",
      "labels. For example, as illustrated in Figure 12,\n",
      "Grounding DINO outputs ambiguous labels such\n",
      "as ‚Äúone‚Äù, \"four men one one gray\", \"another\".\n",
      "The second issue, which is more severe, is\n",
      "Grounding DINO‚Äôs difficulty in distinguishing be-\n",
      "tween different individuals of the same category.\n",
      "As shown in Figure 12, although Grounding DINO\n",
      "identifies four people, it is challenging to determine\n",
      "which individual is represented by which bounding\n",
      "box with vague labels like \"four men one\". Note\n",
      "that the BACON benchmark serves as such a com-\n",
      "plex benchmark, incorporating numerous scenarios\n",
      "that more closely mirror real-life situations where it\n",
      "is necessary to distinguish different objects within\n",
      "the same or similar categories.\n",
      "Benefiting from BACON‚Äôs powerful capabilities,\n",
      "Grounding DINO can overcome these two issues\n",
      "with the aid of BACON. For the first problem, BA-\n",
      "CON inherently possesses the ability to identify\n",
      "important objects in an image, allowing Ground-\n",
      "ing DINO to receive a list of objects from BACON,\n",
      "resulting in a more accurate and comprehensive\n",
      "list of nouns. Regarding the second issue, as intro-\n",
      "15\n",
      "Image\n",
      "SAM\n",
      "Segmentation list\n",
      "‚Ä¶(more segment)\n",
      "Please describe the part of an image ‚Ä¶\n",
      "Instruction\n",
      "GPT-4V\n",
      "Name: Road    Category1: Inanimate    Category2: Background\n",
      "Description: The road is a black asphalt surface with red edges. ‚Ä¶\n",
      "Color: black paved surface, ‚Ä¶\n",
      "Name: Motorcycle    Category1: Inanimate    Category2: Foreground\n",
      "Description: The motorcycle is parked on the road, .‚Ä¶\n",
      "Color: Green motorcycle, silver metal frame, ‚Ä¶\n",
      "‚Ä¶(more object descriptions)\n",
      "GPT-4V\n",
      "(2) List of objects\n",
      "Style: The style is‚Ä¶  \n",
      "Theme: The theme‚Ä¶\n",
      "Global description of background: \n",
      " \n",
      "The background of ‚Ä¶\n",
      "Global description of foreground: \n",
      " \n",
      "The foreground is‚Ä¶\n",
      "(1) Overall description\n",
      "GPT-4V\n",
      "<Motorcycle> [is parked on] <road>\n",
      "<People1> [is next to] <People2>\n",
      "<Motorcycle> [is next to] <Billboard>\n",
      "‚Ä¶\n",
      "(3) Relationship\n",
      "BACON\n",
      "(1) Overall description\n",
      "(2) List of objects\n",
      "(3) Relationship\n",
      "Combined \n",
      "segmentation\n",
      "‚Ä¶(more)\n",
      "Figure 11: A detailed overview of the method used to collect the BACON benchmark, segmented into five\n",
      "distinct steps. 1) The SAM model segments all components within the image. 2) VLMs identify the names of\n",
      "objects in the masked image obtained from the first step. 3) Using the names identified in the second step, VLMs\n",
      "annotate each object in detail. 4) VLMs generate an overall description of the image based on the list of objects\n",
      "derived from the above steps. 5) images created by randomly pairing two masked images from the first step are fed\n",
      "to VLMs to identify the relationship between the combined segments. It is important to note that human annotation\n",
      "is required to correct and verify the outputs from steps two through five.\n",
      "duced in Section 2.4, by utilizing the list of objects\n",
      "provided by BACON, along with detailed descrip-\n",
      "tions of each object, it is possible to post-process\n",
      "Grounding DINO‚Äôs predictions. This enables the\n",
      "precise distinction of different individuals within\n",
      "the same category label.\n",
      "A.4.3\n",
      "Point question answering\n",
      "Method of applying BACON. In our experiment,\n",
      "PointQA is designed to answer questions related\n",
      "to image regions based on the description of the\n",
      "image. Most descriptions provided by Visual Lan-\n",
      "guage Models (VLMs) cannot accomplish this task\n",
      "as their descriptions lack positional information.\n",
      "However, BACON provides both the positional in-\n",
      "formation of objects within the image and their\n",
      "corresponding descriptions. Given a target area, by\n",
      "combining descriptions of different objects based\n",
      "on their positional relationships, one can create a\n",
      "description relevant to the location. Specifically,\n",
      "as illustrated in Figure 10, we compute the Inter-\n",
      "section Over Union (IOU) between the target area\n",
      "and the positions of all objects. By combining\n",
      "the descriptions of objects with high overlap, we\n",
      "obtain a description that is closely related to the\n",
      "target area. Then, we feed this description to the\n",
      "question-answering model to answer the question.\n",
      "A.4.4\n",
      "Pointing question answering\n",
      "Method of applying BACON. The PointingQA\n",
      "task requires selecting the most appropriate region\n",
      "from a set of candidate areas based on a textual\n",
      "prompt. VLMs struggle to complete this task be-\n",
      "cause they often lack the ability to perceive input\n",
      "location information. However, since BACON de-\n",
      "composes image descriptions into a series of basic\n",
      "elements, each with its corresponding location, we\n",
      "can leverage this feature to accomplish the task.\n",
      "As shown in Figure 10, the method is divided into\n",
      "three steps. First, we calculate the CLIP similarity\n",
      "between each object‚Äôs description and the input tex-\n",
      "tual prompt, obtaining scores for each object. The\n",
      "more relevant an object is to the text description,\n",
      "the higher its score. Secondly, we calculate scores\n",
      "for each candidate region by weighting the sum of\n",
      "object scores based on the overlap between the can-\n",
      "didate region and the object‚Äôs location. The greater\n",
      "the overlap with the candidate area, the larger the\n",
      "proportion of that object‚Äôs score. In the third step,\n",
      "the region with the highest score is selected as the\n",
      "answer.\n",
      "A.4.5\n",
      "Open-vocabulary scene graph\n",
      "generation\n",
      "Visual Genome dataset. Visual Genome is an\n",
      "open-vocabulary dataset. However, since most cur-\n",
      "rent scene graph generation (SGG) models only\n",
      "consider a certain number of categories, researchers\n",
      "often treat it as a dataset with a limited set of cate-\n",
      "gories. Specifically, they usually identify the most\n",
      "frequent 70 or 150 noun classes, along with the\n",
      "50 most common predicates, to create a filtered\n",
      "dataset. In our case, as we are working on an\n",
      "16\n",
      "GroundingDINO\n",
      "Caption:\n",
      "Four men are working together, one is wearing \n",
      "black clothing, one is in white, and one is in \n",
      "gray. There is another one looking at his phone.\n",
      "Figure 12: An example of Grounding DINO undertaking an open-vocabulary task, where it encounters issues\n",
      "with ambiguous labels and faces challenges in distinguishing between different individuals within the same category.\n",
      "open-vocabulary scene graph generation (OV-SGG)\n",
      "task, we treat the VG dataset as an open-vocabulary\n",
      "dataset, retaining all of its annotations.\n",
      "Evaluation metrics. Traditional SGG tasks often\n",
      "use recall-related metrics to evaluate performance,\n",
      "that is, how many (subject-predicate-object) triplets\n",
      "are predicted in an image. However, these met-\n",
      "rics cannot be directly applied to assess the perfor-\n",
      "mance of open-vocabulary tasks. This is because\n",
      "previous metrics involve performing classification\n",
      "tasks within a set of fixed categories and using\n",
      "the confidence of those classifications to obtain\n",
      "the top K predictions with the highest likelihood.\n",
      "However, in an open-vocabulary scenario, there\n",
      "are theoretically an infinite number of possibili-\n",
      "ties, making it impractical to calculate and sort all\n",
      "scores. Therefore, we use the number of correctly\n",
      "predicted triplets in the dataset as the evaluation\n",
      "metric. The more triplets are accurately predicted,\n",
      "the better the performance.\n",
      "Additionally, we use CLIP to determine the\n",
      "correctness of predictions on an open-vocabulary\n",
      "dataset. Given a prediction, it is considered correct\n",
      "as long as the CLIP similarity of its subject, predi-\n",
      "cate, and object to the corresponding ground truth\n",
      "exceeds a certain threshold (0.85 here), and the\n",
      "IOU between the positions of the subject and ob-\n",
      "ject with the ground truth also surpasses a threshold\n",
      "(0.5 here).\n",
      "A.4.6\n",
      "Image generation\n",
      "Method of enhancing SDXL by BACON. Even\n",
      "as one of the most renowned models for text-to-\n",
      "image generation, SDXL often struggles to under-\n",
      "stand complex prompts and generate precise im-\n",
      "ages accurately. This is primarily because SDXL\n",
      "employs CLIP for text understanding, which lim-\n",
      "its its ability to comprehend the text. However,\n",
      "each basic element within a complex prompt is not\n",
      "complicated for SDXL to understand and gener-\n",
      "ate. Therefore, by breaking down complex texts\n",
      "into basic elements, BACON can significantly as-\n",
      "sist SDXL in simplifying complex tasks. Specifi-\n",
      "cally, SDXL can first create the background, then\n",
      "sequentially generate each object, and finally as-\n",
      "semble the different parts. Currently, there are\n",
      "many methods that can be utilized for image stitch-\n",
      "ing, such as Anydoor (Chen et al., 2024), Collage\n",
      "Diffusion (Sarukkai et al., 2024), etc. Sometimes,\n",
      "images can also be directly stitched together and\n",
      "then refined using SDXL as the base model, with\n",
      "SDEdit (Meng et al., 2021) for refining the images,\n",
      "but this typically requires the images to be rela-\n",
      "tively simple. Aside from generating individual\n",
      "parts of the image and then stitching them together,\n",
      "another approach is to sequentially inpaint (Rom-\n",
      "bach et al., 2022) objects onto the image using\n",
      "inpainting methods.\n",
      "More results. We provide more examples in Fig-\n",
      "ure 19\n",
      "A.4.7\n",
      "Precision & recall and user study\n",
      "When calculating precision and recall, it involves\n",
      "identifying which objects have been predicted by\n",
      "different captioners. For other captioners, this can\n",
      "be challenging because directly extracting nouns\n",
      "would include many nouns that cannot be consid-\n",
      "ered objects. Therefore, we utilize VLMs to accom-\n",
      "plish this task. Specifically, we input the model‚Äôs\n",
      "captions into the VLMs, requesting them to ex-\n",
      "tract the important objects contained within. For\n",
      "BACON-Captioner, this process is straightforward\n",
      "because BACON explicitly provides a list of objects.\n",
      "This also highlights the advantages of BACON.\n",
      "A.4.8\n",
      "Additional capabilities of captioner\n",
      "Evaluation metrics. Evaluating the performance\n",
      "of the planning task is a subject that hasn‚Äôt been\n",
      "widely discussed. As one of the pioneers, Lay-\n",
      "outGPT (Feng et al., 2024) collected some images\n",
      "from the COCO dataset (Lin et al., 2014), which\n",
      "17\n",
      "have varying numbers of objects of the same cat-\n",
      "egory and used precision and recall as evaluation\n",
      "metrics to assess whether the quantity of objects\n",
      "planned is accurate. Inspired by their approach, we\n",
      "have slightly expanded the concepts of precision\n",
      "and recall. We randomly sample 1000 images from\n",
      "COCO and use their official captions as input for\n",
      "either LayoutGPT or BACON-Captioner. Then, we\n",
      "apply precision and recall metrics to assess how\n",
      "many of the objects predicted by different plan-\n",
      "ning methods actually exist in the images, and how\n",
      "many objects present in the images are predicted.\n",
      "It‚Äôs important to note that both the captioner\n",
      "and LayoutGPT operate in an open-vocabulary\n",
      "manner. Hence, we used CLIP to map the open-\n",
      "vocabulary predictions to COCO‚Äôs fixed set of cat-\n",
      "egories. Specifically, for an open-vocabulary pre-\n",
      "diction, we compute its similarity to all categories\n",
      "in COCO, treating the similarity as logits, and then\n",
      "use a softmax function to map it to a category in\n",
      "COCO. If the softmax score for the most likely\n",
      "category exceeds a threshold (0.9 here), we con-\n",
      "sider the prediction to be correct; otherwise, it is\n",
      "deemed incorrect. In BACON dataset, the situa-\n",
      "tion is quite similar. A slight difference is that the\n",
      "model‚Äôs predictions are mapped onto the list of\n",
      "ground truth objects for the current image, rather\n",
      "than a fixed set of categories. Similarly, when the\n",
      "softmax score exceeds a certain threshold, it is con-\n",
      "sidered a correct prediction. Given that BACON\n",
      "benchmark is significantly more challenging than\n",
      "COCO, if the threshold is set too high, almost all\n",
      "predictions would be incorrect; hence, we lowered\n",
      "the threshold to 0.5.\n",
      "Precision and recall do not take into account the\n",
      "positioning of the planning. This is because evaluat-\n",
      "ing whether a position is appropriate is a subjective\n",
      "task, and so long as it is reasonable, it should suf-\n",
      "fice. Nonetheless, since the positional distribution\n",
      "in the original images is assuredly reasonable, we\n",
      "can also use the positions in the original images\n",
      "as a certain reference. Therefore, we calculated\n",
      "the mean Intersection Over Union (mIOU) of the\n",
      "positions of the objects in the planning compared\n",
      "to those in the original images, and used this as an\n",
      "evaluation metric.\n",
      "Examples. We provide two examples (as shown\n",
      "in Figures 17 and 18) to demonstrate how BA-\n",
      "CON transforms a standard prompt into the format\n",
      "we need. Example in Figure 17 features a simple\n",
      "prompt; in this scenario, the BACON-Captioner of-\n",
      "ten imagines additional objects not present in the\n",
      "prompt to enrich the overall scene and converts\n",
      "it into the required format. Conversely, Example\n",
      "in Figure 18 involves a more complex input; in\n",
      "such cases, the likelihood of the BACON-Captioner\n",
      "adding new objects is reduced.\n",
      "A.4.9\n",
      "BACON on video captioning\n",
      "We provide more examples (as Figures 20 and 21)\n",
      "as a supplementary of the main paper.\n",
      "18\n",
      "Overall description:\n",
      "StyleÔºö'The image is a photograph with a realistic style.'\n",
      "ThemeÔºö'The theme of the image is transportation, specifically a train traveling through a rural landscape.'\n",
      "Background descriptionÔºö'The background of the image features a rural landscape with elements of nature and infrastructure. There is a \n",
      "bridge with green metal railings crossing over the train tracks. Beyond the bridge, a fence made of wooden posts and rails encloses a field. \n",
      "The field appears to be grassy with some patches of bare earth. The sky is overcast, with a pale, diffused light suggesting an overcast or \n",
      "cloudy day.'\n",
      "Foreground description: 'In the foreground, a train is captured in motion on the tracks. The train is painted in a blue and yellow color \n",
      "scheme. The train has multiple carriages, and the windows reflect the surrounding environment. The tracks are made of steel rails with \n",
      "wooden sleepers, and they run parallel to a grassy embankment on the left side of the image.'\n",
      "Object list:\n",
      "Train:\n",
      "Category: inanimate, foreground \n",
      "Description: 'The <train> ‚Äòs body is long and sleek, with <windows> lined along its side. The front <car> has a curved nose \n",
      "with a destination <sign> and <headlights>. The <train> is composed of several <carriages> connected together.‚Äô\n",
      "Color: blue and yellow \n",
      "Position: [200, 160, 441, 367] \n",
      "Track:\n",
      "Category: inanimate, foreground \n",
      "Description: 'The <track> consists of parallel <steel rails> supported by wooden <sleepers>. It stretches into the distance, \n",
      "guiding the <train>.‚Äô\n",
      "Color: rusty brown rails, brown sleepers \n",
      "Position: [128, 112, 553, 425] \n",
      "Bridge:\n",
      "Category: inanimate, background \n",
      "Description: 'The <bridge> spans over the <tracks> with a structure made of metal <beams> and <railings>. It appears \n",
      "functional and unadorned.‚Äô\n",
      "Color: green railings \n",
      "Position: [54, 95, 271, 160]\n",
      "Fence:\n",
      "Category: inanimate, background\n",
      "Description: 'The <fence> is constructed of wooden <posts> and <rails>, enclosing the <field> and providing a boundary.‚Äô\n",
      "Color: natural wood tone\n",
      "Position: [274, 137, 638, 184] \n",
      "Field: \n",
      "Category: inanimate, background\n",
      "Description: 'The <field> is predominantly grass-covered, with some areas of bare <soil>. It is bordered by the <fence> and \n",
      "<trees>.‚Äô\n",
      "Color: green grass, brown soil\n",
      "Position: [283, 161, 638, 421] \n",
      "Tree: \n",
      "Category: inanimate, background\n",
      "Description: 'The <trees> have bare branches, indicating a lack of <leaves> which could suggest a seasonal change.‚Äô\n",
      "Color: dark brown branches\n",
      "Position: [207, 0, 404, 146] \n",
      "Sky: \n",
      "Category: inanimate, background\n",
      "Description: 'The <sky> is overcast, with a uniform light grey color, suggesting cloudy weather.‚Äô\n",
      "Color: light grey\n",
      "Position: [1, 0, 636, 103] \n",
      "Relationship:\n",
      "<Train> [is traveling on] <track>\n",
      "<Train> [is passing under] <bridge>\n",
      "<Bridge> [spans over] <track>\n",
      "<Fence> [encloses] <field>\n",
      "<Field> [is bordered by] <tree>\n",
      "<Field> [is bordered by] <Fence>\n",
      "<Tree> [is standing in] <Field>\n",
      "Captioner&Grounding\n",
      "Figure 13: A complete example of BACON.\n",
      "19\n",
      "Overall description:\n",
      "StyleÔºö'The image is a candid, action-oriented photograph.'\n",
      "ThemeÔºö'The theme of the image is water sports, specifically surfing.'\n",
      "Background descriptionÔºöThe background of the image is dominated by the ocean with its varying shades of blue and white due to the \n",
      "churning waves and foam. The water appears dynamic and powerful, suggesting a strong surf. There is no clear horizon line, but the \n",
      "waves occupy the majority of the background space, creating a sense of immersion in the ocean environment. The light seems natural, \n",
      "likely from the sun, and it enhances the texture of the water and waves. The ocean is the only element in the background, emphasizing the \n",
      "vastness and the solitary nature of the surfing activity.'\n",
      "Foreground description: 'In the foreground, a man is captured in the midst of surfing a wave. He is wearing a cap, shorts, and is shirtless, \n",
      "which indicates a warm climate or season. The man is in a semi-crouched position on a surfboard, navigating the wave with a paddle in \n",
      "his left hand. His facial expression shows concentration and determination. The surfboard is mostly white with a design on its surface, \n",
      "and it cuts through the water, leaving a trail of spray behind. The man's musculature and stance suggest that he is experienced and in \n",
      "control of the surfboard. The water around the surfboard is turbulent, with foam and spray being generated by the movement of the \n",
      "surfboard and the wave.'\n",
      "Object list:\n",
      "Man:\n",
      "Category: living, foreground \n",
      "Description: 'The <man>‚Äôs <torso> is bare, and he is wearing a <cap> on his <head>. His lower body is covered by <shorts>. \n",
      "He is in a semi-crouched position on the <surfboard>, holding a <paddle> in his left hand. His facial expression shows \n",
      "focus.‚Äô\n",
      "Color: skin tone, green shorts, white cap \n",
      "Position: [200, 201, 270, 288] \n",
      "Surfboard:\n",
      "Category: inanimate, foreground \n",
      "Description: 'The <surfboard> is under the <man>, supporting him as he rides the <wave>. It has a design on its surface and \n",
      "is cutting through the <water>.‚Äô\n",
      "Color: predominantly white with a design\n",
      "Position: [178, 285, 283, 326] \n",
      "Paddle:\n",
      "Category: inanimate, foreground \n",
      "Description: 'The <paddle> is held by the <man> in his left hand, assisting him in navigating the <wave>.‚Äô\n",
      "Color: black shaft, white blade\n",
      "Position: [103, 171, 266, 271]\n",
      "Wave:\n",
      "Category: inanimate, background\n",
      "Description: 'The <wave> is large and powerful, with <water> churning and creating <foam> and <spray> as it breaks.‚Äô\n",
      "Color: shades of blue and white\n",
      "Position: [2, 63, 638, 422] \n",
      "Ocean: \n",
      "Category: inanimate, background\n",
      "Description: 'The <ocean> fills the background, characterized by its dynamic <waves> and <foam>.‚Äô\n",
      "Color: various shades of blue \n",
      "Position: [2, 2, 638, 424] \n",
      "Relationship:\n",
      "<Man> [is riding] <Surfboard>\n",
      "<Man> [is holding] <Paddle>\n",
      "<Surfboard> [is cutting through] <Wave>\n",
      "<Wave> [is breaking around] <Man>\n",
      "<Man> [is surfing on] <Ocean>\n",
      "<Ocean> [is supporting] <Surfboard>\n",
      "Captioner&Grounding\n",
      "Figure 14: A complete example of BACON.\n",
      "20\n",
      "%%Part1: Overall descrip2on%%\n",
      "&&Part1.1: Style&&\n",
      "The image is a photograph with a realis2c style.\n",
      "&&Part1.2: Theme&&\n",
      "The theme of the image is urban transporta2on during twilight.\n",
      "&&Part1.3: Global descrip2on of background&&\n",
      "The background of the image consists of an overcast <sky> with a gradient of blue tones, transi2oning from a deeper blue at the top to a <lighter> \n",
      "hue near the horizon. A large, modern <structure> labeled \"DOKK1\" dominates the leM side, featuring an angular design with illuminated \n",
      "<windows>. To the right is a tall <building> with numerous <windows>, some of which emit a warm glow. Further back, there are more <city \n",
      "buildings> with varying architectural designs, including one with a greenish <glass fa√ßade>. The <ambient ligh2ng> suggests it is either dawn or \n",
      "dusk, with <ar2Ô¨Åcial lights> beginning to have a pronounced eÔ¨Äect on the scene.\n",
      "&&Part1.4: Global descrip2on of foreground&&\n",
      "The foreground shows a <city street scene> with mul2ple <lanes>. A <tram> is on the leM, labeled ‚ÄúL2 Aarhus H\" and displaying a <des2na2on sign>. \n",
      "It is sta2oned at what appears to be a <tram stop>, with a <plaWorm> and a <railing>. The <street> is busy with <cars>, all with <headlights> on, \n",
      "indica2ng low light condi2ons. The <vehicles> vary in size and shape, sugges2ng a mix of personal and commercial <traÔ¨Éc>. The <pavement> along \n",
      "the <street> is wet, reÔ¨Çec2ng the lights of the <tram> and nearby <street lamps>. The overall <atmosphere> is one of a bustling <urban \n",
      "environment> in the evening hours.\n",
      "%%Part2: List of objects%%\n",
      "<Sky> (Inanimate; background; The <sky> presents a gradient of <blue> shades and is sca^ered with <clouds>; Color informa2on: shades of <blue>.)\n",
      "<Building 1> (Inanimate; background; The <building> has an angular <design> with <windows> that are illuminated from within; Color informa2on: \n",
      "<black> and <yellow> lights.)\n",
      "<Building 2> (Inanimate; background; This <building> is tall with many <windows>, some of which are lit, and has a cylindrical <shape>; Color \n",
      "informa2on: <white> with <yellow> lit windows.)\n",
      "<Building 3> (Inanimate; background; Visible behind <Building 1>, this <building> features a <glass fa√ßade> with a <greenish> hue; Color \n",
      "informa2on: <green> glass and <gray> structure.)\n",
      "<Tram> (Inanimate; foreground; The <tram> is sta2onary with a <front display> showing its <route> and a <design> that is sleek and modern; Color \n",
      "informa2on: predominantly <white> with <black> and <blue> accents.)\n",
      "<Street> (Inanimate; foreground/background; The <street> shows <lanes> with mul2ple <vehicles> and <wet> condi2ons reÔ¨Çec2ng <lights>; Color \n",
      "informa2on: <dark gray> asphalt, <white> road markings.)\n",
      "<Car 1> (Inanimate; foreground; A <car> with its <headlights> on, driving on the <street>, exhibi2ng a <sedan> body style; Color informa2on: \n",
      "<black>.)\n",
      "<Car 2> (Inanimate; foreground; Another <car> follows <Car 1>, also with <headlights> on, and appears to be a <hatchback>; Color informa2on: \n",
      "<silver>.)\n",
      "<Car 3> (Inanimate; foreground; This <car> is in the <lane> closest to the <camera>, showing a <compact> shape with <headlights> on; Color \n",
      "informa2on: <dark blue>.)\n",
      "<Lamppost> (Inanimate; background; A <lamppost> stands tall with a <light> at the top, illumina2ng the <area> below; Color informa2on: <black> \n",
      "post, <white> light.)\n",
      "%%Part3: Rela2onships%%\n",
      "<Sky> [overarches] <Buildings>.\n",
      "<Building 1> [is adjacent to] <Tram>.\n",
      "<Building 2> [towers over] <Street>.\n",
      "<Building 3> [is situated behind] <Building 1>.\n",
      "<Tram> [is sta2oned at] <Street>.\n",
      "<Tram> [reÔ¨Çects on] <Street>.\n",
      "<Car 1> [drives on] <Street>.\n",
      "<Car 2> [follows] <Car 1>.\n",
      "<Car 3> [is closest to] <Camera>.\n",
      "<Lamppost> [illuminates] <Street>.\n",
      "GPT4v\n",
      "Instruction\n",
      "Figure 15: An example of BACON in string format obtained by GPT-4V.\n",
      "21\n",
      "Hello, I would like to ask for your help in describing an image. Please note that I would like the description to be as \n",
      "detailed as possible. Please strictly respond following my instructions and do not print any redundant words.\n",
      "This description needs to include three parts. The title of each part should be ‚Äò%%Part1: Overall description%%‚Äô, ‚Äò%%Part2: \n",
      "List of objects%%‚Äô, and ‚Äò%%Part3: Relationships%%‚Äò. All important nouns in your response have to be bounded by '<' and \n",
      "'>‚Äô!\n",
      "The first part is an overall description of the image. Your answer to this part should consist of three parts, one sentence to \n",
      "describe the style of the image, one sentence to describe the theme of the image, and several sentences to describe the \n",
      "image. The titles of these parts are '&&Part1.1: Style&&', '&&Part1.2: Theme&&', '&&Part1.3: Global description of \n",
      "background&&', 'Part1.4: Global description of foreground&&'. The global description should be as detailed as possible \n",
      "and at least 150 words in total. If there is text content in the image, you can also describe the text, which should be bound \n",
      "by quotation marks. All important nouns in your response have to be bounded by '<' and '>‚Äô!\n",
      "The second part is to list all the objects in the image, as many as possible, in order of importance. Note that any object \n",
      "should not be a part of other objects. Note that the listed object should not be the plural. If there are multiple individuals \n",
      "of the same category of objects, please list them separately. For example, if there are three apples in the picture, they \n",
      "should be listed as 'Apple 1,' 'Apple 2,' and 'Apple 3.', respectively. Additionally, the objects should be classified into two \n",
      "categories: living and inanimate objects. Living refers to creatures such as humans, cats, dogs, and plants, while other \n",
      "lifeless objects belong to the category of inanimate objects. Finally, each object should have a very detailed description, \n",
      "with more important objects receiving more detailed descriptions. Each description should be at least 30 words and the \n",
      "important nouns in it have to be bounded by '<' and '>'. You should also identify whether this object belongs to the \n",
      "foreground or background. You should additionally provide a sentence to describe the color information of the object. \n",
      "Therefore, the format for listing each object should be 'Object Name (Category (Living/Inanimate); \n",
      "foreground/background; Description; Color information)'. Specifically, the detailed description of an object should focus \n",
      "on its part and its action. All descriptions should be in the forms of, object's + part + verb + object/adjective or object + is + \n",
      "present participle. The description should be detailed as well as possible, and try to describe all parts of this object. You \n",
      "should specifically notice if there is a sky, tree, sun, or other object in the background of the environment. All important \n",
      "nouns in your response have to be bounded by '<' and '>‚Äô!\n",
      "The third part is to describe the relationships between all the objects in pairs. Please list them one by one. Additionally, \n",
      "please describe the relationship between object A and object B in the format of 'Object A' + 'Action' + 'Object B.' Please \n",
      "don't print the same relation twice. For example, if there is ‚ÄúA relation B‚Äù, you shouldn't print 'B relation A' again. All \n",
      "important nouns in your response have to be bounded by '<' and '>‚Äô!\n",
      "I will provide you with an example of the last two parts of a description to show you the desired format. You should only \n",
      "focus on the format of this example instead of the content of it. You should use the same format to respond.\n",
      "\"%%Part2: List of objects%%\n",
      "<Woman> (Living; foreground; The <woman>'s <hair> is bundled in a <scarf>. Her <torso> is covered with a <black shirt>. \n",
      "Her <lower body> is clad in <blue jeans>. Her <legs> move through the <water>. Her <right hand> holds a pair of <shoes>; \n",
      "Color information: <black> shirt, <blue> jeans, <orange> scarf.)\n",
      "<Water> (Inanimate; foreground/background; The <water> floods the <street>, reflecting the <sky> and <surrounding \n",
      "objects>; Color information: <murky blue-grey>.)\n",
      "<Building 1> (Inanimate; background; The <building> has a <fa√ßade> with <doors> and <windows>, showing signs of \n",
      "<water damage>; Color information: <pale yellow>.)\n",
      "<Building 2> (Inanimate; background; This <building> is similar to <Building 1> but with a <red> roof visible above the \n",
      "<flood>; Color information: <light orange> walls, <red> roof.)\n",
      "<Vehicle 1> (Inanimate; background; A <vehicle> is partially submerged, showing only the <roof> and <upper parts>; Color \n",
      "information: <white>.)\n",
      "<Vehicle 2> (Inanimate; background; Another <vehicle>, also partially submerged, with a <visible logo>; Color information: \n",
      "<silver>.)\n",
      "<Sky> (Inanimate; background; The <sky> is filled with <clouds>, implying recent or ongoing <precipitation>; Color \n",
      "information: <gray>.)\n",
      "%%Part3: Relationships%%\n",
      "<Woman> [is walking through] <Water>.\n",
      "<Woman> [is moving away from] <Camera>.\n",
      "<Water> [reflects] <Sky>.\n",
      "<Water> [surrounds] <Vehicles>.\n",
      "<Buildings> [line] <Street>.\n",
      "<Vehicle 1> [is submerged by] <Water>.\n",
      "<Vehicle 2> [is submerged by] <Water>.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "Figure 16: The instruction for GPT-4V to obtain BACON from an image. We highlight the parts involving specific\n",
      "examples in orange.\n",
      "22\n",
      "A cat and a dog gather around a pizza on the highway.\n",
      "BACON-\n",
      "Captioner\n",
      "Overall description\n",
      "Style: The image is a candid, real-life photograph with a humorous setup. \n",
      "Theme: The theme of the image is an unexpected interaction between animals and human food.\n",
      "Background description: The background of the image features a highway setting with a large, dark-colored rock \n",
      "on the left side and a scrapped car on the right. The sky is overcast, with a dull, uniform gray tone suggesting an \n",
      "overcast or cloudy day. The highway itself is made of asphalt and has visible lane markings. The rock and car \n",
      "appear to be out of place on the road, indicating a possible scene of an accident or roadside debris. There are no \n",
      "visible people or moving vehicles, which adds to the stillness of the scene.\n",
      "Foreground: In the foreground, a cat and a dog are positioned on the road, closely inspecting a pizza that lies \n",
      "between them. The cat, on the right, is leaning forward with its head down, sniffing or showing interest in the pizza. \n",
      "The dog, on the left, mirrors the cat's posture, with its nose also directed towards the pizza. The pizza itself is large, \n",
      "with visible toppings that include cheese and pepperoni. The animals' proximity to the pizza and their focused \n",
      "attention on it create a humorous and unexpected scene, as it is unusual to see wild animals interacting in such a \n",
      "human-like manner. The road surface is a faded gray, and the animals' fur appears well-defined against this \n",
      "backdrop.\n",
      "Object list\n",
      "‚ë†Name: Cat;  Category: Animal, foreground;  Description: The cat's body is arched slightly as it leans over the \n",
      "pizza. Its head is lowered towards the pizza, its eyes seem to be focused on the food, and the cat's fur is dark blue;  \n",
      "Color: dark blue;  Position: [677, 178, 981, 361]\n",
      "‚ë°Name: Dog;  Category: Animal, foreground;  Description: The dog is standing with its body angled towards the \n",
      "pizza. Its head is also lowered to get a closer look at the pizza. The dog has fur that appears to be short and smooth;  \n",
      "Color: dark gray;  Position: [45, 167, 375, 423]\n",
      "‚ë£Name: Pizza;  Category: Inanimate, foreground;  Description: The pizza is round with a crust and several \n",
      "toppings that include cheese and pepperoni. It is placed directly on the road surface;  Color: golden crust, red \n",
      "pepperoni, yellow cheese;  Position: [428, 183, 632, 324]\n",
      "‚ë§Name: Rock;  Category: Inanimate, background;  Description: The rock is large and irregularly shaped, with a \n",
      "rough texture and a dark color that contrasts with the asphalt of the highway;  Color: dark gray;  Position: [281, 537, \n",
      "488, 649]\n",
      "‚ë•Name: Scrapped Car;  Category: Inanimate, background;  Description: The scrapped car is heavily damaged, \n",
      "with twisted metal. It is positioned on its side, suggesting it was involved in a severe accident;  Color: rusty and \n",
      "metallic tones;  Position: [584, 478,  852,  655]\n",
      "Relationship\n",
      "‚ë†< Dog > on the left side of < pizza >           ‚ë°< Cat > on the right side of < pizza > \n",
      "       ‚ë¢< Pizza > is lying on < highway >         \n",
      "‚ë£< Rock > is blocking < highway >\n",
      "       ‚ë§< Rock > is adjacent to < Scrapped Car >  ‚ë•< Scrapped Car > is blocking < highway>\n",
      "Figure 17: An example of how BACON-Captioner transforms a brief prompt into the distinctive style of\n",
      "BACON.\n",
      "23\n",
      "Under the bright moon, there is an icy sky and snow. In the \n",
      "center of the scene, a bonfire blazes. To the left of the bonfire \n",
      "are an arctic fox and a polar bear, to the right is a penguin, and \n",
      "above the bonfire flutters a small ice butterfly.\n",
      "Overall description\n",
      "Style: The image is a color photograph with a clear focus and a balanced composition.\n",
      "Theme: The theme of the image is a serene, wintery scene featuring various arctic animals gathered around a warm bonfire.\n",
      "Background description: The background of the image is mainly the bright moon in the upper part of the screen. Below the \n",
      "sky, the horizon is lined with a layer of snow that blankets the ground, reflecting the light from the bonfire and creating a \n",
      "contrast with the dark sky. The snow appears to be undisturbed except for the area immediately around the fire, where it is \n",
      "melted.\n",
      "Foreground: In the foreground, the central focus is the bonfire, with vibrant flames leaping upwards, casting a warm glow \n",
      "on the surrounding snow. The fire's light illuminates the figures of the animals and the snow around them. To the left of the \n",
      "fire, an arctic fox sits attentively, its fur appearing thick and white. To the right, a polar bear sits with a relaxed posture, its \n",
      "white fur contrasting with the darker elements of the scene. In front of the fire, a penguin stands out with its black and white \n",
      "coloring, with its back facing the camera. Above the fire, an ice butterfly with outstretched wings adds a whimsical touch to \n",
      "the scene. The animals are positioned on a flat surface, likely a frozen body of water, as indicated by the presence of the ice \n",
      "butterfly and the reflection of the fire on the snow.\n",
      "Object list\n",
      "‚ë†Name: Arctic Fox;  Category: Animal, foreground;  Description: The arctic fox's body is facing the camera, with its head \n",
      "turned slightly to the side. Its fur is thick and white;  Color: White;  Position: [153, 235, 337, 419]\n",
      "‚ë°Name: Polar Bear;  Category: Animal, foreground;  Description: The polar bear is sitting with its body oriented away \n",
      "from the camera. Its fur is thick and white, and its head is turned to the side, giving a profile view;  Color: White;  Position: \n",
      "[266, 264, 481, 627]\n",
      "‚ë¢Name: Penguin;  Category: Animal, foreground;  Description: The penguin stands with its body turned away from the \n",
      "camera, its head slightly tilted. It has black back and white belly;  Color: Black back, white belly;  Position: [471, 154, 614, \n",
      "416]\n",
      "‚ë£Name: Bonfire;  Category: Inanimate, foreground;  Description: The bonfire consists of flames leaping upwards from a \n",
      "pile of wood. The fire is bright and orange, casting light on the surrounding snow;  Color: Orange flames, brown and black \n",
      "wood;  Position: [307, 204, 614, 409]\n",
      "‚ë§Name: Ice Butterfly;  Category: Animal, foreground;  Description: The ice butterfly is positioned above the bonfire, with \n",
      "its wings spread wide. It appears to be made of ice and is translucent with a white hue;  Color: Translucent with a white tint;  \n",
      "Position: [378, 768, 450, 896]\n",
      "Relationship\n",
      "‚ë†< Arctic Fox > on the left side of < Bonfire >           ‚ë° < Arctic Fox > sits near < Polar Bear> \n",
      "     ‚ë¢< Polar Bear > on the right side of < Arctic Fox > ‚ë£< Penguin > in front of < Bonfire > \n",
      " \n",
      "‚ë§< Ice Butterfly > floats above < Bonfire >                ‚ë•< Polar Bear > sits beside < Bonfire>\n",
      "BACON-\n",
      "Captioner\n",
      "Figure 18: An example of how BACON-Captioner transforms a relatively complex prompt into the distinctive\n",
      "style of BACON.\n",
      "24\n",
      "Caption\n",
      "BACON + SDXL\n",
      "DALL-E 3\n",
      "SDXL\n",
      "In the deep sea, an abandoned large ship full of \n",
      "marine life sank to the bottom of the sea. There \n",
      "are two blue balloons floating in front of the ship. \n",
      "There is a dolphin swimming below the balloon. \n",
      "There is a drifting bottle floating in the deep sea, \n",
      "inside which is a sailboat\n",
      "In a yoga studio, there is an artwork of a green \n",
      "jade dragon, with a white cat lying on the right \n",
      "side of the artwork. On the distant ground, against \n",
      "the wall, there is a painting depicting war\n",
      "On a pink night, there was a pool in the center of \n",
      "the lawn, and a purple sports car was floating on \n",
      "the pool. There was a light bulb on the hood of the \n",
      "sports car, and there was an orange goldfish in the \n",
      "bulb. On the left side of the car is a small, colorful \n",
      "robot\n",
      "There is a small river in the forest, and there is a \n",
      "stone bridge on the river. There is a golden \n",
      "praying mantis on the bridge. There is a \n",
      "mongoose standing by the riverbank, and to its \n",
      "right lies a turtle\n",
      "In an abandoned factory building, sunlight filtered \n",
      "in. A technologically advanced spaceship flies \n",
      "over the factory building. Listening to a \n",
      "motorcycle below the spaceship, there is a pink \n",
      "guitar on the ground to the right of the motorcycle.\n",
      "In an old-fashioned subway station, there is a \n",
      "emerald green lion, a gray white wolf, and a \n",
      "colorful paper crane standing together waiting for \n",
      "the subway\n",
      "Figure 19: Additional examples of BACON on image generation.\n",
      "25\n",
      "Example 1: Dog\n",
      "Category 1:\n",
      "Living\n",
      "Category 2:\n",
      "Foreground\n",
      "Description: The dog has a fluffy white coat and appears to be a small breed. It is looking up \n",
      "towards the person and is attached to a leash.\n",
      "Color: white coat.\n",
      "Example 2: Bicycle\n",
      "Category 1: \n",
      "Inanimate\n",
      "Category 2:\n",
      "Foreground\n",
      "Description: The bicycle's frame is supporting person. Its wheels are in contact with the grass. \n",
      "The handlebars are being gripped by person's hands. A backpack is attached to the bicycle's rear \n",
      "rack.\n",
      "Color: silver frame, black tires, blue backpack.\n",
      "Example 1: Dog\n",
      "Category 1: \n",
      "Living\n",
      "Category 2:\n",
      "Foreground\n",
      "Description: The dog has a fluffy white coat and appears to be a small breed. It is looking up \n",
      "towards the person and is attached to a leash.\n",
      "Color: white coat.\n",
      "Example 2: Sidewalk\n",
      "Category 1: \n",
      "Inanimate\n",
      "Category 2:\n",
      "Background\n",
      "Description: The sidewalk is a concrete path that borders the lawn and leads towards the house. It \n",
      "is typical of suburban streets.\n",
      "Color: gray.\n",
      "Example 1: Dog\n",
      "Category 1:\n",
      "Living\n",
      "Category 2:\n",
      "Foreground\n",
      "Description: The dog has a fluffy white coat and appears to be a small breed. It is looking up \n",
      "towards the person and is attached to a leash.\n",
      "Color: white coat.\n",
      "Example 2: Text\n",
      "Category 1:\n",
      "Inanimate\n",
      "Category 2:\n",
      "Foreground\n",
      "Description: The text is superimposed on the image in the lower part, stating 'Thirteen years \n",
      "earlier‚Äô.\n",
      "Color: white on a semi-transparent background.\n",
      "Example 1: Dog\n",
      "Category 1:\n",
      "Living\n",
      "Category 2:\n",
      "Foreground\n",
      "Description: The dog has a fluffy white coat and appears to be a small breed. It is looking up \n",
      "towards the person and is attached to a leash.\n",
      "Color: white coat.\n",
      "Example 2: Sky  \n",
      "Category 1: \n",
      "Inanimate\n",
      "Category 2:\n",
      "Background\n",
      "Description: The sky is clear and blue, indicating good weather and no visible clouds.\n",
      "Color: blue.\n",
      "l New item compared to last frame\n",
      "l Changed item compared to last frame\n",
      "l Deleted item compared to last frame\n",
      "car\n",
      "dog\n",
      "sky\n",
      "tree\n",
      "lawn\n",
      "text\n",
      "house\n",
      "child\n",
      "person\n",
      "bicycle\n",
      "sidewalk\n",
      "car\n",
      "dog\n",
      "sky\n",
      "tree\n",
      "lawn\n",
      "text\n",
      "house\n",
      "child\n",
      "person\n",
      "bicycle\n",
      "sidewalk\n",
      "car\n",
      "dog\n",
      "sky\n",
      "tree\n",
      "lawn\n",
      "text\n",
      "house\n",
      "child\n",
      "person\n",
      "bicycle\n",
      "sidewalk\n",
      "car\n",
      "dog\n",
      "sky\n",
      "tree\n",
      "lawn\n",
      "text\n",
      "house\n",
      "child\n",
      "person\n",
      "bicycle\n",
      "sidewalk\n",
      "=\n",
      "Relationship\n",
      "text\n",
      "sky\n",
      "lawn\n",
      "dog\n",
      "child\n",
      "play with\n",
      "stand on\n",
      "stand on\n",
      "bicycle\n",
      "person\n",
      "sidewalk\n",
      "ride on\n",
      "stand on\n",
      "stand on\n",
      "tree\n",
      "house\n",
      "car\n",
      "near\n",
      "under\n",
      "near\n",
      "Relationship\n",
      "text\n",
      "sky\n",
      "lawn\n",
      "dog\n",
      "child\n",
      "bicycle\n",
      "person\n",
      "sidewalk\n",
      "tree\n",
      "house\n",
      "car\n",
      "under\n",
      "Relationship\n",
      "text\n",
      "sky\n",
      "lawn\n",
      "dog\n",
      "child\n",
      "bicycle\n",
      "person\n",
      "sidewalk\n",
      "tree\n",
      "house\n",
      "car\n",
      "near\n",
      "Relationship\n",
      "text\n",
      "sky\n",
      "lawn\n",
      "dog\n",
      "child\n",
      "bicycle\n",
      "person\n",
      "sidewalk\n",
      "tree\n",
      "house\n",
      "car\n",
      "Note: Considering the limited drawing space and beauty of graph structure, we have performed some simplification for relationships that appear more than once.\n",
      "Figure 20: An additional example of BACON on video captioning.\n",
      "26\n",
      "Relationship\n",
      "SUV\n",
      "flower\n",
      "tree\n",
      "road\n",
      "fence1\n",
      "street lamp\n",
      "side walk\n",
      "sky\n",
      "person\n",
      "house\n",
      "car\n",
      "lawn\n",
      "park  on\n",
      "shade for\n",
      "grow  on\n",
      "in front of\n",
      "in front of\n",
      "Relationship\n",
      "SUV\n",
      "flower\n",
      "tree\n",
      "road\n",
      "fence1\n",
      "street lamp\n",
      "side walk\n",
      "sky\n",
      "person\n",
      "house\n",
      "car\n",
      "lawn\n",
      "drive \n",
      "stand inside\n",
      "stand below\n",
      "obscured  by\n",
      "near\n",
      "Relationship\n",
      "SUV\n",
      "flower\n",
      "tree\n",
      "road\n",
      "fence1\n",
      "street lamp\n",
      "side walk\n",
      "sky\n",
      "person\n",
      "house\n",
      "car\n",
      "lawn\n",
      "in front of\n",
      "behind\n",
      "in front of\n",
      "Relationship\n",
      "SUV\n",
      "flower\n",
      "tree\n",
      "road\n",
      "fence2\n",
      "street lamp\n",
      "side walk\n",
      "sky\n",
      "person\n",
      "house\n",
      "car\n",
      "lawn\n",
      "pass \n",
      "through\n",
      "behind\n",
      "park on\n",
      "Example 1: SUV\n",
      "Category 1:\n",
      "Inanimate\n",
      "Category 2:\n",
      "Background\n",
      "Description: The vehicle has a prominent Mercedes-Benz emblem on the front grille, which is a \n",
      "three-pointed star encircled. ‚Ä¶ The overall condition of the car looks well-maintained, with no \n",
      "apparent damage or wear.\n",
      "Color: silver body, black spare tire cover, black tinted windows.\n",
      "Example 2: Flower1\n",
      "Category 1: \n",
      "Living\n",
      "Category 2:\n",
      "Foreground\n",
      "Description: The flower1 in the picture is a red rose, bright red, with layered petals that appear to \n",
      "be in full bloom. A large number of green leaves cluster around the flowers.\n",
      "Color: red petals, green leaves.\n",
      "Example 1: SUV\n",
      "Category 1: \n",
      "Inanimate\n",
      "Category 2:\n",
      "Foreground\n",
      "Description: The vehicle has a prominent Mercedes-Benz emblem on the front grille, which is a \n",
      "three-pointed star encircled. ‚Ä¶ The overall condition of the car looks well-maintained, with no \n",
      "apparent damage or wear.\n",
      "Color: silver body, black spare tire cover, black tinted windows.\n",
      "Example 2: Flowers\n",
      "Category 1: \n",
      "Living\n",
      "Category 2:\n",
      "Background\n",
      "Description: The flower in the picture is a red rose, bright red, with layered petals that appear to \n",
      "be in full bloom. A large number of green leaves cluster around the flowers\n",
      "Color: red petals, green leaves.\n",
      "Example 1: SUV\n",
      "Category 1:\n",
      "Inanimate\n",
      "Category 2:\n",
      "Foreground\n",
      "Description: The vehicle has a prominent Mercedes-Benz emblem on the front grille, which is a \n",
      "three-pointed star encircled. ‚Ä¶ The overall condition of the car looks well-maintained, with no \n",
      "apparent damage or wear.\n",
      "Color: silver body, black spare tire cover, black tinted windows.\n",
      "Example 2: Road\n",
      "Category 1:\n",
      "Inanimate\n",
      "Category 2:\n",
      "Foreground\n",
      "Description: A smooth and flat concrete road. The road condition is good, with no obvious cracks \n",
      "or severe wear. There are no visible markings or markings on the road.\n",
      "Color: gray\n",
      "Example 1: SUV\n",
      "Category 1:\n",
      "Inanimate\n",
      "Category 2:\n",
      "Foreground\n",
      "Description: The vehicle has a prominent Mercedes-Benz emblem on the front grille, which is a \n",
      "three-pointed star encircled. ‚Ä¶ The overall condition of the car looks well-maintained, with no \n",
      "apparent damage or wear.\n",
      "Color: silver body, black spare tire cover, black tinted windows.\n",
      "Example 2: Car  \n",
      "Category 1: \n",
      "Inanimate\n",
      "Category 2:\n",
      "Background\n",
      "Description: A small, red car with a hatchback design. The vehicle is captured in motion. It has \n",
      "black windows, silver wheels\n",
      "Color: red body, black windows, silver wheels.\n",
      "SUV\n",
      "tree\n",
      "road\n",
      "fence\n",
      "flower\n",
      "red car\n",
      "SUV\n",
      "tree\n",
      "road\n",
      "fence\n",
      "flower\n",
      "sky\n",
      "house\n",
      "person\n",
      "sidewalk\n",
      "street lamp\n",
      "lawn\n",
      "carcar\n",
      "SUV\n",
      "tree\n",
      "road\n",
      "fence\n",
      "flower\n",
      "sky\n",
      "house\n",
      "person\n",
      "sidewalk\n",
      "street lamp\n",
      "lawn\n",
      "l New item compared to last frame\n",
      "l Changed item compared to last frame\n",
      "l Deleted item compared to last frame\n",
      "SUV\n",
      "tree1\n",
      "road\n",
      "fence1\n",
      "fence2\n",
      "sky\n",
      "house\n",
      "person\n",
      "sidewalk\n",
      "street lamp\n",
      "lawn\n",
      "car\n",
      "SUV\n",
      "tree1\n",
      "road\n",
      "fence1\n",
      "flower1\n",
      "sky\n",
      "house\n",
      "person\n",
      "sidewalk\n",
      "street lamp\n",
      "lawn\n",
      "SUV\n",
      "tree1\n",
      "road\n",
      "fence1\n",
      "flower1\n",
      "sky\n",
      "house\n",
      "person\n",
      "sidewalk\n",
      "street lamp\n",
      "SUV\n",
      "tree1\n",
      "road\n",
      "fence1\n",
      "flower1\n",
      "Note: Considering the limited drawing space, some similar objects with different names are omitted here. For relationships that appear more than once, we have performed some simplification.\n",
      "Figure 21: An additional example of BACON on video captioning.\n",
      "27\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# URL of the webpage containing PDF links\n",
    "webpage_url = 'https://arxiv.org/list/cs.CV/recent?skip=1&show=1'# i just giving only one paper to extract we can alter show=\n",
    "\n",
    "# Fetch webpage content\n",
    "response = requests.get(webpage_url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract the publication date\n",
    "date_element = soup.find('h3')\n",
    "publication_date = date_element.get_text(strip=True) if date_element else 'Unknown date'\n",
    "\n",
    "# Extract all titles, authors, and PDF URLs\n",
    "base_url = 'https://arxiv.org'\n",
    "papers = []\n",
    "\n",
    "# Find all entries\n",
    "entries = soup.find_all('dl')\n",
    "\n",
    "for entry in entries:\n",
    "    titles = entry.find_all('div', class_='list-title mathjax')\n",
    "    authors = entry.find_all('div', class_='list-authors')\n",
    "    pdf_links = entry.find_all('a', title='Download PDF')\n",
    "\n",
    "    for title, author, pdf_link in zip(titles, authors, pdf_links):\n",
    "        paper_title = title.get_text(strip=True).replace('Title:', '').strip()\n",
    "        paper_authors = [a.get_text(strip=True) for a in author.find_all('a')]\n",
    "        paper_pdf_url = base_url + pdf_link['href']\n",
    "        papers.append({\n",
    "            'title': paper_title,\n",
    "            'authors': paper_authors,\n",
    "            'pdf_url': paper_pdf_url,\n",
    "            'date': publication_date\n",
    "        })\n",
    "\n",
    "def extract_text_from_pdf(paper):\n",
    "    text = f\"publication date of {paper['title']} is {paper['date']}\\nTitle {paper['title']}\\nAuthors of {paper['title']} {', '.join(paper['authors'])}\\n\\n\"\n",
    "    try:\n",
    "        # Download PDF or directly process it\n",
    "        response = requests.get(paper['pdf_url'], stream=True)\n",
    "        document = fitz.open(stream=response.content, filetype=\"pdf\")\n",
    "\n",
    "        for page_num in range(len(document)):\n",
    "            page = document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "\n",
    "    except Exception as e:\n",
    "        text += f\"Error processing PDF at {paper['pdf_url']}: {e}\"\n",
    "\n",
    "    return text\n",
    "\n",
    "# Extract text from each PDF and print it along with title, authors, and date\n",
    "for paper in papers:\n",
    "    extracted_text = extract_text_from_pdf(paper)\n",
    "    print(extracted_text)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "extracted_texts=[extracted_text]\n",
    "# Split text into manageable chunks/documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=350, chunk_overlap=35)\n",
    "\n",
    "documents = []\n",
    "for text in extracted_texts:\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    documents.extend(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "# Initialize the embeddings model from HuggingFace\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "# Create a FAISS vector store from documents and embeddings\n",
    "vector = FAISS.from_texts(documents, embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "# Initialize the Ollama language model\n",
    "llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# Define the system instruction for reformulating the user's question\n",
    "\n",
    "instruction_to_system = \"\"\"\n",
    "Given a chat history and the latest user question \n",
    "which might reference context in the chat history, formulate a standalone question \n",
    "which can be understood without the chat history. Do NOT answer the question, \n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "# Create a prompt template for reformulating questions\n",
    "question_maker_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", instruction_to_system),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Define a chain that reformulates the question if needed\n",
    "question_chain = question_maker_prompt | llm #| StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the question-answering assistant\n",
    "qa_system_prompt =  \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "There might be multiple research papers so check every document for better answer . \\\n",
    "If you don't know the answer,do not hallicuniate i need concise answer . Do not generate your answer.\\\n",
    "{context}\"\"\"\n",
    "\n",
    "# Create a prompt template for the question-answering task\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine if question needs reformulation based on chat history\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return question_chain\n",
    "    else:\n",
    "        return input[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# Create a retriever chain to fetch relevant context for the question\n",
    "retriever_chain = RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Retrieval-Augmented Generation (RAG) chain\n",
    "rag_chain = (\n",
    "    retriever_chain\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, BACON can significantly assist SDXL in simplifying complex tasks by breaking down complex texts into basic elements.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = \"Method of enhancing SDXL by BACON \"\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "#Invoke the RAG chain with the question and chat history, and update chat history with responses\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "###code to automate the scraping process everyday\n",
    "#import schedule\n",
    "#import time\n",
    "#import subprocess\n",
    "\n",
    "#def job():\n",
    "#    subprocess.run([\"python\", \"C:\\\\Path\\\\To\\\\YourScript\\\\scrape.py\"])\n",
    "\n",
    "#schedule.every().day.at(\"12:00\").do(job)\n",
    "\n",
    "#while True:\n",
    "#    schedule.run_pending()\n",
    "#    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Function to handle the chat interaction\n",
    "def chat_complete(message, state):\n",
    "    if state is None:\n",
    "        state = []\n",
    "    ai_msg = rag_chain.invoke({\"question\": message, \"chat_history\": state})\n",
    "    state.append({\"role\": \"user\", \"content\": message})\n",
    "    state.append({\"role\": \"assistant\", \"content\": ai_msg})\n",
    "    response = [(msg[\"content\"], state[i+1][\"content\"]) for i, msg in enumerate(state) if msg[\"role\"] == \"user\"]\n",
    "    return response, state\n",
    "\n",
    "# Define the Gradio interface\n",
    "with gr.Blocks() as block:\n",
    "    gr.Markdown(\"\"\"<h1><center> EduVisionBot </center></h1>\"\"\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    message = gr.Textbox(placeholder=\"Type your Message.........\")\n",
    "    state = gr.State([])\n",
    "    submit = gr.Button(\"SEND\")\n",
    "    \n",
    "    submit.click(chat_complete, inputs=[message, state], outputs=[chatbot, state])\n",
    "block.launch(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
