{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:\n",
      "DiffIR2VR-Zero: Zero-Shot Video Restoration with Diffusion-based Image Restoration Models\n",
      "\n",
      "Authors:\n",
      "Chi-Wei Hsiao3, Ting-Hsuan Chen1, Yu-Lun Liu1\n",
      "1\n",
      "\n",
      "Full Text:\n",
      "1 Introduction\n",
      "2 Related Work\n",
      "3 Method\n",
      "4 Experiments\n",
      "5 Conclusion\n",
      "References\n",
      "Appendix A Appendix / supplemental material\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_paper_details(html_url):\n",
    "    # Step 1: Fetch HTML content\n",
    "    response = requests.get(html_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Step 2: Extract title\n",
    "    title = soup.find('h1', class_='ltx_title ltx_title_document').text.strip()\n",
    "    \n",
    "   # Step 3: Extract authors\n",
    "    authors = []\n",
    "    authors_tag = soup.find_all('span', class_='ltx_creator ltx_role_author')\n",
    "    for tag in authors_tag:\n",
    "        author_names = tag.find_all('span', class_='ltx_text ltx_font_bold')\n",
    "        for author_name in author_names:\n",
    "            authors.append(author_name.text.strip())\n",
    "    \n",
    "    authors = \", \".join(authors) if authors else \"Authors not found\"\n",
    "    \n",
    "    # Step 4: Find and extract the main content (assuming it's within an article tag)\n",
    "    article_content = soup.find('article')\n",
    "    if article_content:\n",
    "        # Extract all paragraphs within the article, excluding side headings\n",
    "        paragraphs = []\n",
    "        for p in article_content.find_all('h2'):\n",
    "            if 'class' in p.attrs and 'ltx_title ltx_title_section' in p['class']:\n",
    "                continue  # Skip side headings or other labeled sections\n",
    "            paragraphs.append(p.text.strip())\n",
    "        \n",
    "        paper_text = \"\\n\".join(paragraphs)\n",
    "        \n",
    "    else:\n",
    "        paper_text = \"Article content section not found.\"\n",
    "\n",
    "    # Combine title, authors, and full text into a single string\n",
    "    output = f\"Title:\\n{title}\\n\\nAuthors:\\n{authors}\\n\\nFull Text:\\n{paper_text}\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "def main():\n",
    "    # URL of the HTML page\n",
    "    html_url = 'https://arxiv.org/html/2407.01519v1'\n",
    "    \n",
    "    # Extract paper details and full text\n",
    "    paper_output = extract_paper_details(html_url)\n",
    "    \n",
    "    # Print the output\n",
    "    print(paper_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffIR2VR-Zero: Zero-Shot Video Restoration with\n",
      "Diffusion-based Image Restoration Models\n",
      "Chang-Han Yeh1\n",
      "Chin-Yang Lin1\n",
      "Zhixiang Wang2\n",
      "Chi-Wei Hsiao3\n",
      "Ting-Hsuan Chen1\n",
      "Yu-Lun Liu1\n",
      "1National Yang Ming Chiao Tung University\n",
      "2University of Tokyo\n",
      "3MediaTek Inc.\n",
      "Abstract\n",
      "This paper introduces a method for zero-shot video restoration using pre-trained im-\n",
      "age restoration diffusion models. Traditional video restoration methods often need\n",
      "retraining for different settings and struggle with limited generalization across vari-\n",
      "ous degradation types and datasets. Our approach uses a hierarchical token merging\n",
      "strategy for keyframes and local frames, combined with a hybrid correspondence\n",
      "mechanism that blends optical flow and feature-based nearest neighbor matching\n",
      "(latent merging). We show that our method not only achieves top performance in\n",
      "zero-shot video restoration but also significantly surpasses trained models in gener-\n",
      "alization across diverse datasets and extreme degradations (8× super-resolution and\n",
      "high-standard deviation video denoising). We present evidence through quantitative\n",
      "metrics and visual comparisons on various challenging datasets. Additionally, our\n",
      "technique works with any 2D restoration diffusion model, offering a versatile and\n",
      "powerful tool for video enhancement tasks without extensive retraining. This re-\n",
      "search leads to more efficient and widely applicable video restoration technologies,\n",
      "supporting advancements in fields that require high-quality video output. See our\n",
      "project page for video results: jimmycv07.github.io/DiffIR2VR_web.\n",
      "1\n",
      "Introduction\n",
      "Video restoration is a valuable topic that transforms low-quality video into high-quality video. It\n",
      "usually involves video denoising, super-resolution, and deblurring. The state-of-art methods that\n",
      "employ convolutional neural networks (CNNs) [2, 32, 57] or transformers [19, 49, 78] trained on\n",
      "large-scale data achieve incredible effectiveness. However, the regression-based methods often result\n",
      "in blurry outputs without realistic details (Fig. 2(a)). Furthermore, the degradations they address are\n",
      "typically well-defined (e.g., bicubic downsampling, given noise standard deviation), and models are\n",
      "often tailored to specific degradations only. This limitation restricts their generalization capabilities,\n",
      "as different settings often require additional paired data and retraining the model.\n",
      "Diffusion models recently are adapted to image restorations [85, 45] Because of their powerful\n",
      "generative ability, they can hallucinate realistic details. But this ability inherently comes with high\n",
      "randomness. As a result, directly performing per-frame inference to process videos leads to severe\n",
      "flickering (Fig. 2(b)). This phenomenon is even more pronounced in Latent Diffusion Models (LDM)\n",
      "because the decoder will magnify the randomness.\n",
      "A potential solution to reduce the temporal flickering is to fine-tune or train a single-image diffusion\n",
      "model by inserting 3D convolution and temporal attention layers into it. However, the modified\n",
      "model needs to be trained on videos to impose temporal consistency, which often requires unfordable\n",
      "computational resources (e.g., 32 A100-80G GPUs for video upscaling [102]). Moreover, different\n",
      "tasks necessitate retraining a model.\n",
      "Given limited computational resources, we present a novel zero-shot video restoration framework that\n",
      "transforms low-quality input videos into temporally consistent high-quality outputs. We design two\n",
      "arXiv:2407.01519v1  [cs.CV]  1 Jul 2024\n",
      "Input\n",
      "Result\n",
      "Input\n",
      "Result\n",
      "Input\n",
      "Result\n",
      "Figure 1: Zero-shot temporal-consistent diffusion model for video restoration. Given a pre-trained\n",
      "diffusion model for single-image restoration, our method generates temporally consistent restored\n",
      "video with fine details without any further training.\n",
      "training-free modules— hierarchical latent warping, hybrid flow-guided spatial-aware token merging—\n",
      "to enforce temporal consistency in both latent and token (feature from the attention layer) spaces.\n",
      "Our method can be applied to any pre-trained image diffusion model without additional training\n",
      "or fine-tuning. Extensive experiments demonstrate that our video restoration method outperforms\n",
      "state-of-the-art approaches in both video quality and temporal consistency, even under extreme\n",
      "degradation conditions. To summarize, our main contributions are as follows:\n",
      "• We propose a novel, zero-shot video restoration method that achieves realistic results and\n",
      "maintains temporal consistency, compatible with any image-based diffusion models.\n",
      "• Our training-free framework manipulates both latent and token spaces to enforce semantic\n",
      "consistency across frames, introducing hierarchical latent warping to maintain consistency\n",
      "within and between batches and improving token merging with flow correspondence and\n",
      "spatial information.\n",
      "• Our method demonstrates state-of-the-art restoration results, especially in scenarios of\n",
      "extreme degradation and large motion, which handle various levels of degradation with\n",
      "a single model, offering greater generalizability and robustness compared to traditional\n",
      "regression-based methods.\n",
      "2\n",
      "Related Work\n",
      "Video Restoration.\n",
      "Video restoration aims to restore high-quality frames from degraded videos,\n",
      "addressing issues such as noise, blur, and low resolution [7, 9, 31, 39, 96, 101, 47, 48]. This task\n",
      "is more challenging than image restoration because it requires maintaining temporal consistency\n",
      "across frames. Learning-based approaches often employ architectures like optical flow warping [30,\n",
      "58, 67, 68, 88], deformable convolutions [7, 8, 17, 77, 80, 81, 103], and attention mechanisms to\n",
      "handle temporal dependencies [5, 40, 43, 44, 98]. One major limitation is their dependency on paired\n",
      "high-quality (HQ) and low-quality (LQ) data for training [10, 86, 93], which is even more difficult to\n",
      "2\n",
      "Input frames\n",
      "(a) FMA-Net\n",
      "(b) DiffBIR (per-frame)\n",
      "(c) Ours\n",
      "Figure 2: 4× video super-resolution results. (a) Traditional regression-based methods such as\n",
      "FMA-Net [96] are limited to the training data domain and tend to produce blurry results when\n",
      "encountering out-of-domain inputs. (b) Although applying image-based diffusion models such as\n",
      "DiffBIR [45] to individual frames can generate realistic details, these details often lack consistency\n",
      "across frames. (c) Our method leverages an image diffusion model to restore videos, achieving both\n",
      "realistic and consistent results without any additional training.\n",
      "obtain for videos than for images. Moreover, most existing approaches [37, 36, 38, 40, 44] assume\n",
      "predefined degradation processes, reducing their effectiveness in real-world applications where\n",
      "degradations are unknown and diverse, thus leading to poor generalization performance. Additionally,\n",
      "these models often need retraining for different degradation levels or types [43, 46, 55, 95, 96],\n",
      "highlighting their limited generalization capabilities. Last but not least, these methods tend to lose\n",
      "significant detail, similar to image restoration [11, 42, 82, 99]\n",
      "Diffusion Models for Image Restoration.\n",
      "With significant advancements in diffusion models [13,\n",
      "18, 25, 26, 63], many diffusion-based approaches have been proposed for image restoration [21,\n",
      "26, 56, 70, 73, 79, 92]. One straightforward method involves training a diffusion model from\n",
      "scratch [63, 66, 85, 97], conditioned on low-quality images [66, 1]. However, this approach requires\n",
      "substantial computational resources. To reduce these costs, a common strategy is to introduce\n",
      "constraints into the reverse diffusion process of pre-trained models, as demonstrated by DDRM [34].\n",
      "While efficient, these methods [13, 15, 21, 34, 72, 83, 89] depend on predefined image degradation\n",
      "processes or pretrained super-resolution (SR) models, which limits their generalizability. Recent\n",
      "works have enhanced performance by fine-tuning frozen pre-trained diffusion models with additional\n",
      "trainable layers [79, 92, 100], as seen in StableSR [79] and DiffBIR [45]. Despite their effectiveness,\n",
      "these methods encounter challenges in video restoration, where the inherent randomness of the\n",
      "diffusion process can cause temporal inconsistencies across frames. Our method allows these\n",
      "methods to work on video without any training.\n",
      "Diffusion Models for Video Task.\n",
      "Building on the success of text-to-image diffusion models [3, 13,\n",
      "22, 24, 25, 53, 62, 65, 100], recent research explores using diffusion models for video tasks [20, 27–\n",
      "29, 50–52], extending pre-trained image diffusion models to video processing. Methods [12, 69], like\n",
      "Upscale-A-Video [102] and MGLD-VSR [94], achieve this by integrating and fine-tuning temporal\n",
      "layers. Upscale-A-Video adds temporal layers to UNet and VAE-Decoder, ensuring sequence\n",
      "consistency. Similarly, MGLD-VSR uses motion dynamics from low-resolution videos and calculated\n",
      "optical flows to align latent features. These methods require paired video data and substantial\n",
      "computational resources. Alternatively, zero-shot methods use existing image diffusion models to\n",
      "generate videos without training [6, 14, 16, 23, 61, 84, 91, 93, 100], employing techniques like\n",
      "token merging [4], noise shuffling, and latent warping. VidToMe [41] and TokenFlow [23] enhance\n",
      "temporal consistency by merging and aligning attention tokens across frames; Rerender-A-Video [90]\n",
      "employs latent warping [76, 87] and frame interpolation; RAVE [33] uses noise shuffling to maintain\n",
      "frame consistency in longer videos with reduced time complexity. These techniques are capable of\n",
      "generating impressive video sequences with minimal effort. However, they often produce blurry\n",
      "results and struggle with semantic consistency in demanding video restoration tasks. Inspired by\n",
      "3\n",
      "Figure 3: Pipeline of our proposed zero-shot video restoration method. We process low-quality\n",
      "(LQ) videos in batches using a diffusion model, with a keyframe randomly sampled within each batch.\n",
      "(a) At the beginning of the diffusion denoising process, hierarchical latent warping provides rough\n",
      "shape guidance both globally, through latent warping between keyframes, and locally, by propagating\n",
      "these latents within the batch. (b) Throughout most of the denoising process, tokens are merged before\n",
      "the self-attention layer. For the downsample blocks, optical flow is used to find the correspondence\n",
      "between tokens, and for the upsample blocks, cosine similarity is utilized. This hybrid flow-guided,\n",
      "spatial-aware token merging accurately identifies correspondences between tokens by leveraging both\n",
      "flow and spatial information, thereby enhancing overall consistency at the token level.\n",
      "these methods, our training-free framework manipulates latent and token spaces to ensure semantic\n",
      "consistency across frames, introducing hierarchical latent warping and improving token merging with\n",
      "flow correspondence and spatial information.\n",
      "3\n",
      "Method\n",
      "Given a low-quality video with n frames {lq1, lq2, . . . , lqn}, our goal is to restore it into a high-\n",
      "quality video {hq1, hq2, . . . , hqn} using off-the-shelf image-based diffusion models. However, as\n",
      "illustrated in Fig. 2 and Fig. 7, directly applying these models to each frame individually results in\n",
      "temporal inconsistency due to the inherent stochasticity of the diffusion models, particularly in cases\n",
      "of extreme degradation. Our method, as depicted in the Fig. 3, addresses this by enforcing temporal\n",
      "stability in both latent and token space during restoration through two main components: Hierarchical\n",
      "Latent Warping (Sec. 3.2) and Hybrid Flow-guided Spatial-aware Token Merging (Sec. 3.3). In this\n",
      "section, we first briefly introduce the background of diffusion models and video token merging in\n",
      "Sec. 3.1. We then introduce the hierarchical latent warping strategy in Sec. 3.2, hybrid flow-guided\n",
      "spatial-aware token merging in Sec. 3.3, and scheduling of them in Sec. 3.4.\n",
      "3.1\n",
      "Preliminaries\n",
      "Diffusion Models.\n",
      "Diffusion models are a type of generative model that models a data distribution\n",
      "pdata through gradual diffusing and denoising. The forward process diffuses a clean image x0 by\n",
      "Gaussian noises in T steps, given by\n",
      "xt = √αtxt−1 +\n",
      "√\n",
      "1 −αtϵt−1 ⇒xt = √¯\n",
      "αtx0 +\n",
      "√\n",
      "1 −¯\n",
      "αtϵ,\n",
      "(1)\n",
      "where t ∼[1, T], ϵt, ϵ ∼N(0, I), and ¯\n",
      "αt = Qt\n",
      "s=1 αs. The latent variable xT will be nearly a\n",
      "standard Gaussian distribution when T is large enough. A denoiser ϵθ, usually implemented with\n",
      "UNet [64], is trained to estimate the noise ϵt by minimizing Et∼[1,T ],x0,ϵt\n",
      "\u0002\n",
      "||ϵt −ϵθ(xt, t)||2\u0003\n",
      ". During\n",
      "inference, the inverse process starts from an i.i.d. noise xt, and produce a clean image x0 ∼pdata\n",
      "by gradual denoising with the well-trained denoiser over ¯\n",
      "T steps [26, 71, 74], where ¯\n",
      "T ∈[1, T].\n",
      "These unconditional generative models can be further enhanced with additional guidance, such as\n",
      "text prompts and images for guided generations. One will inject the control signal c into the denoiser\n",
      "ϵθ(xt, t, c) through training [100] or optimization [34].\n",
      "Video Token Merging.\n",
      "Video Token Merging (VidToMe) [41] enhances the temporal consistency\n",
      "of generated videos by utilizing image diffusion models to merge similar tokens within frame chunks\n",
      "4\n",
      "Key frame\n",
      "Key frame\n",
      "Key frame\n",
      "Step2: Latent Warping\n",
      "Step 3: Token Merging with Optical Flow\n",
      "Step 4: Token Merging with Similarity\n",
      "Self\n",
      "Attention\n",
      "Step 1: Keyframe Latent Warping\n",
      "Token\n",
      "Unmerging\n",
      "Cross\n",
      "Attention\n",
      "Token\n",
      "Merging\n",
      "Residual\n",
      "Block\n",
      "Correspondence\n",
      "Correspondence\n",
      "Figure 4: An illustration of our key modules.\n",
      "Without requiring any training, these modules can\n",
      "achieve coherence across frames by enforcing tem-\n",
      "poral stability in both latent and token space. Hi-\n",
      "erarchical latent warping provides global and lo-\n",
      "cal shape guidance; Hybrid spatial-aware token\n",
      "merging before the self-attention layer improves\n",
      "temporal consistency by matching similar tokens\n",
      "using optical flow in the down blocks and cosine\n",
      "similarity in the up blocks of the UNet.\n",
      "2\n",
      "Step 9\n",
      "Step 18\n",
      "Step 36\n",
      "Flow\n",
      "Figure 5: Token correspondences. Corre-\n",
      "spondences found by cosine similarity and\n",
      "by optical flow. (Top) At the beginning of\n",
      "the denoising process, the latents in the UNet\n",
      "downblocks are too noisy for cosine simi-\n",
      "larity to be effective, while optical flow es-\n",
      "timated from LQ frames remains reliable.\n",
      "(Bottom) Flow and cosine similarity often\n",
      "identify different correspondences, so a hy-\n",
      "brid approach is more effective.\n",
      "in the attention blocks. This token merging not only improves temporal coherence but also reduces\n",
      "the computational overhead of the attention mechanism by decreasing the size of token chunks.\n",
      "Given a token chunk T ∈RB×A×C, where A = w ∗h, the algorithm first separates the tokens into\n",
      "source tokens Tsrc ∈RB×A−1×C and a target token Ttar ∈RB×1×C. It then calculates the cosine\n",
      "between each source and target token, determining their corresponding similarity levels, denoted\n",
      "score ∈R((B−1)∗A)×A. The algorithm then identifies the most similar target token for each source\n",
      "token by taking the maximum value in the last column.\n",
      "s(Tsrc, Ttar) =\n",
      "Tsrc · Ttar\n",
      "∥Tsrc∥∥Ttar∥, c = max\n",
      "{t∈Ttar}(s(Tsrc, t)),\n",
      "(2)\n",
      "where s(·, ·) is the cosine similarity score and c indicates the correspondences. Next, the r most\n",
      "similar paired source-target tokens are merged, and the remaining tokens are concatenated as the\n",
      "output. Merged tokens are subsequently unmerged after self-attention to preserve the original shape\n",
      "by simply assigning the merged source-target tokens the exact same value. The token merging and\n",
      "unmerging are defined as follows:\n",
      "Tmerge = M(Tsrc, Ttar, c, r) , Tunmerge = U(Tmerge, c) ,\n",
      "(3)\n",
      "where M and U denote the merging and unmerging operations, respectively.\n",
      "3.2\n",
      "Hierarchical Latent Warping\n",
      "We introduce a hierarchical latent warping module that operates in the latent space. As illustrated in\n",
      "Fig. 4, this module provides rough shape guidance on both global and local scales by hierarchically\n",
      "propagating latents within keyframes and further from keyframes to their respective batches. Let\n",
      "ˆ\n",
      "xi\n",
      "t→0 be the predicted ˆ\n",
      "x0 latent for the ith keyframe at denoising step t. We warp the latents between\n",
      "keyframes as follows:\n",
      "ˆ\n",
      "xi\n",
      "t→0 ←Mji · ˆ\n",
      "xi\n",
      "t→0 + (1 −Mji) · W(ˆ\n",
      "xj\n",
      "t→0, fji)\n",
      "(4)\n",
      "5\n",
      "where j = i −1 and fji, Mji denotes the optical flow and the occlusion mask from lqj to lqi\n",
      "estimated by GMFlow [87]. After keyframe latent warping, these latents are further warped to the\n",
      "remaining frames following the same procedure. The flows and masks are downsampled to match\n",
      "the latent size, and these operations are omitted for simplicity. This module primarily functions in\n",
      "the early stages of the denoising process, ensuring that corresponding points between frames share\n",
      "similar latents both globally and locally from the beginning.\n",
      "3.3\n",
      "Hybrid Flow-guided Spatial-aware Token Merging\n",
      "While latent manipulation can achieve a certain degree of consistency, manipulating latents during the\n",
      "later stages of the denoising process would result in blurry outcomes. Additionally, the token space is\n",
      "highly semantically related to the image. Therefore, we propose hybrid flow-guided spatial-aware\n",
      "token merging to achieve consistency in the token space.\n",
      "Flow-guided.\n",
      "Even with low-quality video, we can identify correspondences between frames based\n",
      "on color, indicating that flow calculated from lq can still provide useful guidance. As shown in the\n",
      "top of Fig. 5, in the early stages of the denoising process, when the latents are still very noisy, cosine\n",
      "similarity struggles to find the correct correspondences, especially in the downsample blocks of the\n",
      "UNet. Therefore, we use flow for correspondences at the downsample blocks in the UNet and employ\n",
      "the confidence from forward-backward consistency check as a criterion to determine r most similar\n",
      "paired source token Tsrc and target token Ttar:\n",
      "σ = exp(−∥fsrc→tar(X(Tsrc)) + ftar→src (X(Tsrc) + fsrc→tar(X(Tsrc)))∥2\n",
      "2),\n",
      "(5)\n",
      "where σ is the confidence, X(Tsrc) is the spatial location of Tsrc, and fsrc→tar, ftar→src denotes the\n",
      "forward and backward flow between Tsrc and Ttar. Thus, the proposed flow-guided token merging is\n",
      "termed:\n",
      "Tmerge = M(Tsrc, Ttar, fsrc→tar, σ, r).\n",
      "(6)\n",
      "Fig. 4 provides a clearer illustration of our proposed component. Additionally, as shown at the bottom\n",
      "of Fig. 5, flow and cosine similarity identify different correspondences, so a hybrid approach can\n",
      "provide comprehensive guidance, leading to improved temporal consistency and overall video quality.\n",
      "Spatial-awareness and Padding Removal.\n",
      "Directly finding correspondences relying on cosine\n",
      "similarity can easily lead to mismatches in places with uniform textures, especially video backgrounds\n",
      "(e.g., sky, sand, grass), bottom of Fig. 5, resulting in blurrier outcomes. Given that corresponding\n",
      "points in adjacent frames are typically spatially close in videos, leveraging spatial information is\n",
      "crucial for accurate correspondence. We can effectively utilize this information by weighting the\n",
      "cosine similarity scores with the tokens’ spatial distances. The weighted scores are defined as:\n",
      "s′\n",
      "ij = sij · e−τ , with τ =\n",
      "\u0004\u0002\n",
      "∥X(i) −X(j)∥2\n",
      "2\n",
      "\u0003\n",
      "/R\n",
      "\u0005\n",
      ",\n",
      "(7)\n",
      "where X(i), X(j) is the spatial location of the ith source token and the jth target token; R is a\n",
      "hyperparameter defining the radius of the region with uniform weight.\n",
      "Images are often padded to propagate through the UNet, and we find that this padding can significantly\n",
      "impact the correspondences found in the tokens. While it is acceptable for padding to correspond to\n",
      "padding in another token, when the token feature dimension is low, cosine similarity may mistakenly\n",
      "identify padding as corresponding to the actual image content. This issue persists until the later stages\n",
      "of the denoising process. Please refer to the appendix for the non-padding correspondence figure. To\n",
      "address this, we remove padding before merging and add it back when unmerging.\n",
      "Merging Ratio Annealing.\n",
      "For restoration tasks that demand fine details, maintaining a high\n",
      "merging ratio in the later stages of the denoising process can result in blurred and unrealistic outcomes.\n",
      "To address this, we employ ratio annealing to gradually reduce the merging ratio, preserving detail\n",
      "and realism in the restored video. The merging ratio of the ith denoising step is computed as:\n",
      "ri = r · cos\n",
      "\u0012π\n",
      "2 · max\n",
      "\u0012\n",
      "min\n",
      "\u0012\n",
      "δ ·\n",
      "i −ibeg\n",
      "iend −ibeg\n",
      ", 1\n",
      "\u0013\n",
      ", 0\n",
      "\u0013\u0013\n",
      ",\n",
      "(8)\n",
      "where ibeg, iend are predefined steps indicating the beginning and end of the merging process, and δ\n",
      "represents a hyperparameter for controlling the annealing speed.\n",
      "6\n",
      "Table 1: Quantitative comparisons. (Left) 4× and 8× video super-resolution on the SPMCS [75]\n",
      "and DAVIS [60] datasets. (Right) video denoising of various noise levels on the REDS30 [55]\n",
      "dataset. The best and second performances are marked in red and blue, respectively. E∗\n",
      "warp denotes\n",
      "Ewarp(×10−3) and Einter, LPIPSinter denotes interpolation error and LPIPS.\n",
      "SD ×4\n",
      "DiffBIR\n",
      "Metrics\n",
      "VidToMe\n",
      "FMA-Net\n",
      "Frame\n",
      "Ours\n",
      "Frame\n",
      "Ours\n",
      "SPMCS\n",
      "PSNR ↑\n",
      "20.516\n",
      "21.910\n",
      "20.573\n",
      "20.636\n",
      "21.534\n",
      "21.843\n",
      "SSIM ↑\n",
      "0.471\n",
      "0.617\n",
      "0.490\n",
      "0.517\n",
      "0.544\n",
      "0.572\n",
      "LPIPS ↓\n",
      "0.352\n",
      "0.230\n",
      "0.298\n",
      "0.286\n",
      "0.261\n",
      "0.258\n",
      "E∗\n",
      "warp ↓\n",
      "0.531\n",
      "0.157\n",
      "1.058\n",
      "8.290\n",
      "0.571\n",
      "0.571\n",
      "Einter ↓\n",
      "10.102\n",
      "3.271\n",
      "13.817\n",
      "11.961\n",
      "10.257\n",
      "9.712\n",
      "LPIPSinter ↓\n",
      "0.218\n",
      "0.015\n",
      "0.241\n",
      "0.226\n",
      "0.177\n",
      "0.158\n",
      "DAVIS ×4\n",
      "PSNR ↑\n",
      "23.948\n",
      "25.215\n",
      "23.504\n",
      "23.843\n",
      "23.780\n",
      "24.182\n",
      "SSIM ↑\n",
      "0.608\n",
      "0.727\n",
      "0.584\n",
      "0.618\n",
      "0.601\n",
      "0.621\n",
      "LPIPS ↓\n",
      "0.298\n",
      "0.347\n",
      "0.277\n",
      "0.272\n",
      "0.264\n",
      "0.262\n",
      "E∗\n",
      "warp ↓\n",
      "0.512\n",
      "0.186\n",
      "0.912\n",
      "0.745\n",
      "0.654\n",
      "0.474\n",
      "Einter ↓\n",
      "14.615\n",
      "11.558\n",
      "18.125\n",
      "17.431\n",
      "16.529\n",
      "14.666\n",
      "LPIPSinter ↓\n",
      "0.278\n",
      "0.078\n",
      "0.292\n",
      "0.274\n",
      "0.266\n",
      "0.232\n",
      "DAVIS ×8\n",
      "PSNR ↑\n",
      "22.570\n",
      "22.690\n",
      "20.268\n",
      "20.519\n",
      "21.964\n",
      "22.331\n",
      "SSIM ↑\n",
      "0.527\n",
      "0.594\n",
      "0.446\n",
      "0.424\n",
      "0.502\n",
      "0.519\n",
      "LPIPS ↓\n",
      "0.454\n",
      "0.528\n",
      "0.470\n",
      "0.434\n",
      "0.362\n",
      "0.367\n",
      "E∗\n",
      "warp ↓\n",
      "0.523\n",
      "0.351\n",
      "2.199\n",
      "1.759\n",
      "0.964\n",
      "0.699\n",
      "Einter ↓\n",
      "14.117\n",
      "13.978\n",
      "24.496\n",
      "21.746\n",
      "17.981\n",
      "15.853\n",
      "LPIPSinter ↓\n",
      "0.379\n",
      "0.132\n",
      "0.457\n",
      "0.442\n",
      "0.372\n",
      "0.333\n",
      "DiffBIR\n",
      "σ\n",
      "Metrics\n",
      "VRT\n",
      "Shift-Net\n",
      "VidToMe\n",
      "Frame\n",
      "Ours\n",
      "75\n",
      "PSNR ↑\n",
      "25.050\n",
      "21.033\n",
      "23.791\n",
      "24.585\n",
      "24.520\n",
      "SSIM ↑\n",
      "0.787\n",
      "0.381\n",
      "0.618\n",
      "0.649\n",
      "0.649\n",
      "LPIPS ↓\n",
      "0.275\n",
      "0.735\n",
      "0.296\n",
      "0.276\n",
      "0.275\n",
      "E∗\n",
      "warp ↓\n",
      "0.314\n",
      "1.757\n",
      "0.765\n",
      "0.751\n",
      "0.706\n",
      "Einter ↓\n",
      "17.825\n",
      "27.094\n",
      "21.751\n",
      "21.798\n",
      "21.166\n",
      "LPIPSinter ↓\n",
      "0.095\n",
      "0.501\n",
      "0.287\n",
      "0.275\n",
      "0.264\n",
      "100\n",
      "PSNR ↑\n",
      "24.582\n",
      "22.573\n",
      "24.606\n",
      "24.524\n",
      "24.534\n",
      "SSIM ↑\n",
      "0.744\n",
      "0.484\n",
      "0.676\n",
      "0.648\n",
      "0.652\n",
      "LPIPS ↓\n",
      "0.346\n",
      "0.518\n",
      "0.318\n",
      "0.275\n",
      "0.271\n",
      "E∗\n",
      "warp ↓\n",
      "0.294\n",
      "1.126\n",
      "0.781\n",
      "0.763\n",
      "0.696\n",
      "Einter ↓\n",
      "17.079\n",
      "23.424\n",
      "21.460\n",
      "21.835\n",
      "20.639\n",
      "LPIPSinter ↓\n",
      "0.095\n",
      "0.375\n",
      "0.278\n",
      "0.281\n",
      "0.267\n",
      "random\n",
      "PSNR ↑\n",
      "24.989\n",
      "21.113\n",
      "23.692\n",
      "24.579\n",
      "24.508\n",
      "SSIM ↑\n",
      "0.780\n",
      "0.386\n",
      "0.615\n",
      "0.650\n",
      "0.649\n",
      "LPIPS ↓\n",
      "0.284\n",
      "0.728\n",
      "0.303\n",
      "0.276\n",
      "0.270\n",
      "E∗\n",
      "warp ↓\n",
      "0.363\n",
      "1.896\n",
      "0.772\n",
      "0.755\n",
      "0.713\n",
      "Einter ↓\n",
      "18.147\n",
      "27.565\n",
      "21.929\n",
      "21.743\n",
      "21.140\n",
      "LPIPSinter ↓\n",
      "0.099\n",
      "0.542\n",
      "0.291\n",
      "0.282\n",
      "0.272\n",
      "Table 2: Ablation studies for 8× VSR on DAVIS [59] test sets. (Left) different correspondence\n",
      "matching methods. (Right) the proposed components applied at different stages of the denoising\n",
      "process. We apply our two proposed components, hierarchical latent warping (HLW) and hybrid\n",
      "spatial-aware token merging (HS-ToMe) at the early, mid, and late stages of the denoising process.\n",
      "Down\n",
      "blocks\n",
      "Up\n",
      "blocks\n",
      "Spatial-\n",
      "aware\n",
      "LPIPS ↓\n",
      "E∗\n",
      "warp ↓\n",
      "LPIPSinter ↓\n",
      "Flow\n",
      "Flow\n",
      "–\n",
      "0.518\n",
      "1.214\n",
      "0.563\n",
      "Cos\n",
      "Cos\n",
      "–\n",
      "0.390\n",
      "0.736\n",
      "0.350\n",
      "Cos\n",
      "Flow\n",
      "–\n",
      "0.507\n",
      "1.049\n",
      "0.545\n",
      "Flow\n",
      "Cos\n",
      "–\n",
      "0.375\n",
      "0.677\n",
      "0.347\n",
      "Flow\n",
      "Cos\n",
      "✓\n",
      "0.367\n",
      "0.699\n",
      "0.333\n",
      "HLW (Sec. 3.2)\n",
      "HS-ToMe (Sec. 3.3)\n",
      "Early\n",
      "Mid\n",
      "Late\n",
      "Early\n",
      "Mid\n",
      "Late\n",
      "LPIPS ↓\n",
      "E∗\n",
      "warp ↓\n",
      "LPIPSinter ↓\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "0.362\n",
      "0.964\n",
      "0.372\n",
      "✓\n",
      "–\n",
      "–\n",
      "✓\n",
      "–\n",
      "–\n",
      "0.368\n",
      "0.887\n",
      "0.369\n",
      "✓\n",
      "✓\n",
      "–\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "0.43\n",
      "0.804\n",
      "0.383\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "0.411\n",
      "0.704\n",
      "0.339\n",
      "✓\n",
      "–\n",
      "–\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "0.367\n",
      "0.699\n",
      "0.333\n",
      "3.4\n",
      "Scheduling\n",
      "As depicted in Fig. 3, at the initial stage of the diffusion denoising process, hierarchical latent warping\n",
      "offers rough shape guidance on a global scale by warping latents between keyframes and on a local\n",
      "scale by propagating these latents within the batch. During the majority of the denoising process,\n",
      "tokens are processed with our hybrid spatial-aware token merging before entering the attention layer.\n",
      "This component further improves temporal consistency by matching similar tokens, utilizing both\n",
      "flow and spatial information.\n",
      "4\n",
      "Experiments\n",
      "Testing Dataset.\n",
      "For video super-resolution, we evaluate on SPMCS [95] and DAVIS [59] testing\n",
      "sets, with two downsample scales (×4, ×8), following the same degradation pipeline of RealBa-\n",
      "sicVSR [10] to generate LQ-HQ video pairs. For video denoising, we evaluate on REDS30 [54] with\n",
      "3 different noise levels (std. = 75, 100, and std. is uniformly sampled from the range [50, 100]).\n",
      "Evaluation Metrics.\n",
      "We evaluate the restoration performance based on two aspects: (1) image\n",
      "quality, using LPIPS, SSIM, and PSNR; (2) temporal consistency, using warping error Ewarp, in-\n",
      "terpolation error, and interpolation LPIPS. Since LPIPS better reflects visual quality, we propose\n",
      "interpolation LPIPS, based on the interpolation error used in a previous study [41], to more accurately\n",
      "measure video continuity from a visual perspective. This involves interpolating a target frame from\n",
      "its previous and next frames and computing the LPIPS between the estimated and target frames.\n",
      "Implementation Details.\n",
      "The experiment is conducted on an NVIDIA RTX 4090 GPU. We\n",
      "apply our method to DiffBIR [85] and SDx4 upscaler [1], both image-based diffusion models, to\n",
      "demonstrate the propsed method’s compatibility with different models. Noted that for models that\n",
      "are restricted to a super-resolution scale of 4×, we will apply the process twice and then use bicubic\n",
      "downsampling to achieve 8× results.\n",
      "7\n",
      "Input Video\n",
      "Flow + Cosine\n",
      "Flow + Flow\n",
      "Cosine + Cosine\n",
      "Cosine + Flow\n",
      "Ours\n",
      "𝑥\n",
      "𝑡\n",
      "𝑡\n",
      "𝑥\n",
      "Figure 6: Comparison of temporal profile. We examine a row of pixels and track changes over\n",
      "time. The profiles from Flow + Flow and Cosine + Flow methods exhibit noise, indicating flickering\n",
      "artifacts. The Cosine + Cosine method shows smoother profiles but contains some discontinuities.\n",
      "Flow + Cosine demonstrates improved consistency but retains some distortions. Utilizing flow, cosine,\n",
      "and spatial-aware techniques, our method achieves the most seamless and consistent transitions,\n",
      "effectively minimizing artifacts.\n",
      "VidToMe\n",
      "DiffBIR\n",
      "(per-frame)\n",
      "Ours\n",
      "(DiffBIR)\n",
      "GT\n",
      "FMA-Net\n",
      "Input\n",
      "Figure 7: Qualitative comparisons on 8× video super-resolution. As shown in the first row, the\n",
      "low-quality input lacks almost all details. In the zoomed-in patches, our method produces clearer and\n",
      "more consistent results.\n",
      "4.1\n",
      "Comparisons with State-of-the-Art Methods\n",
      "To verify the effectiveness of our approach, we compare it with several state-of-the-art methods,\n",
      "including FMA-Net [96] for video super-resolution, and VRT [43] and Shift-Net [39] for video\n",
      "denoising. We also compare our method to per-frame restoration and the application of VidToMe [41],\n",
      "a zero-shot video editing method, onto the same model as ours.\n",
      "Video Super-resolution.\n",
      "As shown in Tab. 1, regression-based methods like FMA-Net [96] perform\n",
      "better on datasets like SPMCS that have minimal motion. However, their generalization ability\n",
      "diminishes significantly with increased motion or severe degradation. VidToMe [41] can generate\n",
      "highly consistent results, but they are often very blurry, leading to poor visual quality. In contrast,\n",
      "our method enhances temporal consistency while maintaining the generation quality of the original\n",
      "diffusion model, making it the most competitive approach. Fig. 7 provides visualizations of two\n",
      "challenging VSR cases. FMA-Net fails to produce sharp results due to domain gaps between training\n",
      "and testing. Diffusion-based image restoration method DiffBIR [45] and SD×4 upscaler [1] can\n",
      "generate sharp results with details, while per-frame processing makes the result video temporal\n",
      "inconsistent and jitters across frames. On the contrary, our zero-shot video restoration framework\n",
      "restores a low-quality input video into a temporally consistent high-quality video.\n",
      "Video Denoising.\n",
      "Video denoising, compared to VSR, is a simpler task for regression models, as\n",
      "they can often find the correct pixel value given a sufficiently large batch size. However, our method\n",
      "consistently outperforms others in terms of visual quality (LPIPS) and remains highly robust even as\n",
      "8\n",
      "VRT\n",
      "DiffBIR\n",
      "(per-frame)\n",
      "Ours\n",
      "GT\n",
      "Shift-Net\n",
      "Input\n",
      "Figure 8: Qualitative comparisons on video denoising in REDS30 [54] dataset for σ = 100. Our\n",
      "method effectively denoises and generates detailed results while maintaining temporal coherence.\n",
      "degradation becomes severe. Fig. 8 visualizes the denoising results on the REDS30 dataset. Shift-\n",
      "Net [39] fails to remove all noise, likely due to the out-of-domain noise level; VRT [43] produces\n",
      "smooth results but lacks fine details. Although DiffBIR [45] generates highly detailed images, it\n",
      "suffers from poor temporal consistency, as evident in the changes to the pedestrian’s head and the\n",
      "statue’s face. In contrast, our method preserves both fine details and temporal consistency, effectively\n",
      "balancing these two aspects.\n",
      "4.2\n",
      "Ablation Study\n",
      "Ways of Identifying Correspondence.\n",
      "Tab. 2 presents an ablation study comparing different\n",
      "approaches (optical flow and cosine similarity) for finding correspondences and their order in the\n",
      "UNet. As detailed in Sec. 3.3, the hybrid approach of using optical flow at the downsample blocks and\n",
      "cosine similarity at the upsample blocks achieves the best performance. Additionally, our proposed\n",
      "spatial-aware token merging further enhances performance by utilizing spatial information to guide\n",
      "correspondences. The comparisons in Fig. 6 also indicate that our results are smoother, demonstrating\n",
      "better temporal stability.\n",
      "Applied Stages in the Denoising Process.\n",
      "Tab. 2 presents an ablation study evaluating the ap-\n",
      "plication of our two proposed components, hierarchical latent warping (HLW, Sec. 3.2) and hybrid\n",
      "spatial-aware token merging (HS-ToMe, Sec. 3.3), at the early, mid, and late stages of the denoising\n",
      "process. The results indicate that applying latent warping in the mid or late stages can significantly\n",
      "degrade the generated outcomes. Furthermore, ensuring consistency in the token space is crucial for\n",
      "achieving coherent and high-quality results.\n",
      "5\n",
      "Conclusion\n",
      "We introduce a novel zero-shot video restoration framework utilizing pre-trained image-based dif-\n",
      "fusion models, eliminating the need for extensive retraining. Our approach integrates hierarchical\n",
      "latent warping and hybrid flow-guided, spatial-aware token merging, significantly enhancing temporal\n",
      "consistency and video quality under various degradation conditions. Experimental results demonstrate\n",
      "that our framework surpasses existing methods both in quality and consistency.\n",
      "Limitations.\n",
      "Our zero-shot video restoration framework has some limitations. Random keyframe\n",
      "sampling may not always select the most representative frames, potentially affecting restoration\n",
      "quality, especially if frames with severe degradation are chosen. Additionally, the sensitivity of the\n",
      "LDM decoder to minor variations in input latents can cause flickering, particularly in dynamic scenes.\n",
      "Future improvements will focus on refining keyframe selection and stabilizing the decoder output to\n",
      "enhance the practical application of diffusion-based video restoration methods.\n",
      "9\n",
      "References\n",
      "[1] Stable diffusion x4 upscaler, 2023.\n",
      "URL https://huggingface.co/stabilityai/\n",
      "stable-diffusion-x4-upscaler.\n",
      "[2] Saad Albawi, Tareq Abed Mohammed, and Saad Al-Zawi. Understanding of a convolutional\n",
      "neural network. In 2017 international conference on engineering and technology (ICET),\n",
      "pages 1–6. Ieee, 2017.\n",
      "[3] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of\n",
      "natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition (CVPR), pages 18208–18218, June 2022.\n",
      "[4] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and\n",
      "Judy Hoffman. Token merging: Your ViT but faster. In International Conference on Learning\n",
      "Representations, 2023.\n",
      "[5] Jiezhang Cao, Yawei Li, Kai Zhang, and Luc Van Gool. Video super-resolution transformer.\n",
      "arXiv preprint arXiv:2106.06847, 2021.\n",
      "[6] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra. Pix2video: Video editing using image\n",
      "diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\n",
      "pages 23206–23217, 2023.\n",
      "[7] Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Basicvsr: The\n",
      "search for essential components in video super-resolution and beyond. In Proceedings of the\n",
      "IEEE conference on computer vision and pattern recognition, 2021.\n",
      "[8] Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Understanding\n",
      "deformable alignment in video super-resolution. In Proceedings of the AAAI conference on\n",
      "artificial intelligence, volume 35, pages 973–981, 2021.\n",
      "[9] Kelvin C.K. Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy. Basicvsr++:\n",
      "Improving video super-resolution with enhanced propagation and alignment. 2021.\n",
      "[10] Kelvin C.K. Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy. Investigating\n",
      "tradeoffs in real-world video super-resolution. In IEEE Conference on Computer Vision and\n",
      "Pattern Recognition, 2022.\n",
      "[11] Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui\n",
      "Guo. Real-world blind super-resolution via feature matching with implicit high-resolution\n",
      "priors. In Proceedings of the 30th ACM International Conference on Multimedia, pages\n",
      "1329–1338, 2022.\n",
      "[12] Zhikai Chen, Fuchen Long, Zhaofan Qiu, Ting Yao, Wengang Zhou, Jiebo Luo, and Tao\n",
      "Mei. Learning spatial adaptation and temporal coherence in diffusion models for video\n",
      "super-resolution. arXiv preprint arXiv:2403.17000, 2024.\n",
      "[13] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr:\n",
      "Conditioning method for denoising diffusion probabilistic models. In Proceedings of the\n",
      "IEEE/CVF International Conference on Computer Vision, 2021.\n",
      "[14] Ernie Chu, Tzuhsuan Huang, Shuo-Yen Lin, and Jun-Cheng Chen. Medm: Mediating image\n",
      "diffusion models for video-to-video translation with temporal correspondence guidance. In\n",
      "Proceedings of the AAAI Conference on Artificial Intelligence, 2024.\n",
      "[15] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models\n",
      "for inverse problems using manifold constraints. Advances in Neural Information Processing\n",
      "Systems, 35:25683–25696, 2022.\n",
      "[16] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-\n",
      "Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flow-guided\n",
      "attention for consistent text-to-video editing. arXiv preprint arXiv:2310.05922, 2023.\n",
      "10\n",
      "[17] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei.\n",
      "Deformable convolutional networks. In 2017 IEEE International Conference on Computer\n",
      "Vision (ICCV), pages 764–773, 2017. doi: 10.1109/ICCV.2017.89.\n",
      "[18] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\n",
      "Advances in neural information processing systems, 34:8780–8794, 2021.\n",
      "[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\n",
      "Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\n",
      "Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\n",
      "recognition at scale, 2021.\n",
      "[20] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Ger-\n",
      "manidis. Structure and content-guided video synthesis with diffusion models. In Proceedings\n",
      "of the IEEE/CVF International Conference on Computer Vision, pages 7346–7356, 2023.\n",
      "[21] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang,\n",
      "and Bo Dai. Generative diffusion prior for unified image restoration and enhancement. In\n",
      "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n",
      "9935–9946, 2023.\n",
      "[22] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel Cohen-\n",
      "Or. Encoder-based domain tuning for fast personalization of text-to-image models. ACM\n",
      "Transactions on Graphics (TOG), 42(4):1–13, 2023.\n",
      "[23] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion\n",
      "features for consistent video editing. arXiv preprint arXiv:2307.10373, 2023.\n",
      "[24] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and\n",
      "Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of\n",
      "the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10696–10706,\n",
      "2022.\n",
      "[25] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\n",
      "Prompt-to-prompt image editing with cross attention control. In International Conference on\n",
      "Learning Representations, 2023.\n",
      "[26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\n",
      "in neural information processing systems, 33:6840–6851, 2020.\n",
      "[27] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,\n",
      "Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High\n",
      "definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.\n",
      "[28] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and\n",
      "David J Fleet. Video diffusion models. Advances in Neural Information Processing Systems,\n",
      "35:8633–8646, 2022.\n",
      "[29] Yaosi Hu, Zhenzhong Chen, and Chong Luo. Lamd: Latent motion diffusion for video\n",
      "generation. arXiv preprint arXiv:2304.11603, 2023.\n",
      "[30] Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang, Ka Chun Cheung, Hongwei Qin,\n",
      "Jifeng Dai, and Hongsheng Li. Flowformer: A transformer architecture for optical flow. In\n",
      "European conference on computer vision, pages 668–685. Springer, 2022.\n",
      "[31] Takashi Isobe, Xu Jia, Shuhang Gu, Songjiang Li, Shengjin Wang, and Qi Tian. Video\n",
      "super-resolution with recurrent structure-detail network, 2020.\n",
      "[32] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network\n",
      "for modelling sentences. arXiv preprint arXiv:1404.2188, 2014.\n",
      "[33] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M. Rehg, and Pinar Yanardag. Rave:\n",
      "Randomized noise shuffling for fast and consistent video editing with diffusion models. In\n",
      "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.\n",
      "11\n",
      "[34] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restora-\n",
      "tion models. In Advances in Neural Information Processing Systems, 2022.\n",
      "[35] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Kon-\n",
      "rad Schindler. Repurposing diffusion-based image generators for monocular depth estimation.\n",
      "2024.\n",
      "[36] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using\n",
      "very deep convolutional networks. In Proceedings of the IEEE conference on computer vision\n",
      "and pattern recognition, pages 1646–1654, 2016.\n",
      "[37] Tae Hyun Kim, Seungjun Nah, and Kyoung Mu Lee. Dynamic video deblurring using a locally\n",
      "adaptive blur model. IEEE transactions on pattern analysis and machine intelligence, 40(10):\n",
      "2374–2387, 2017.\n",
      "[38] Lingshun Kong, Jiangxin Dong, Jianjun Ge, Mingqiang Li, and Jinshan Pan.\n",
      "Efficient\n",
      "frequency domain-based transformers for high-quality image deblurring. In Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5886–5895, 2023.\n",
      "[39] Dasong Li, Xiaoyu Shi, Yi Zhang, Ka Chun Cheung, Simon See, Xiaogang Wang, Hongwei\n",
      "Qin, and Hongsheng Li. A simple baseline for video restoration with grouped spatial-temporal\n",
      "shift. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n",
      "(CVPR), pages 9822–9832, June 2023.\n",
      "[40] Wenbo Li, Xin Tao, Taian Guo, Lu Qi, Jiangbo Lu, and Jiaya Jia. Mucan: Multi-correspondence\n",
      "aggregation network for video super-resolution. In Computer Vision–ECCV 2020: 16th\n",
      "European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part X 16, pages\n",
      "335–351. Springer, 2020.\n",
      "[41] Xirui Li, Chao Ma, Xiaokang Yang, and Ming-Hsuan Yang. Vidtome: Video token merging\n",
      "for zero-shot video editing. In Proceedings of the IEEE/CVF Conference on Computer Vision\n",
      "and Pattern Recognition, 2024.\n",
      "[42] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.\n",
      "Swinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF interna-\n",
      "tional conference on computer vision, pages 1833–1844, 2021.\n",
      "[43] Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang, Rakesh Ranjan, Yawei Li, Radu Timofte,\n",
      "and Luc Van Gool. Vrt: A video restoration transformer. arXiv preprint arXiv:2201.12288,\n",
      "2022.\n",
      "[44] Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan, Eddy Ilg, Simon Green, Jiezhang\n",
      "Cao, Kai Zhang, Radu Timofte, and Luc V Gool. Recurrent video restoration transformer\n",
      "with guided deformable attention. Advances in Neural Information Processing Systems, 35:\n",
      "378–393, 2022.\n",
      "[45] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Bo Dai, Fanghua Yu, Wanli Ouyang,\n",
      "Yu Qiao, and Chao Dong. Diffbir: Towards blind image restoration with generative diffusion\n",
      "prior, 2024.\n",
      "[46] Ce Liu and Deqing Sun. On bayesian adaptive video super resolution. IEEE transactions on\n",
      "pattern analysis and machine intelligence, 36(2):346–360, 2013.\n",
      "[47] Yu-Lun Liu, Yi-Tung Liao, Yen-Yu Lin, and Yung-Yu Chuang. Deep video frame interpolation\n",
      "using cyclic frame generation. 2019.\n",
      "[48] Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, and Jia-Bin Huang. Hybrid\n",
      "neural fusion for full-frame video stabilization. 2021.\n",
      "[49] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\n",
      "Guo. Swin transformer: Hierarchical vision transformer using shifted windows, 2021.\n",
      "[50] Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding. Vdt:\n",
      "An empirical study on video diffusion with transformers. arXiv preprint arXiv:2305.13311,\n",
      "2023.\n",
      "12\n",
      "[51] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli\n",
      "Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-\n",
      "quality video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision\n",
      "and Pattern Recognition, pages 10209–10218, 2023.\n",
      "[52] Kangfu Mei and Vishal Patel. Vidm: Video implicit diffusion models. In Proceedings of the\n",
      "AAAI Conference on Artificial Intelligence, volume 37, pages 9117–9125, 2023.\n",
      "[53] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying\n",
      "Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image\n",
      "diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38,\n",
      "pages 4296–4304, 2024.\n",
      "[54] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte,\n",
      "and Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and super-resolution: Dataset\n",
      "and study. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n",
      "Workshops (CVPRW), pages 1996–2005, 2019. doi: 10.1109/CVPRW.2019.00251.\n",
      "[55] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte,\n",
      "and Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and super-resolution: Dataset\n",
      "and study. In Proceedings of the IEEE/CVF conference on computer vision and pattern\n",
      "recognition workshops, pages 0–0, 2019.\n",
      "[56] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mc-\n",
      "Grew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and\n",
      "editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n",
      "[57] Keiron O’shea and Ryan Nash. An introduction to convolutional neural networks. arXiv\n",
      "preprint arXiv:1511.08458, 2015.\n",
      "[58] Jinshan Pan, Haoran Bai, and Jinhui Tang. Cascaded deep video deblurring using temporal\n",
      "sharpness prior. In Proceedings of the IEEE/CVF conference on computer vision and pattern\n",
      "recognition, pages 3043–3051, 2020.\n",
      "[59] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. A\n",
      "benchmark dataset and evaluation methodology for video object segmentation. In 2016 IEEE\n",
      "Conference on Computer Vision and Pattern Recognition (CVPR), pages 724–732, 2016. doi:\n",
      "10.1109/CVPR.2016.85.\n",
      "[60] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and\n",
      "Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology for video\n",
      "object segmentation. In Proceedings of the IEEE conference on computer vision and pattern\n",
      "recognition, pages 724–732, 2016.\n",
      "[61] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and\n",
      "Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In Proceedings\n",
      "of the IEEE/CVF International Conference on Computer Vision, pages 15932–15942, 2023.\n",
      "[62] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\n",
      "text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3,\n",
      "2022.\n",
      "[63] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.\n",
      "High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\n",
      "conference on computer vision and pattern recognition, pages 10684–10695, 2022.\n",
      "[64] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\n",
      "U-net: Convolutional networks\n",
      "for biomedical image segmentation. In Medical image computing and computer-assisted\n",
      "intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9,\n",
      "2015, proceedings, part III 18, pages 234–241. Springer, 2015.\n",
      "13\n",
      "[65] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\n",
      "Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.\n",
      "Photorealistic text-to-image diffusion models with deep language understanding. Advances in\n",
      "neural information processing systems, 35:36479–36494, 2022.\n",
      "[66] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad\n",
      "Norouzi. Image super-resolution via iterative refinement. IEEE transactions on pattern\n",
      "analysis and machine intelligence, 45(4):4713–4726, 2022.\n",
      "[67] Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, Dasong Li, Manyuan Zhang, Ka Chun Cheung,\n",
      "Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Videoflow: Exploiting temporal\n",
      "cues for multi-frame optical flow estimation. In Proceedings of the IEEE/CVF International\n",
      "Conference on Computer Vision, pages 12469–12480, 2023.\n",
      "[68] Xiaoyu Shi, Zhaoyang Huang, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See,\n",
      "Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer++: Masked cost volume autoencod-\n",
      "ing for pretraining optical flow estimation. In Proceedings of the IEEE/CVF Conference on\n",
      "Computer Vision and Pattern Recognition, pages 1599–1610, 2023.\n",
      "[69] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu,\n",
      "Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without\n",
      "text-video data. arXiv preprint arXiv:2209.14792, 2022.\n",
      "[70] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\n",
      "vised learning using nonequilibrium thermodynamics. In International conference on machine\n",
      "learning, pages 2256–2265. PMLR, 2015.\n",
      "[71] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\n",
      "preprint arXiv:2010.02502, 2020.\n",
      "[72] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion\n",
      "models for inverse problems. In International Conference on Learning Representations, 2022.\n",
      "[73] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\n",
      "Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv\n",
      "preprint arXiv:2011.13456, 2020.\n",
      "[74] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models, 2023.\n",
      "[75] Xin Tao, Hongyun Gao, Renjie Liao, Jue Wang, and Jiaya Jia. Detail-revealing deep video\n",
      "super-resolution. In Proceedings of the IEEE international conference on computer vision,\n",
      "pages 4472–4480, 2017.\n",
      "[76] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In\n",
      "Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,\n",
      "Proceedings, Part II 16, pages 402–419. Springer, 2020.\n",
      "[77] Yapeng Tian, Yulun Zhang, Yun Fu, and Chenliang Xu. Tdan: Temporally-deformable\n",
      "alignment network for video super-resolution. In Proceedings of the IEEE/CVF conference on\n",
      "computer vision and pattern recognition, pages 3360–3369, 2020.\n",
      "[78] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
      "Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\n",
      "processing systems, 30, 2017.\n",
      "[79] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Ex-\n",
      "ploiting diffusion prior for real-world image super-resolution. arXiv preprint arXiv:2305.07015,\n",
      "2023.\n",
      "[80] Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, and Chen Change Loy. Edvr: Video\n",
      "restoration with enhanced deformable convolutional networks. In Proceedings of the IEEE/CVF\n",
      "conference on computer vision and pattern recognition workshops, pages 0–0, 2019.\n",
      "14\n",
      "[81] Xintao Wang, Ke Yu, Kelvin C.K. Chan, Chao Dong, and Chen Change Loy.\n",
      "Basicsr.\n",
      "https://github.com/xinntao/BasicSR, 2020.\n",
      "[82] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world\n",
      "blind super-resolution with pure synthetic data. In Proceedings of the IEEE/CVF international\n",
      "conference on computer vision, pages 1905–1914, 2021.\n",
      "[83] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising\n",
      "diffusion null-space model. arXiv preprint arXiv:2212.00490, 2022.\n",
      "[84] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne\n",
      "Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image\n",
      "diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International\n",
      "Conference on Computer Vision, pages 7623–7633, 2023.\n",
      "[85] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming\n",
      "Yang, and Luc Van Gool. Diffir: Efficient diffusion model for image restoration. In Proceedings\n",
      "of the IEEE/CVF International Conference on Computer Vision, pages 13095–13105, 2023.\n",
      "[86] Liangbin Xie, Xintao Wang, Shuwei Shi, Jinjin Gu, Chao Dong, and Ying Shan. Mitigating\n",
      "artifacts in real-world video super-resolution models. In Proceedings of the AAAI Conference\n",
      "on Artificial Intelligence, volume 37, pages 2956–2964, 2023.\n",
      "[87] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. Gmflow: Learning\n",
      "optical flow via global matching. In Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition, pages 8121–8130, 2022.\n",
      "[88] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William T Freeman. Video en-\n",
      "hancement with task-oriented flow. International Journal of Computer Vision (IJCV), 127(8):\n",
      "1106–1125, 2019.\n",
      "[89] Peiqing Yang, Shangchen Zhou, Qingyi Tao, and Chen Change Loy. Pgdiff: Guiding diffusion\n",
      "models for versatile face restoration via partial guidance. Advances in Neural Information\n",
      "Processing Systems, 36, 2024.\n",
      "[90] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender a video: Zero-shot\n",
      "text-guided video-to-video translation. In SIGGRAPH Asia 2023 Conference Papers, pages\n",
      "1–11, 2023.\n",
      "[91] Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change Loy. Fresco: Spatial-temporal\n",
      "correspondence for zero-shot video translation. In Proceedings of IEEE Conference on\n",
      "Computer Vision and Pattern Recognition, 2024.\n",
      "[92] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. Pixel-aware stable diffusion for realistic\n",
      "image super-resolution and personalized stylization. arXiv preprint arXiv:2308.14469, 2023.\n",
      "[93] Xi Yang, Wangmeng Xiang, Hui Zeng, and Lei Zhang. Real-world video super-resolution:\n",
      "A benchmark dataset and a decomposition based learning scheme. In Proceedings of the\n",
      "IEEE/CVF International Conference on Computer Vision, pages 4781–4790, 2021.\n",
      "[94] Xi Yang, Chenhang He, Jianqi Ma, and Lei Zhang. Motion-guided latent diffusion for\n",
      "temporally consistent real-world video super-resolution. arXiv preprint arXiv:2312.00853,\n",
      "2023.\n",
      "[95] Peng Yi, Zhongyuan Wang, Kui Jiang, Junjun Jiang, and Jiayi Ma. Progressive fusion video\n",
      "super-resolution network via exploiting non-local spatio-temporal correlations. In Proceedings\n",
      "of the IEEE/CVF international conference on computer vision, pages 3106–3115, 2019.\n",
      "[96] Geunhyuk Youk, Jihyong Oh, and Munchurl Kim. Fma-net: Flow-guided dynamic filtering and\n",
      "iterative feature refinement with multi-attention for joint video super-resolution and deblurring.\n",
      "In CVPR, 2024.\n",
      "[97] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. Resshift: Efficient diffusion model\n",
      "for image super-resolution by residual shifting. Advances in Neural Information Processing\n",
      "Systems, 36, 2024.\n",
      "15\n",
      "[98] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and\n",
      "Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In\n",
      "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n",
      "5728–5739, 2022.\n",
      "[99] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a practical degradation\n",
      "model for deep blind image super-resolution. In Proceedings of the IEEE/CVF International\n",
      "Conference on Computer Vision, pages 4791–4800, 2021.\n",
      "[100] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image\n",
      "diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer\n",
      "Vision, pages 3836–3847, 2023.\n",
      "[101] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network\n",
      "for image super-resolution. In Proceedings of the IEEE conference on computer vision and\n",
      "pattern recognition, pages 2472–2481, 2018.\n",
      "[102] Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, and Chen Change Loy. Upscale-\n",
      "a-video: Temporal-consistent diffusion model for real-world video super-resolution. arXiv\n",
      "preprint arXiv:2312.06640, 2023.\n",
      "[103] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable,\n",
      "better results. In Proceedings of the IEEE/CVF conference on computer vision and pattern\n",
      "recognition, pages 9308–9316, 2019.\n",
      "A\n",
      "Appendix / supplemental material\n",
      "In this supplementary material, we first provide additional details on the testing datasets and evaluation\n",
      "metrics. Subsequently, we present more visual comparisons of various methods.\n",
      "A.1\n",
      "Correspondences identified by cosine similarity without padding removal\n",
      "Fig. 9 shows that padding value will affect the matching severely.\n",
      "Figure 9: The padded regions affect the matching severely.\n",
      "A.2\n",
      "Additional Application: Consistent Video Depth\n",
      "Our zero-shot framework is applicable to any pre-trained image-based diffusion models and could\n",
      "improve the predicted video consistency. Therefore, we integrate our proposed zero-shot framework\n",
      "into a state-of-the-art latent diffusion-based monocular depth estimator: Marigold [35]. Fig. 10 shows\n",
      "that integrating our proposed framework into Marigold helps improve the temporal consistency of\n",
      "video depth estimation.\n",
      "16\n",
      "Input\n",
      "Marigold [35]\n",
      "Ours\n",
      "Input\n",
      "Marigold [35]\n",
      "Ours\n",
      "Input\n",
      "Marigold [35]\n",
      "Ours\n",
      "Figure 10: Integrating our proposed framework into Marigold [35] helps improve the temporal\n",
      "consistency of video depth estimation.\n",
      "17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def extract_text_from_online_pdf(pdf_url):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Fetch the PDF file content\n",
    "        response = requests.get(pdf_url)\n",
    "        pdf_bytes = BytesIO(response.content)\n",
    "\n",
    "        # Open the PDF document\n",
    "        pdf_document = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "\n",
    "        # Iterate through each page and extract text\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "\n",
    "        # Close the PDF document\n",
    "        pdf_document.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "\n",
    "    return text\n",
    "\n",
    "# URL of the online PDF\n",
    "pdf_url = 'https://arxiv.org/pdf/2407.01519.pdf'\n",
    "\n",
    "# Extract text from the online PDF\n",
    "extracted_text = extract_text_from_online_pdf(pdf_url)\n",
    "\n",
    "# Print the extracted text\n",
    "print(extracted_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      3\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m documents \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(extracted_text)\n",
      "File \u001b[1;32mc:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\langchain_text_splitters\\base.py:93\u001b[0m, in \u001b[0;36mTextSplitter.split_documents\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m     91\u001b[0m texts, metadatas \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m---> 93\u001b[0m     texts\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mpage_content)\n\u001b[0;32m     94\u001b[0m     metadatas\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_documents(texts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/2407.01519.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='DiffIR2VR-Zero: Zero-Shot Video Restoration with\\nDiffusion-based Image Restoration Models\\nChang-Han Yeh1Chin-Yang Lin1Zhixiang Wang2\\nChi-Wei Hsiao3Ting-Hsuan Chen1Yu-Lun Liu1\\n1National Yang Ming Chiao Tung University2University of Tokyo3MediaTek Inc.\\nAbstract\\nThis paper introduces a method for zero-shot video restoration using pre-trained im-\\nage restoration diffusion models. Traditional video restoration methods often need\\nretraining for different settings and struggle with limited generalization across vari-\\nous degradation types and datasets. Our approach uses a hierarchical token merging\\nstrategy for keyframes and local frames, combined with a hybrid correspondence\\nmechanism that blends optical flow and feature-based nearest neighbor matching\\n(latent merging). We show that our method not only achieves top performance in\\nzero-shot video restoration but also significantly surpasses trained models in gener-\\nalization across diverse datasets and extreme degradations (8 ×super-resolution and\\nhigh-standard deviation video denoising). We present evidence through quantitative\\nmetrics and visual comparisons on various challenging datasets. Additionally, our\\ntechnique works with any 2D restoration diffusion model, offering a versatile and\\npowerful tool for video enhancement tasks without extensive retraining. This re-\\nsearch leads to more efficient and widely applicable video restoration technologies,\\nsupporting advancements in fields that require high-quality video output. See our\\nproject page for video results: jimmycv07.github.io/DiffIR2VR_web.\\n1 Introduction\\nVideo restoration is a valuable topic that transforms low-quality video into high-quality video. It\\nusually involves video denoising, super-resolution, and deblurring. The state-of-art methods that\\nemploy convolutional neural networks (CNNs) [ 2,32,57] or transformers [ 19,49,78] trained on\\nlarge-scale data achieve incredible effectiveness. However, the regression-based methods often result\\nin blurry outputs without realistic details (Fig. 2(a)). Furthermore, the degradations they address are\\ntypically well-defined ( e.g., bicubic downsampling, given noise standard deviation), and models are\\noften tailored to specific degradations only. This limitation restricts their generalization capabilities,\\nas different settings often require additional paired data and retraining the model.\\nDiffusion models recently are adapted to image restorations [ 85,45] Because of their powerful\\ngenerative ability, they can hallucinate realistic details. But this ability inherently comes with high\\nrandomness. As a result, directly performing per-frame inference to process videos leads to severe\\nflickering (Fig. 2(b)). This phenomenon is even more pronounced in Latent Diffusion Models (LDM)\\nbecause the decoder will magnify the randomness.\\nA potential solution to reduce the temporal flickering is to fine-tune or train a single-image diffusion\\nmodel by inserting 3D convolution and temporal attention layers into it. However, the modified\\nmodel needs to be trained on videos to impose temporal consistency, which often requires unfordable\\ncomputational resources ( e.g., 32 A100-80G GPUs for video upscaling [102]). Moreover, different\\ntasks necessitate retraining a model.\\nGiven limited computational resources, we present a novel zero-shot video restoration framework that\\ntransforms low-quality input videos into temporally consistent high-quality outputs. We design twoarXiv:2407.01519v1  [cs.CV]  1 Jul 2024', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 0}),\n",
       " Document(page_content='Input\\nResult\\nInput\\nResult\\nInput\\nResultFigure 1: Zero-shot temporal-consistent diffusion model for video restoration. Given a pre-trained\\ndiffusion model for single-image restoration, our method generates temporally consistent restored\\nvideo with fine details without any further training.\\ntraining-free modules— hierarchical latent warping, hybrid flow-guided spatial-aware token merging—\\nto enforce temporal consistency in both latent and token (feature from the attention layer) spaces.\\nOur method can be applied to any pre-trained image diffusion model without additional training\\nor fine-tuning. Extensive experiments demonstrate that our video restoration method outperforms\\nstate-of-the-art approaches in both video quality and temporal consistency, even under extreme\\ndegradation conditions. To summarize, our main contributions are as follows:\\n•We propose a novel, zero-shot video restoration method that achieves realistic results and\\nmaintains temporal consistency, compatible with any image-based diffusion models.\\n•Our training-free framework manipulates both latent and token spaces to enforce semantic\\nconsistency across frames, introducing hierarchical latent warping to maintain consistency\\nwithin and between batches and improving token merging with flow correspondence and\\nspatial information.\\n•Our method demonstrates state-of-the-art restoration results, especially in scenarios of\\nextreme degradation and large motion, which handle various levels of degradation with\\na single model, offering greater generalizability and robustness compared to traditional\\nregression-based methods.\\n2 Related Work\\nVideo Restoration. Video restoration aims to restore high-quality frames from degraded videos,\\naddressing issues such as noise, blur, and low resolution [ 7,9,31,39,96,101,47,48]. This task\\nis more challenging than image restoration because it requires maintaining temporal consistency\\nacross frames. Learning-based approaches often employ architectures like optical flow warping [ 30,\\n58,67,68,88], deformable convolutions [ 7,8,17,77,80,81,103], and attention mechanisms to\\nhandle temporal dependencies [ 5,40,43,44,98]. One major limitation is their dependency on paired\\nhigh-quality (HQ) and low-quality (LQ) data for training [ 10,86,93], which is even more difficult to\\n2', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 1}),\n",
       " Document(page_content='Input frames(a) FMA-Net(b) DiffBIR(per-frame)(c) Ours\\nFigure 2: 4×video super-resolution results. (a) Traditional regression-based methods such as\\nFMA-Net [ 96] are limited to the training data domain and tend to produce blurry results when\\nencountering out-of-domain inputs. (b) Although applying image-based diffusion models such as\\nDiffBIR [ 45] to individual frames can generate realistic details, these details often lack consistency\\nacross frames. (c) Our method leverages an image diffusion model to restore videos, achieving both\\nrealistic and consistent results without any additional training.\\nobtain for videos than for images. Moreover, most existing approaches [ 37,36,38,40,44] assume\\npredefined degradation processes, reducing their effectiveness in real-world applications where\\ndegradations are unknown and diverse, thus leading to poor generalization performance. Additionally,\\nthese models often need retraining for different degradation levels or types [ 43,46,55,95,96],\\nhighlighting their limited generalization capabilities. Last but not least, these methods tend to lose\\nsignificant detail, similar to image restoration [11, 42, 82, 99]\\nDiffusion Models for Image Restoration. With significant advancements in diffusion models [ 13,\\n18,25,26,63], many diffusion-based approaches have been proposed for image restoration [ 21,\\n26,56,70,73,79,92]. One straightforward method involves training a diffusion model from\\nscratch [ 63,66,85,97], conditioned on low-quality images [ 66,1]. However, this approach requires\\nsubstantial computational resources. To reduce these costs, a common strategy is to introduce\\nconstraints into the reverse diffusion process of pre-trained models, as demonstrated by DDRM [ 34].\\nWhile efficient, these methods [ 13,15,21,34,72,83,89] depend on predefined image degradation\\nprocesses or pretrained super-resolution (SR) models, which limits their generalizability. Recent\\nworks have enhanced performance by fine-tuning frozen pre-trained diffusion models with additional\\ntrainable layers [ 79,92,100], as seen in StableSR [ 79] and DiffBIR [ 45]. Despite their effectiveness,\\nthese methods encounter challenges in video restoration, where the inherent randomness of the\\ndiffusion process can cause temporal inconsistencies across frames. Our method allows these\\nmethods to work on video without any training.\\nDiffusion Models for Video Task. Building on the success of text-to-image diffusion models [ 3,13,\\n22,24,25,53,62,65,100], recent research explores using diffusion models for video tasks [ 20,27–\\n29,50–52], extending pre-trained image diffusion models to video processing. Methods [ 12,69], like\\nUpscale-A-Video [ 102] and MGLD-VSR [ 94], achieve this by integrating and fine-tuning temporal\\nlayers. Upscale-A-Video adds temporal layers to UNet and V AE-Decoder, ensuring sequence\\nconsistency. Similarly, MGLD-VSR uses motion dynamics from low-resolution videos and calculated\\noptical flows to align latent features. These methods require paired video data and substantial\\ncomputational resources. Alternatively, zero-shot methods use existing image diffusion models to\\ngenerate videos without training [ 6,14,16,23,61,84,91,93,100], employing techniques like\\ntoken merging [ 4], noise shuffling, and latent warping. VidToMe [ 41] and TokenFlow [ 23] enhance\\ntemporal consistency by merging and aligning attention tokens across frames; Rerender-A-Video [ 90]\\nemploys latent warping [ 76,87] and frame interpolation; RA VE [ 33] uses noise shuffling to maintain\\nframe consistency in longer videos with reduced time complexity. These techniques are capable of\\ngenerating impressive video sequences with minimal effort. However, they often produce blurry\\nresults and struggle with semantic consistency in demanding video restoration tasks. Inspired by\\n3', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 2}),\n",
       " Document(page_content='Figure 3: Pipeline of our proposed zero-shot video restoration method. We process low-quality\\n(LQ) videos in batches using a diffusion model, with a keyframe randomly sampled within each batch.\\n(a) At the beginning of the diffusion denoising process, hierarchical latent warping provides rough\\nshape guidance both globally, through latent warping between keyframes, and locally, by propagating\\nthese latents within the batch. (b) Throughout most of the denoising process, tokens are merged before\\nthe self-attention layer. For the downsample blocks, optical flow is used to find the correspondence\\nbetween tokens, and for the upsample blocks, cosine similarity is utilized. This hybrid flow-guided,\\nspatial-aware token merging accurately identifies correspondences between tokens by leveraging both\\nflow and spatial information, thereby enhancing overall consistency at the token level.\\nthese methods, our training-free framework manipulates latent and token spaces to ensure semantic\\nconsistency across frames, introducing hierarchical latent warping and improving token merging with\\nflow correspondence and spatial information.\\n3 Method\\nGiven a low-quality video with nframes {lq1, lq2, . . . , lq n}, our goal is to restore it into a high-\\nquality video {hq1, hq2, . . . , hq n}using off-the-shelf image-based diffusion models. However, as\\nillustrated in Fig. 2 and Fig. 7, directly applying these models to each frame individually results in\\ntemporal inconsistency due to the inherent stochasticity of the diffusion models, particularly in cases\\nof extreme degradation. Our method, as depicted in the Fig. 3, addresses this by enforcing temporal\\nstability in both latent and token space during restoration through two main components: Hierarchical\\nLatent Warping (Sec. 3.2) and Hybrid Flow-guided Spatial-aware Token Merging (Sec. 3.3). In this\\nsection, we first briefly introduce the background of diffusion models and video token merging in\\nSec. 3.1. We then introduce the hierarchical latent warping strategy in Sec. 3.2, hybrid flow-guided\\nspatial-aware token merging in Sec. 3.3, and scheduling of them in Sec. 3.4.\\n3.1 Preliminaries\\nDiffusion Models. Diffusion models are a type of generative model that models a data distribution\\npdatathrough gradual diffusing and denoising. The forward process diffuses a clean image x0by\\nGaussian noises in Tsteps, given by\\nxt=√αtxt−1+√\\n1−αtϵt−1⇒xt=√¯αtx0+√\\n1−¯αtϵ, (1)\\nwhere t∼[1, T],ϵt, ϵ∼ N (0,I), and ¯αt=Qt\\ns=1αs. The latent variable xTwill be nearly a\\nstandard Gaussian distribution when Tis large enough. A denoiser ϵθ, usually implemented with\\nUNet [ 64], is trained to estimate the noise ϵtby minimizing Et∼[1,T],x0,ϵt\\x02\\n||ϵt−ϵθ(xt, t)||2\\x03\\n. During\\ninference, the inverse process starts from an i.i.d. noise xt, and produce a clean image x0∼pdata\\nby gradual denoising with the well-trained denoiser over ¯Tsteps [ 26,71,74], where ¯T∈[1, T].\\nThese unconditional generative models can be further enhanced with additional guidance, such as\\ntext prompts and images for guided generations. One will inject the control signal cinto the denoiser\\nϵθ(xt, t, c)through training [100] or optimization [34].\\nVideo Token Merging. Video Token Merging (VidToMe) [ 41] enhances the temporal consistency\\nof generated videos by utilizing image diffusion models to merge similar tokens within frame chunks\\n4', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 3}),\n",
       " Document(page_content='Key frame Key frame\\n Key frame\\nStep2: Latent Warping\\nStep 3: Token Merging with Optical Flow\\nStep 4: Token Merging with SimilaritySelf\\nAttentionStep 1: Keyframe Latent Warping\\nToken\\nUnmergin g\\nCross\\nAttentionToken\\nMergingResidual\\nBlockCorrespondence\\nCorrespondence\\nFigure 4: An illustration of our key modules.\\nWithout requiring any training, these modules can\\nachieve coherence across frames by enforcing tem-\\nporal stability in both latent and token space. Hi-\\nerarchical latent warping provides global and lo-\\ncal shape guidance; Hybrid spatial-aware token\\nmerging before the self-attention layer improves\\ntemporal consistency by matching similar tokens\\nusing optical flow in the down blocks and cosine\\nsimilarity in the up blocks of the UNet.\\n2\\nStep 9\\nStep 18\\nStep 36\\nFlowFigure 5: Token correspondences. Corre-\\nspondences found by cosine similarity and\\nby optical flow. ( Top) At the beginning of\\nthe denoising process, the latents in the UNet\\ndownblocks are too noisy for cosine simi-\\nlarity to be effective, while optical flow es-\\ntimated from LQ frames remains reliable.\\n(Bottom ) Flow and cosine similarity often\\nidentify different correspondences, so a hy-\\nbrid approach is more effective.\\nin the attention blocks. This token merging not only improves temporal coherence but also reduces\\nthe computational overhead of the attention mechanism by decreasing the size of token chunks.\\nGiven a token chunk T∈RB×A×C, where A=w∗h, the algorithm first separates the tokens into\\nsource tokens Tsrc∈RB×A−1×Cand a target token Ttar∈RB×1×C. It then calculates the cosine\\nbetween each source and target token, determining their corresponding similarity levels, denoted\\nscore∈R((B−1)∗A)×A. The algorithm then identifies the most similar target token for each source\\ntoken by taking the maximum value in the last column.\\ns(Tsrc,Ttar) =Tsrc·Ttar\\n∥Tsrc∥∥Ttar∥, c= max\\n{t∈Ttar}(s(Tsrc,t)), (2)\\nwhere s(·,·)is the cosine similarity score and cindicates the correspondences. Next, the rmost\\nsimilar paired source-target tokens are merged, and the remaining tokens are concatenated as the\\noutput. Merged tokens are subsequently unmerged after self-attention to preserve the original shape\\nby simply assigning the merged source-target tokens the exact same value. The token merging and\\nunmerging are defined as follows:\\nTmerge=M(Tsrc,Ttar, c, r ),Tunmerge =U(Tmerge, c), (3)\\nwhere MandUdenote the merging and unmerging operations, respectively.\\n3.2 Hierarchical Latent Warping\\nWe introduce a hierarchical latent warping module that operates in the latent space. As illustrated in\\nFig. 4, this module provides rough shape guidance on both global and local scales by hierarchically\\npropagating latents within keyframes and further from keyframes to their respective batches. Let\\nˆxi\\nt→0be the predicted ˆx0latent for the ithkeyframe at denoising step t. We warp the latents between\\nkeyframes as follows:\\nˆxi\\nt→0←Mji·ˆxi\\nt→0+ (1−Mji)· W(ˆxj\\nt→0, fji) (4)\\n5', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 4}),\n",
       " Document(page_content='where j=i−1andfji,Mjidenotes the optical flow and the occlusion mask from lqjtolqi\\nestimated by GMFlow [ 87]. After keyframe latent warping, these latents are further warped to the\\nremaining frames following the same procedure. The flows and masks are downsampled to match\\nthe latent size, and these operations are omitted for simplicity. This module primarily functions in\\nthe early stages of the denoising process, ensuring that corresponding points between frames share\\nsimilar latents both globally and locally from the beginning.\\n3.3 Hybrid Flow-guided Spatial-aware Token Merging\\nWhile latent manipulation can achieve a certain degree of consistency, manipulating latents during the\\nlater stages of the denoising process would result in blurry outcomes. Additionally, the token space is\\nhighly semantically related to the image. Therefore, we propose hybrid flow-guided spatial-aware\\ntoken merging to achieve consistency in the token space.\\nFlow-guided. Even with low-quality video, we can identify correspondences between frames based\\non color, indicating that flow calculated from lqcan still provide useful guidance. As shown in the\\ntop of Fig. 5, in the early stages of the denoising process, when the latents are still very noisy, cosine\\nsimilarity struggles to find the correct correspondences, especially in the downsample blocks of the\\nUNet. Therefore, we use flow for correspondences at the downsample blocks in the UNet and employ\\nthe confidence from forward-backward consistency check as a criterion to determine rmost similar\\npaired source token Tsrcand target token Ttar:\\nσ= exp( −∥fsrc→tar(X(Tsrc)) +ftar→src(X(Tsrc) +fsrc→tar(X(Tsrc)))∥2\\n2), (5)\\nwhere σis the confidence, X(Tsrc)is the spatial location of Tsrc, andfsrc→tar,ftar→srcdenotes the\\nforward and backward flow between TsrcandTtar. Thus, the proposed flow-guided token merging is\\ntermed:\\nTmerge=M(Tsrc,Ttar, fsrc→tar, σ, r ). (6)\\nFig. 4 provides a clearer illustration of our proposed component. Additionally, as shown at the bottom\\nof Fig. 5, flow and cosine similarity identify different correspondences, so a hybrid approach can\\nprovide comprehensive guidance, leading to improved temporal consistency and overall video quality.\\nSpatial-awareness and Padding Removal. Directly finding correspondences relying on cosine\\nsimilarity can easily lead to mismatches in places with uniform textures, especially video backgrounds\\n(e.g., sky, sand, grass), bottom of Fig. 5, resulting in blurrier outcomes. Given that corresponding\\npoints in adjacent frames are typically spatially close in videos, leveraging spatial information is\\ncrucial for accurate correspondence. We can effectively utilize this information by weighting the\\ncosine similarity scores with the tokens’ spatial distances. The weighted scores are defined as:\\ns′\\nij=sij·e−τ,withτ=\\x04\\x02\\n∥X(i)−X(j)∥2\\n2\\x03\\n/R\\x05\\n, (7)\\nwhere X(i),X(j)is the spatial location of the ithsource token and the jthtarget token; Ris a\\nhyperparameter defining the radius of the region with uniform weight.\\nImages are often padded to propagate through the UNet, and we find that this padding can significantly\\nimpact the correspondences found in the tokens. While it is acceptable for padding to correspond to\\npadding in another token, when the token feature dimension is low, cosine similarity may mistakenly\\nidentify padding as corresponding to the actual image content. This issue persists until the later stages\\nof the denoising process. Please refer to the appendix for the non-padding correspondence figure. To\\naddress this, we remove padding before merging and add it back when unmerging.\\nMerging Ratio Annealing. For restoration tasks that demand fine details, maintaining a high\\nmerging ratio in the later stages of the denoising process can result in blurred and unrealistic outcomes.\\nTo address this, we employ ratio annealing to gradually reduce the merging ratio, preserving detail\\nand realism in the restored video. The merging ratio of the ithdenoising step is computed as:', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 5}),\n",
       " Document(page_content='To address this, we employ ratio annealing to gradually reduce the merging ratio, preserving detail\\nand realism in the restored video. The merging ratio of the ithdenoising step is computed as:\\nri=r·cos\\x12π\\n2·max\\x12\\nmin\\x12\\nδ·i−ibeg\\niend−ibeg,1\\x13\\n,0\\x13\\x13\\n, (8)\\nwhere ibeg,iendare predefined steps indicating the beginning and end of the merging process, and δ\\nrepresents a hyperparameter for controlling the annealing speed.\\n6', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 5}),\n",
       " Document(page_content='Table 1: Quantitative comparisons. (Left) 4×and 8×video super-resolution on the SPMCS [ 75]\\nand DA VIS [ 60] datasets. ( Right ) video denoising of various noise levels on the REDS30 [ 55]\\ndataset. The best and second performances are marked in red and blue, respectively. E∗\\nwarpdenotes\\nEwarp(×10−3)andEinter, LPIPS interdenotes interpolation error and LPIPS.\\nSD×4 DiffBIR\\nMetrics VidToMe FMA-Net Frame Ours Frame OursSPMCSPSNR ↑ 20.516 21.910 20.573 20.636 21.534 21.843\\nSSIM↑ 0.471 0.617 0.490 0.517 0.544 0.572\\nLPIPS ↓ 0.352 0.230 0.298 0.286 0.261 0.258\\nE∗\\nwarp↓ 0.531 0.157 1.058 8.290 0.571 0.571\\nEinter↓ 10.102 3.271 13.817 11.961 10.257 9.712\\nLPIPS inter↓ 0.218 0.015 0.241 0.226 0.177 0.158DA VIS ×4PSNR ↑ 23.948 25.215 23.504 23.843 23.780 24.182\\nSSIM↑ 0.608 0.727 0.584 0.618 0.601 0.621\\nLPIPS ↓ 0.298 0.347 0.277 0.272 0.264 0.262\\nE∗\\nwarp↓ 0.512 0.186 0.912 0.745 0.654 0.474\\nEinter↓ 14.615 11.558 18.125 17.431 16.529 14.666\\nLPIPS inter↓ 0.278 0.078 0.292 0.274 0.266 0.232DA VIS ×8PSNR ↑ 22.570 22.690 20.268 20.519 21.964 22.331\\nSSIM↑ 0.527 0.594 0.446 0.424 0.502 0.519\\nLPIPS ↓ 0.454 0.528 0.470 0.434 0.362 0.367\\nE∗\\nwarp↓ 0.523 0.351 2.199 1.759 0.964 0.699\\nEinter↓ 14.117 13.978 24.496 21.746 17.981 15.853\\nLPIPS inter↓ 0.379 0.132 0.457 0.442 0.372 0.333DiffBIR\\nσ Metrics VRT Shift-Net VidToMe Frame Ours75PSNR ↑25.050 21.033 23.791 24.585 24.520\\nSSIM↑ 0.787 0.381 0.618 0.649 0.649\\nLPIPS ↓ 0.275 0.735 0.296 0.276 0.275\\nE∗\\nwarp↓ 0.314 1.757 0.765 0.751 0.706\\nEinter↓17.825 27.094 21.751 21.798 21.166\\nLPIPS inter↓ 0.095 0.501 0.287 0.275 0.264100PSNR ↑24.582 22.573 24.606 24.524 24.534\\nSSIM↑ 0.744 0.484 0.676 0.648 0.652\\nLPIPS ↓ 0.346 0.518 0.318 0.275 0.271\\nE∗\\nwarp↓ 0.294 1.126 0.781 0.763 0.696\\nEinter↓17.079 23.424 21.460 21.835 20.639\\nLPIPS inter↓ 0.095 0.375 0.278 0.281 0.267randomPSNR ↑24.989 21.113 23.692 24.579 24.508\\nSSIM↑ 0.780 0.386 0.615 0.650 0.649\\nLPIPS ↓ 0.284 0.728 0.303 0.276 0.270\\nE∗\\nwarp↓ 0.363 1.896 0.772 0.755 0.713\\nEinter↓18.147 27.565 21.929 21.743 21.140\\nLPIPS inter↓ 0.099 0.542 0.291 0.282 0.272\\nTable 2: Ablation studies for 8 ×VSR on DA VIS [ 59] test sets. (Left) different correspondence\\nmatching methods. ( Right ) the proposed components applied at different stages of the denoising\\nprocess. We apply our two proposed components, hierarchical latent warping (HLW) and hybrid\\nspatial-aware token merging (HS-ToMe) at the early, mid, and late stages of the denoising process.\\nDown\\nblocksUp\\nblocksSpatial-\\nawareLPIPS ↓E∗\\nwarp↓LPIPS inter↓\\nFlow Flow – 0.518 1.214 0.563\\nCos Cos – 0.390 0.736 0.350\\nCos Flow – 0.507 1.049 0.545\\nFlow Cos – 0.375 0.677 0.347\\nFlow Cos ✓ 0.367 0.699 0.333HLW (Sec. 3.2) HS-ToMe (Sec. 3.3)\\nEarly Mid Late Early Mid Late LPIPS ↓E∗\\nwarp↓LPIPS inter↓\\n– – – – – – 0.362 0.964 0.372\\n✓ – – ✓ – – 0.368 0.887 0.369\\n✓ ✓ – ✓ ✓ ✓ 0.43 0.804 0.383\\n✓ ✓ ✓ ✓ ✓ ✓ 0.411 0.704 0.339\\n✓ – – ✓ ✓ ✓ 0.367 0.699 0.333\\n3.4 Scheduling\\nAs depicted in Fig. 3, at the initial stage of the diffusion denoising process, hierarchical latent warping\\noffers rough shape guidance on a global scale by warping latents between keyframes and on a local\\nscale by propagating these latents within the batch. During the majority of the denoising process,\\ntokens are processed with our hybrid spatial-aware token merging before entering the attention layer.\\nThis component further improves temporal consistency by matching similar tokens, utilizing both\\nflow and spatial information.\\n4 Experiments\\nTesting Dataset. For video super-resolution, we evaluate on SPMCS [ 95] and DA VIS [ 59] testing\\nsets, with two downsample scales ( ×4,×8), following the same degradation pipeline of RealBa-\\nsicVSR [ 10] to generate LQ-HQ video pairs. For video denoising, we evaluate on REDS30 [ 54] with\\n3 different noise levels (std. =75, 100, and std. is uniformly sampled from the range [50, 100]).\\nEvaluation Metrics. We evaluate the restoration performance based on two aspects: (1) image', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 6}),\n",
       " Document(page_content='3 different noise levels (std. =75, 100, and std. is uniformly sampled from the range [50, 100]).\\nEvaluation Metrics. We evaluate the restoration performance based on two aspects: (1) image\\nquality, using LPIPS, SSIM, and PSNR; (2) temporal consistency, using warping error Ewarp, in-\\nterpolation error, and interpolation LPIPS. Since LPIPS better reflects visual quality, we propose\\ninterpolation LPIPS, based on the interpolation error used in a previous study [ 41], to more accurately\\nmeasure video continuity from a visual perspective. This involves interpolating a target frame from\\nits previous and next frames and computing the LPIPS between the estimated and target frames.\\nImplementation Details. The experiment is conducted on an NVIDIA RTX 4090 GPU. We\\napply our method to DiffBIR [ 85] and SDx4 upscaler [ 1], both image-based diffusion models, to\\ndemonstrate the propsed method’s compatibility with different models. Noted that for models that\\nare restricted to a super-resolution scale of 4 ×, we will apply the process twice and then use bicubic\\ndownsampling to achieve 8 ×results.\\n7', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 6}),\n",
       " Document(page_content='Input Video\\nFlow + Cosine\\nFlow + FlowCosine + CosineCosine + FlowOurs𝑥𝑡𝑡𝑥Figure 6: Comparison of temporal profile. We examine a row of pixels and track changes over\\ntime. The profiles from Flow + Flow and Cosine + Flow methods exhibit noise, indicating flickering\\nartifacts. The Cosine + Cosine method shows smoother profiles but contains some discontinuities.\\nFlow + Cosine demonstrates improved consistency but retains some distortions. Utilizing flow, cosine,\\nand spatial-aware techniques, our method achieves the most seamless and consistent transitions,\\neffectively minimizing artifacts.\\nVidToMeDiffBIR(per-frame)Ours(DiffBIR)GTFMA-NetInput\\nFigure 7: Qualitative comparisons on 8 ×video super-resolution. As shown in the first row, the\\nlow-quality input lacks almost all details. In the zoomed-in patches, our method produces clearer and\\nmore consistent results.\\n4.1 Comparisons with State-of-the-Art Methods\\nTo verify the effectiveness of our approach, we compare it with several state-of-the-art methods,\\nincluding FMA-Net [ 96] for video super-resolution, and VRT [ 43] and Shift-Net [ 39] for video\\ndenoising. We also compare our method to per-frame restoration and the application of VidToMe [ 41],\\na zero-shot video editing method, onto the same model as ours.\\nVideo Super-resolution. As shown in Tab. 1, regression-based methods like FMA-Net [ 96] perform\\nbetter on datasets like SPMCS that have minimal motion. However, their generalization ability\\ndiminishes significantly with increased motion or severe degradation. VidToMe [ 41] can generate\\nhighly consistent results, but they are often very blurry, leading to poor visual quality. In contrast,\\nour method enhances temporal consistency while maintaining the generation quality of the original\\ndiffusion model, making it the most competitive approach. Fig. 7 provides visualizations of two\\nchallenging VSR cases. FMA-Net fails to produce sharp results due to domain gaps between training\\nand testing. Diffusion-based image restoration method DiffBIR [ 45] and SD ×4 upscaler [ 1] can\\ngenerate sharp results with details, while per-frame processing makes the result video temporal\\ninconsistent and jitters across frames. On the contrary, our zero-shot video restoration framework\\nrestores a low-quality input video into a temporally consistent high-quality video.\\nVideo Denoising. Video denoising, compared to VSR, is a simpler task for regression models, as\\nthey can often find the correct pixel value given a sufficiently large batch size. However, our method\\nconsistently outperforms others in terms of visual quality (LPIPS) and remains highly robust even as\\n8', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 7}),\n",
       " Document(page_content='VRTDiffBIR(per-frame)OursGTShift-NetInput\\nFigure 8: Qualitative comparisons on video denoising in REDS30 [ 54] dataset for σ= 100 .Our\\nmethod effectively denoises and generates detailed results while maintaining temporal coherence.\\ndegradation becomes severe. Fig. 8 visualizes the denoising results on the REDS30 dataset. Shift-\\nNet [ 39] fails to remove all noise, likely due to the out-of-domain noise level; VRT [ 43] produces\\nsmooth results but lacks fine details. Although DiffBIR [ 45] generates highly detailed images, it\\nsuffers from poor temporal consistency, as evident in the changes to the pedestrian’s head and the\\nstatue’s face. In contrast, our method preserves both fine details and temporal consistency, effectively\\nbalancing these two aspects.\\n4.2 Ablation Study\\nWays of Identifying Correspondence. Tab. 2 presents an ablation study comparing different\\napproaches (optical flow and cosine similarity) for finding correspondences and their order in the\\nUNet. As detailed in Sec. 3.3, the hybrid approach of using optical flow at the downsample blocks and\\ncosine similarity at the upsample blocks achieves the best performance. Additionally, our proposed\\nspatial-aware token merging further enhances performance by utilizing spatial information to guide\\ncorrespondences. The comparisons in Fig. 6 also indicate that our results are smoother, demonstrating\\nbetter temporal stability.\\nApplied Stages in the Denoising Process. Tab. 2 presents an ablation study evaluating the ap-\\nplication of our two proposed components, hierarchical latent warping (HLW, Sec. 3.2) and hybrid\\nspatial-aware token merging (HS-ToMe, Sec. 3.3), at the early, mid, and late stages of the denoising\\nprocess. The results indicate that applying latent warping in the mid or late stages can significantly\\ndegrade the generated outcomes. Furthermore, ensuring consistency in the token space is crucial for\\nachieving coherent and high-quality results.\\n5 Conclusion\\nWe introduce a novel zero-shot video restoration framework utilizing pre-trained image-based dif-\\nfusion models, eliminating the need for extensive retraining. Our approach integrates hierarchical\\nlatent warping and hybrid flow-guided, spatial-aware token merging, significantly enhancing temporal\\nconsistency and video quality under various degradation conditions. Experimental results demonstrate\\nthat our framework surpasses existing methods both in quality and consistency.\\nLimitations. Our zero-shot video restoration framework has some limitations. Random keyframe\\nsampling may not always select the most representative frames, potentially affecting restoration\\nquality, especially if frames with severe degradation are chosen. Additionally, the sensitivity of the\\nLDM decoder to minor variations in input latents can cause flickering, particularly in dynamic scenes.\\nFuture improvements will focus on refining keyframe selection and stabilizing the decoder output to\\nenhance the practical application of diffusion-based video restoration methods.\\n9', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 8}),\n",
       " Document(page_content='References\\n[1]Stable diffusion x4 upscaler, 2023. URL https://huggingface.co/stabilityai/\\nstable-diffusion-x4-upscaler .\\n[2]Saad Albawi, Tareq Abed Mohammed, and Saad Al-Zawi. Understanding of a convolutional\\nneural network. In 2017 international conference on engineering and technology (ICET) ,\\npages 1–6. Ieee, 2017.\\n[3]Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of\\nnatural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 18208–18218, June 2022.\\n[4]Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and\\nJudy Hoffman. Token merging: Your ViT but faster. In International Conference on Learning\\nRepresentations , 2023.\\n[5]Jiezhang Cao, Yawei Li, Kai Zhang, and Luc Van Gool. Video super-resolution transformer.\\narXiv preprint arXiv:2106.06847 , 2021.\\n[6]Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra. Pix2video: Video editing using image\\ndiffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\\npages 23206–23217, 2023.\\n[7]Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Basicvsr: The\\nsearch for essential components in video super-resolution and beyond. In Proceedings of the\\nIEEE conference on computer vision and pattern recognition , 2021.\\n[8]Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Understanding\\ndeformable alignment in video super-resolution. In Proceedings of the AAAI conference on\\nartificial intelligence , volume 35, pages 973–981, 2021.\\n[9]Kelvin C.K. Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy. Basicvsr++:\\nImproving video super-resolution with enhanced propagation and alignment. 2021.\\n[10] Kelvin C.K. Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy. Investigating\\ntradeoffs in real-world video super-resolution. In IEEE Conference on Computer Vision and\\nPattern Recognition , 2022.\\n[11] Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui\\nGuo. Real-world blind super-resolution via feature matching with implicit high-resolution\\npriors. In Proceedings of the 30th ACM International Conference on Multimedia , pages\\n1329–1338, 2022.\\n[12] Zhikai Chen, Fuchen Long, Zhaofan Qiu, Ting Yao, Wengang Zhou, Jiebo Luo, and Tao\\nMei. Learning spatial adaptation and temporal coherence in diffusion models for video\\nsuper-resolution. arXiv preprint arXiv:2403.17000 , 2024.\\n[13] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr:\\nConditioning method for denoising diffusion probabilistic models. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision , 2021.\\n[14] Ernie Chu, Tzuhsuan Huang, Shuo-Yen Lin, and Jun-Cheng Chen. Medm: Mediating image\\ndiffusion models for video-to-video translation with temporal correspondence guidance. In\\nProceedings of the AAAI Conference on Artificial Intelligence , 2024.\\n[15] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models\\nfor inverse problems using manifold constraints. Advances in Neural Information Processing\\nSystems , 35:25683–25696, 2022.\\n[16] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-\\nManuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flow-guided\\nattention for consistent text-to-video editing. arXiv preprint arXiv:2310.05922 , 2023.\\n10', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 9}),\n",
       " Document(page_content='[17] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei.\\nDeformable convolutional networks. In 2017 IEEE International Conference on Computer\\nVision (ICCV) , pages 764–773, 2017. doi: 10.1109/ICCV .2017.89.\\n[18] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\\nAdvances in neural information processing systems , 34:8780–8794, 2021.\\n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\\nrecognition at scale, 2021.\\n[20] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Ger-\\nmanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings\\nof the IEEE/CVF International Conference on Computer Vision , pages 7346–7356, 2023.\\n[21] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang,\\nand Bo Dai. Generative diffusion prior for unified image restoration and enhancement. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\\n9935–9946, 2023.\\n[22] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel Cohen-\\nOr. Encoder-based domain tuning for fast personalization of text-to-image models. ACM\\nTransactions on Graphics (TOG) , 42(4):1–13, 2023.\\n[23] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion\\nfeatures for consistent video editing. arXiv preprint arXiv:2307.10373 , 2023.\\n[24] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and\\nBaining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10696–10706,\\n2022.\\n[25] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\\nPrompt-to-prompt image editing with cross attention control. In International Conference on\\nLearning Representations , 2023.\\n[26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\\nin neural information processing systems , 33:6840–6851, 2020.\\n[27] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,\\nDiederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High\\ndefinition video generation with diffusion models. arXiv preprint arXiv:2210.02303 , 2022.\\n[28] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and\\nDavid J Fleet. Video diffusion models. Advances in Neural Information Processing Systems ,\\n35:8633–8646, 2022.\\n[29] Yaosi Hu, Zhenzhong Chen, and Chong Luo. Lamd: Latent motion diffusion for video\\ngeneration. arXiv preprint arXiv:2304.11603 , 2023.\\n[30] Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang, Ka Chun Cheung, Hongwei Qin,\\nJifeng Dai, and Hongsheng Li. Flowformer: A transformer architecture for optical flow. In\\nEuropean conference on computer vision , pages 668–685. Springer, 2022.\\n[31] Takashi Isobe, Xu Jia, Shuhang Gu, Songjiang Li, Shengjin Wang, and Qi Tian. Video\\nsuper-resolution with recurrent structure-detail network, 2020.\\n[32] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network\\nfor modelling sentences. arXiv preprint arXiv:1404.2188 , 2014.\\n[33] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M. Rehg, and Pinar Yanardag. Rave:\\nRandomized noise shuffling for fast and consistent video editing with diffusion models. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2024.\\n11', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 10}),\n",
       " Document(page_content='[34] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restora-\\ntion models. In Advances in Neural Information Processing Systems , 2022.\\n[35] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Kon-\\nrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation.\\n2024.\\n[36] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using\\nvery deep convolutional networks. In Proceedings of the IEEE conference on computer vision\\nand pattern recognition , pages 1646–1654, 2016.\\n[37] Tae Hyun Kim, Seungjun Nah, and Kyoung Mu Lee. Dynamic video deblurring using a locally\\nadaptive blur model. IEEE transactions on pattern analysis and machine intelligence , 40(10):\\n2374–2387, 2017.\\n[38] Lingshun Kong, Jiangxin Dong, Jianjun Ge, Mingqiang Li, and Jinshan Pan. Efficient\\nfrequency domain-based transformers for high-quality image deblurring. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5886–5895, 2023.\\n[39] Dasong Li, Xiaoyu Shi, Yi Zhang, Ka Chun Cheung, Simon See, Xiaogang Wang, Hongwei\\nQin, and Hongsheng Li. A simple baseline for video restoration with grouped spatial-temporal\\nshift. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 9822–9832, June 2023.\\n[40] Wenbo Li, Xin Tao, Taian Guo, Lu Qi, Jiangbo Lu, and Jiaya Jia. Mucan: Multi-correspondence\\naggregation network for video super-resolution. In Computer Vision–ECCV 2020: 16th\\nEuropean Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part X 16 , pages\\n335–351. Springer, 2020.\\n[41] Xirui Li, Chao Ma, Xiaokang Yang, and Ming-Hsuan Yang. Vidtome: Video token merging\\nfor zero-shot video editing. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , 2024.\\n[42] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte.\\nSwinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF interna-\\ntional conference on computer vision , pages 1833–1844, 2021.\\n[43] Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang, Rakesh Ranjan, Yawei Li, Radu Timofte,\\nand Luc Van Gool. Vrt: A video restoration transformer. arXiv preprint arXiv:2201.12288 ,\\n2022.\\n[44] Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan, Eddy Ilg, Simon Green, Jiezhang\\nCao, Kai Zhang, Radu Timofte, and Luc V Gool. Recurrent video restoration transformer\\nwith guided deformable attention. Advances in Neural Information Processing Systems , 35:\\n378–393, 2022.\\n[45] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Bo Dai, Fanghua Yu, Wanli Ouyang,\\nYu Qiao, and Chao Dong. Diffbir: Towards blind image restoration with generative diffusion\\nprior, 2024.\\n[46] Ce Liu and Deqing Sun. On bayesian adaptive video super resolution. IEEE transactions on\\npattern analysis and machine intelligence , 36(2):346–360, 2013.\\n[47] Yu-Lun Liu, Yi-Tung Liao, Yen-Yu Lin, and Yung-Yu Chuang. Deep video frame interpolation\\nusing cyclic frame generation. 2019.\\n[48] Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, and Jia-Bin Huang. Hybrid\\nneural fusion for full-frame video stabilization. 2021.\\n[49] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows, 2021.\\n[50] Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding. Vdt:\\nAn empirical study on video diffusion with transformers. arXiv preprint arXiv:2305.13311 ,\\n2023.\\n12', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 11}),\n",
       " Document(page_content='[51] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli\\nZhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-\\nquality video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pages 10209–10218, 2023.\\n[52] Kangfu Mei and Vishal Patel. Vidm: Video implicit diffusion models. In Proceedings of the\\nAAAI Conference on Artificial Intelligence , volume 37, pages 9117–9125, 2023.\\n[53] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying\\nShan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image\\ndiffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38,\\npages 4296–4304, 2024.\\n[54] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte,\\nand Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and super-resolution: Dataset\\nand study. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition\\nWorkshops (CVPRW) , pages 1996–2005, 2019. doi: 10.1109/CVPRW.2019.00251.\\n[55] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte,\\nand Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and super-resolution: Dataset\\nand study. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition workshops , pages 0–0, 2019.\\n[56] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mc-\\nGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and\\nediting with text-guided diffusion models. arXiv preprint arXiv:2112.10741 , 2021.\\n[57] Keiron O’shea and Ryan Nash. An introduction to convolutional neural networks. arXiv\\npreprint arXiv:1511.08458 , 2015.\\n[58] Jinshan Pan, Haoran Bai, and Jinhui Tang. Cascaded deep video deblurring using temporal\\nsharpness prior. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition , pages 3043–3051, 2020.\\n[59] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. A\\nbenchmark dataset and evaluation methodology for video object segmentation. In 2016 IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR) , pages 724–732, 2016. doi:\\n10.1109/CVPR.2016.85.\\n[60] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and\\nAlexander Sorkine-Hornung. A benchmark dataset and evaluation methodology for video\\nobject segmentation. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition , pages 724–732, 2016.\\n[61] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and\\nQifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In Proceedings\\nof the IEEE/CVF International Conference on Computer Vision , pages 15932–15942, 2023.\\n[62] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 , 1(2):3,\\n2022.\\n[63] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.\\nHigh-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition , pages 10684–10695, 2022.\\n[64] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks\\nfor biomedical image segmentation. In Medical image computing and computer-assisted\\nintervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9,\\n2015, proceedings, part III 18 , pages 234–241. Springer, 2015.\\n13', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 12}),\n",
       " Document(page_content='[65] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.\\nPhotorealistic text-to-image diffusion models with deep language understanding. Advances in\\nneural information processing systems , 35:36479–36494, 2022.\\n[66] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad\\nNorouzi. Image super-resolution via iterative refinement. IEEE transactions on pattern\\nanalysis and machine intelligence , 45(4):4713–4726, 2022.\\n[67] Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, Dasong Li, Manyuan Zhang, Ka Chun Cheung,\\nSimon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Videoflow: Exploiting temporal\\ncues for multi-frame optical flow estimation. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision , pages 12469–12480, 2023.\\n[68] Xiaoyu Shi, Zhaoyang Huang, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See,\\nHongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer++: Masked cost volume autoencod-\\ning for pretraining optical flow estimation. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition , pages 1599–1610, 2023.\\n[69] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu,\\nHarry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without\\ntext-video data. arXiv preprint arXiv:2209.14792 , 2022.\\n[70] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\\nvised learning using nonequilibrium thermodynamics. In International conference on machine\\nlearning , pages 2256–2265. PMLR, 2015.\\n[71] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\\npreprint arXiv:2010.02502 , 2020.\\n[72] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion\\nmodels for inverse problems. In International Conference on Learning Representations , 2022.\\n[73] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\\nBen Poole. Score-based generative modeling through stochastic differential equations. arXiv\\npreprint arXiv:2011.13456 , 2020.\\n[74] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models, 2023.\\n[75] Xin Tao, Hongyun Gao, Renjie Liao, Jue Wang, and Jiaya Jia. Detail-revealing deep video\\nsuper-resolution. In Proceedings of the IEEE international conference on computer vision ,\\npages 4472–4480, 2017.\\n[76] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In\\nComputer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,\\nProceedings, Part II 16 , pages 402–419. Springer, 2020.\\n[77] Yapeng Tian, Yulun Zhang, Yun Fu, and Chenliang Xu. Tdan: Temporally-deformable\\nalignment network for video super-resolution. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition , pages 3360–3369, 2020.\\n[78] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\\nprocessing systems , 30, 2017.\\n[79] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Ex-\\nploiting diffusion prior for real-world image super-resolution. arXiv preprint arXiv:2305.07015 ,\\n2023.\\n[80] Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, and Chen Change Loy. Edvr: Video\\nrestoration with enhanced deformable convolutional networks. In Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition workshops , pages 0–0, 2019.\\n14', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 13}),\n",
       " Document(page_content='[81] Xintao Wang, Ke Yu, Kelvin C.K. Chan, Chao Dong, and Chen Change Loy. Basicsr.\\nhttps://github.com/xinntao/BasicSR , 2020.\\n[82] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world\\nblind super-resolution with pure synthetic data. In Proceedings of the IEEE/CVF international\\nconference on computer vision , pages 1905–1914, 2021.\\n[83] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising\\ndiffusion null-space model. arXiv preprint arXiv:2212.00490 , 2022.\\n[84] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne\\nHsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image\\ndiffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision , pages 7623–7633, 2023.\\n[85] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming\\nYang, and Luc Van Gool. Diffir: Efficient diffusion model for image restoration. In Proceedings\\nof the IEEE/CVF International Conference on Computer Vision , pages 13095–13105, 2023.\\n[86] Liangbin Xie, Xintao Wang, Shuwei Shi, Jinjin Gu, Chao Dong, and Ying Shan. Mitigating\\nartifacts in real-world video super-resolution models. In Proceedings of the AAAI Conference\\non Artificial Intelligence , volume 37, pages 2956–2964, 2023.\\n[87] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. Gmflow: Learning\\noptical flow via global matching. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pages 8121–8130, 2022.\\n[88] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William T Freeman. Video en-\\nhancement with task-oriented flow. International Journal of Computer Vision (IJCV) , 127(8):\\n1106–1125, 2019.\\n[89] Peiqing Yang, Shangchen Zhou, Qingyi Tao, and Chen Change Loy. Pgdiff: Guiding diffusion\\nmodels for versatile face restoration via partial guidance. Advances in Neural Information\\nProcessing Systems , 36, 2024.\\n[90] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender a video: Zero-shot\\ntext-guided video-to-video translation. In SIGGRAPH Asia 2023 Conference Papers , pages\\n1–11, 2023.\\n[91] Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change Loy. Fresco: Spatial-temporal\\ncorrespondence for zero-shot video translation. In Proceedings of IEEE Conference on\\nComputer Vision and Pattern Recognition , 2024.\\n[92] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. Pixel-aware stable diffusion for realistic\\nimage super-resolution and personalized stylization. arXiv preprint arXiv:2308.14469 , 2023.\\n[93] Xi Yang, Wangmeng Xiang, Hui Zeng, and Lei Zhang. Real-world video super-resolution:\\nA benchmark dataset and a decomposition based learning scheme. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision , pages 4781–4790, 2021.\\n[94] Xi Yang, Chenhang He, Jianqi Ma, and Lei Zhang. Motion-guided latent diffusion for\\ntemporally consistent real-world video super-resolution. arXiv preprint arXiv:2312.00853 ,\\n2023.\\n[95] Peng Yi, Zhongyuan Wang, Kui Jiang, Junjun Jiang, and Jiayi Ma. Progressive fusion video\\nsuper-resolution network via exploiting non-local spatio-temporal correlations. In Proceedings\\nof the IEEE/CVF international conference on computer vision , pages 3106–3115, 2019.\\n[96] Geunhyuk Youk, Jihyong Oh, and Munchurl Kim. Fma-net: Flow-guided dynamic filtering and\\niterative feature refinement with multi-attention for joint video super-resolution and deblurring.\\nInCVPR , 2024.\\n[97] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. Resshift: Efficient diffusion model\\nfor image super-resolution by residual shifting. Advances in Neural Information Processing\\nSystems , 36, 2024.\\n15', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 14}),\n",
       " Document(page_content='[98] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and\\nMing-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In\\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\\n5728–5739, 2022.\\n[99] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a practical degradation\\nmodel for deep blind image super-resolution. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision , pages 4791–4800, 2021.\\n[100] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image\\ndiffusion models. In Proceedings of the IEEE/CVF International Conference on Computer\\nVision , pages 3836–3847, 2023.\\n[101] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network\\nfor image super-resolution. In Proceedings of the IEEE conference on computer vision and\\npattern recognition , pages 2472–2481, 2018.\\n[102] Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, and Chen Change Loy. Upscale-\\na-video: Temporal-consistent diffusion model for real-world video super-resolution. arXiv\\npreprint arXiv:2312.06640 , 2023.\\n[103] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable,\\nbetter results. In Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition , pages 9308–9316, 2019.\\nA Appendix / supplemental material\\nIn this supplementary material, we first provide additional details on the testing datasets and evaluation\\nmetrics. Subsequently, we present more visual comparisons of various methods.\\nA.1 Correspondences identified by cosine similarity without padding removal\\nFig. 9 shows that padding value will affect the matching severely.\\nFigure 9: The padded regions affect the matching severely.\\nA.2 Additional Application: Consistent Video Depth\\nOur zero-shot framework is applicable to any pre-trained image-based diffusion models and could\\nimprove the predicted video consistency. Therefore, we integrate our proposed zero-shot framework\\ninto a state-of-the-art latent diffusion-based monocular depth estimator: Marigold [ 35]. Fig. 10 shows\\nthat integrating our proposed framework into Marigold helps improve the temporal consistency of\\nvideo depth estimation.\\n16', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 15}),\n",
       " Document(page_content='Input\\n Marigold [35]\\n Ours\\n Input\\n Marigold [35]\\n Ours\\n Input\\n Marigold [35]\\n Ours\\nFigure 10: Integrating our proposed framework into Marigold [ 35] helps improve the temporal\\nconsistency of video depth estimation.\\n17', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 16})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "\n",
    "documents = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='DiffIR2VR-Zero: Zero-Shot Video Restoration with\\nDiffusion-based Image Restoration Models\\nChang-Han Yeh1Chin-Yang Lin1Zhixiang Wang2\\nChi-Wei Hsiao3Ting-Hsuan Chen1Yu-Lun Liu1\\n1National Yang Ming Chiao Tung University2University of Tokyo3MediaTek Inc.\\nAbstract\\nThis paper introduces a method for zero-shot video restoration using pre-trained im-\\nage restoration diffusion models. Traditional video restoration methods often need\\nretraining for different settings and struggle with limited generalization across vari-\\nous degradation types and datasets. Our approach uses a hierarchical token merging\\nstrategy for keyframes and local frames, combined with a hybrid correspondence\\nmechanism that blends optical flow and feature-based nearest neighbor matching\\n(latent merging). We show that our method not only achieves top performance in\\nzero-shot video restoration but also significantly surpasses trained models in gener-', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 0}),\n",
       " Document(page_content='alization across diverse datasets and extreme degradations (8 ×super-resolution and\\nhigh-standard deviation video denoising). We present evidence through quantitative\\nmetrics and visual comparisons on various challenging datasets. Additionally, our\\ntechnique works with any 2D restoration diffusion model, offering a versatile and\\npowerful tool for video enhancement tasks without extensive retraining. This re-\\nsearch leads to more efficient and widely applicable video restoration technologies,\\nsupporting advancements in fields that require high-quality video output. See our\\nproject page for video results: jimmycv07.github.io/DiffIR2VR_web.\\n1 Introduction\\nVideo restoration is a valuable topic that transforms low-quality video into high-quality video. It\\nusually involves video denoising, super-resolution, and deblurring. The state-of-art methods that\\nemploy convolutional neural networks (CNNs) [ 2,32,57] or transformers [ 19,49,78] trained on', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 0}),\n",
       " Document(page_content='large-scale data achieve incredible effectiveness. However, the regression-based methods often result\\nin blurry outputs without realistic details (Fig. 2(a)). Furthermore, the degradations they address are\\ntypically well-defined ( e.g., bicubic downsampling, given noise standard deviation), and models are\\noften tailored to specific degradations only. This limitation restricts their generalization capabilities,\\nas different settings often require additional paired data and retraining the model.\\nDiffusion models recently are adapted to image restorations [ 85,45] Because of their powerful\\ngenerative ability, they can hallucinate realistic details. But this ability inherently comes with high\\nrandomness. As a result, directly performing per-frame inference to process videos leads to severe\\nflickering (Fig. 2(b)). This phenomenon is even more pronounced in Latent Diffusion Models (LDM)\\nbecause the decoder will magnify the randomness.', metadata={'source': 'https://arxiv.org/pdf/2407.01519.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 vs 66\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(pages)} vs {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "hf_key = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"gpt2\"  # You can replace this with any other open-source model name\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "instruction_to_system = \"\"\"\n",
    "Given a chat history and the latest user question \n",
    "which might reference context in the chat history, formulate a standalone question \n",
    "which can be understood without the chat history. Do NOT answer the question, \n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "\n",
    "question_maker_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", instruction_to_system),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_chain = question_maker_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use three sentences maximum and keep the answer concise.\\\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, provide a summary of the context. Do not generate your answer.\\\n",
    "\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return question_chain\n",
    "    else:\n",
    "        return input[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "retriever_chain = RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever #| format_docs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    retriever_chain\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what percentage of patients have pathogenic germline gene variants?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChatPromptValue' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m chat_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m ai_msg \u001b[38;5;241m=\u001b[39m rag_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: chat_history})\n\u001b[0;32m      4\u001b[0m chat_history\u001b[38;5;241m.\u001b[39mextend([HumanMessage(content\u001b[38;5;241m=\u001b[39mquestion), ai_msg])\n\u001b[0;32m      5\u001b[0m ai_msg\n",
      "File \u001b[1;32mc:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2399\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 2399\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   2400\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2401\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[0;32m   2402\u001b[0m             patch_config(\n\u001b[0;32m   2403\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2404\u001b[0m             ),\n\u001b[0;32m   2405\u001b[0m         )\n\u001b[0;32m   2406\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3863\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3861\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m   3862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 3863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m   3864\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke,\n\u001b[0;32m   3865\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3866\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config(config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc),\n\u001b[0;32m   3867\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3868\u001b[0m     )\n\u001b[0;32m   3869\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   3871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3873\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1509\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1505\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1506\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1507\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1508\u001b[0m         Output,\n\u001b[1;32m-> 1509\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1510\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1511\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1512\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m             config,\n\u001b[0;32m   1514\u001b[0m             run_manager,\n\u001b[0;32m   1515\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1516\u001b[0m         ),\n\u001b[0;32m   1517\u001b[0m     )\n\u001b[0;32m   1518\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1519\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\langchain_core\\runnables\\config.py:365\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    364\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3737\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3735\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[0;32m   3736\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3737\u001b[0m     output \u001b[38;5;241m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m   3738\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   3739\u001b[0m     )\n\u001b[0;32m   3740\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[0;32m   3741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[1;32mc:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\langchain_core\\runnables\\config.py:365\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    364\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1305\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1303\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1305\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m   1306\u001b[0m     input_ids,\n\u001b[0;32m   1307\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1308\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1309\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1310\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1311\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1312\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1313\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1314\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1315\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1316\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1317\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1318\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1319\u001b[0m )\n\u001b[0;32m   1320\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1005\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[1;32m-> 1005\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m   1006\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m   1007\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ChatPromptValue' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "ai_msg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
