{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publication date of Smart City Surveillance Unveiling Indian Person Attributes in Real Time: Thu, 4 Jul 2024 (continued, showing 1 of 115 entries )\n",
      "Title: Smart City Surveillance Unveiling Indian Person Attributes in Real Time\n",
      "Authors of Smart City Surveillance Unveiling Indian Person Attributes in Real Time: Shubham Kale, Shashank Sharma, Abhilash Khuntia\n",
      "\n",
      "Smart City Surveillance Unveiling Indian Person\n",
      "Attributes in Real Time\n",
      "Shubham Kale\n",
      "M.Tech CSE\n",
      "Dept. of CSE\n",
      "IIIT Delhi\n",
      "shubham23094@iiitd.ac.in\n",
      "Shashank Sharma\n",
      "M.Tech CSE\n",
      "Dept. of CSE\n",
      "IIIT Delhi\n",
      "shashank23088@iiitd.ac.in\n",
      "Abhilash Khuntia\n",
      "M.Tech CSE\n",
      "Dept. of CSE\n",
      "IIIT Delhi\n",
      "abhilash23007@iiitd.ac.in\n",
      "Abstract—This project focuses on creating a smart surveillance\n",
      "system for Indian cities that can identify and analyze people’s\n",
      "attributes in real time. Using advanced technologies like artificial\n",
      "intelligence and machine learning, the system can recognize\n",
      "attributes such as upper body color what the person is wearing,\n",
      "accessories that he or she is wearing, headgear check, etc.,\n",
      "and analyze behavior through cameras installed around the\n",
      "city. We have provided all our code for our experiments at\n",
      "https://github.com/abhilashk23/vehant-scs-par We will be contin-\n",
      "uously updating the above GitHub repo to keep up-to-date with\n",
      "the most cutting-edge work on person attribute recognition.\n",
      "I. INTRODUCTION\n",
      "In today’s rapidly developing world, ensuring the safety and\n",
      "security of citizens has become a concern for city administra-\n",
      "tors. The project ”Smart City Surveillance Unveiling Indian\n",
      "Person Attributes in Real Time” addresses this challenge\n",
      "by harnessing the power of artificial intelligence (AI) and\n",
      "machine learning (ML) to create a cutting-edge surveillance\n",
      "system. This system is tailored specifically for Indian cities,\n",
      "where diverse populations and bustling urban environments\n",
      "necessitate innovative solutions [27] [3] [13].\n",
      "The primary objective of this project is to deploy a network\n",
      "of intelligent cameras with computer vision models capable\n",
      "of not only monitoring, but also comprehensively analyzing\n",
      "individual attributes in real time. These attributes include\n",
      "but are not limited to upper body colors, clothing styles,\n",
      "accessories, and various types of headgear worn by individuals\n",
      "[30] [8]. By leveraging AI algorithms, the system can detect\n",
      "anomalies, identify suspicious behavior patterns, and provide\n",
      "timely alerts to law enforcement agencies [24] [1].\n",
      "Moreover, the project emphasizes the importance of pri-\n",
      "vacy and ethical considerations in the deployment of surveil-\n",
      "lance technologies [16]. Robust measures are implemented\n",
      "to ensure data protection, adherence to legal regulations, and\n",
      "transparency in operations [5] [21]. Public engagement and\n",
      "feedback mechanisms are also integral, fostering community\n",
      "trust and collaboration in enhancing urban safety [26] [12].\n",
      "By integrating advanced data analytics and predictive mod-\n",
      "eling, the system aims to not only mitigate security risks but\n",
      "also optimize urban planning and resource management [35]\n",
      "[33]. Insights derived from real-time surveillance data can in-\n",
      "form city planners about crowd dynamics, traffic patterns, and\n",
      "public infrastructure usage, thereby facilitating more efficient\n",
      "city operations [31] [17].\n",
      "Ultimately, ”Smart City Surveillance Unveiling Indian Per-\n",
      "son Attributes in Real Time” endeavors to set a benchmark\n",
      "for smart city initiatives, demonstrating how AI-driven tech-\n",
      "nologies can contribute to creating safer, more resilient, and\n",
      "inclusive urban environments in India [28] [29].\n",
      "II. DATASET\n",
      "The dataset provided for the VEHANT RESEARCH LAB\n",
      "challenge on ’Smart City Surveillance: Unveiling Indian Per-\n",
      "son Attributes in Real Time consists of around 600 images\n",
      "categorized under various attributes [19]. These attributes\n",
      "encompass a variety of visual features, including colors and\n",
      "types of upper and lower body clothing, length of sleeves, ac-\n",
      "cessories carried, types of footwear, poses, and views [30] [8].\n",
      "Data augmentation is crucial for computer vision tasks where\n",
      "the dataset for a particular task is very low, so in this case,\n",
      "we can perform various data augmentation techniques which\n",
      "can help to upsample the dataset, thus building a more robust\n",
      "model [18] [22] [32]. For the person attribute recognition task,\n",
      "we have used different augmentation techniques, which will\n",
      "help in up-sampling the dataset [40] [25].. The techniques used\n",
      "were:\n",
      "Parameter\n",
      "Value\n",
      "Rotation Range\n",
      "±25 degrees\n",
      "Width Shift Range\n",
      "±15% of the total width\n",
      "Height Shift Range\n",
      "±15% of the total height\n",
      "Shear Range\n",
      "0.5 intensity\n",
      "Zoom Range\n",
      "±50%\n",
      "Horizontal Flip\n",
      "Random horizontal flip\n",
      "Fill Mode\n",
      "’nearest’\n",
      "TABLE I\n",
      "DATA AUGMENTATION PARAMETERS\n",
      "III. METHODOLOGY\n",
      "In our experiment, we used data augmentation techniques to\n",
      "expand our dataset. Initially, we had 600 images. Each image\n",
      "was augmented 12 times, considerably increasing the number\n",
      "of samples for training our model [32] [25].\n",
      "arXiv:2407.03305v1  [cs.CV]  3 Jul 2024\n",
      "Data augmentation is a crucial strategy that is used to\n",
      "increase the diversity of our training data set without collect-\n",
      "ing new data. By applying transformations such as rotation,\n",
      "scaling, and flipping, we created new images from the original\n",
      "set, thus enhancing the robustness of our model [38] [37].\n",
      "Fig. 1. Model Architecture and flow of the project\n",
      "To ensure our model was evaluated effectively, we split the\n",
      "augmented dataset into training and validation sets. We applied\n",
      "an 80-20 split, where 20% of the data was used for validation.\n",
      "This means that out of the total number of augmented images,\n",
      "80% was used to train the model, and 20% was used to validate\n",
      "it [7].\n",
      "A. First Approach :\n",
      "For person attribute recognition, we used the BEiT (Bidi-\n",
      "rectional Encoder representation from Image Transformers)\n",
      "model optimized for person attribute recognition [2]. The train-\n",
      "ing pipeline included data preparation, model initialization,\n",
      "training, and validation. Augmented images and their labels\n",
      "were loaded using PyTorch DataLoader for efficient batch\n",
      "processing [23]. The BEiT model and image processor were\n",
      "initialized with the HuggingFace transformers library\n",
      "[36]. Training was conducted for fifteen epochs with the Adam\n",
      "optimizer at a learning rate of 1 × 10−5, using Binary Cross\n",
      "Entropy with Logits Loss [14]. A custom callback calculated\n",
      "label-based mean accuracy (mA) at each epoch’s end. The\n",
      "model’s performance was evaluated on the validation set, and\n",
      "the model with the lowest validation loss was retained for\n",
      "further testing. figure 1 shows the flow of the project.\n",
      "B. Second Approach :\n",
      "In our second approach, we leveraged cutting-edge deep\n",
      "learning algorithms to classify images using the Swin Trans-\n",
      "former architecture [20]. We used the same augmentation\n",
      "techniques as in our previous strategy, and we used PyTorch’s\n",
      "Dataset and DataLoader utilities to manage data loading with a\n",
      "custom ImageDataset class [23]. The Swin Transformer, which\n",
      "is well-known for its state-of-the-art performance, was chosen\n",
      "and customised for our goal using the pre-trained SwinForIm-\n",
      "ageClassification model from the transformers library, which\n",
      "was trained on ImageNet [6] [15]. The model’s compatibility\n",
      "was ensured by using AutoImageProcessor for preprocessing.\n",
      "Each of the 15 epochs used for training included separate steps\n",
      "for validation and training. After inputting the images into the\n",
      "GPU for training, predictions were generated, and the loss\n",
      "was computed using BCEWithLogitsLoss [7]. Subsequently,\n",
      "the model parameters were fine-tuned using the Adam op-\n",
      "timizer [14]. The training and validation processes involved\n",
      "monitoring two key performance indicators (KPIs): accuracy\n",
      "and loss [4].\n",
      "C. Challenges with Initial Approaches\n",
      "1) Overfitting: The first two approaches to person attribute\n",
      "recognition faced significant overfitting issues. Overfitting\n",
      "occurs when a model performs well on the training data but\n",
      "poorly on unseen validation or test data [10]. This typically\n",
      "happens when the model learns to memorize the training data\n",
      "rather than generalizing from it. Overfitting can be mitigated\n",
      "by using techniques such as data augmentation, regularization\n",
      "(e.g., dropout), and cross-validation [34].\n",
      "2) Computationally Intensive: The initial models were also\n",
      "computationally intensive, which means they required a sig-\n",
      "nificant amount of computational resources (GPU) and time\n",
      "to train [4]. This can be due to various factors, such as:\n",
      "• Large model size with many parameters [7].\n",
      "• Inefficient data loading and preprocessing [23].\n",
      "To address these issues, we went through third approach below\n",
      "[9].\n",
      "D. Third Approach :\n",
      "In the initial approach, we faced issues with class imbalance\n",
      "and overfitting. To overcome these, we include the Scaled-\n",
      "BCELoss and FeatClassifier in this approach. The ”Scaled-\n",
      "BCELoss” fixes class imbalance by adjusting the weights\n",
      "of different attribute classes. This makes sure that learning\n",
      "from both common and rare attributes works well, which\n",
      "leads to better generalization and performance on data that\n",
      "has not been seen before [39]. The FeatClassifier combines\n",
      "a pre-trained ResNet50 backbone with a custom classifier\n",
      "head, leveraging robust feature extraction and tailored attribute\n",
      "mapping. This enhances model accuracy and efficiency, and\n",
      "dropout regularization prevents overfitting, resulting in a more\n",
      "reliable performance on person attribute recognition model\n",
      "[11] [34].\n",
      "1. ScaledBCELoss\n",
      "The ScaledBCELoss class implements a custom binary\n",
      "cross-entropy loss function that scales the logits based on the\n",
      "frequency of each class in the dataset. This helps in balancing\n",
      "the contribution of frequent and infrequent person attributes\n",
      "to the loss, which is particularly useful in cases of class\n",
      "imbalance [39].\n",
      "2. FeatClassifier\n",
      "The FeatClassifier class combines a feature extractor back-\n",
      "bone (in this case, a ResNet50 model pre-trained on ImageNet)\n",
      "with a custom classifier head. The backbone extracts high-\n",
      "level features from the input images, and the classifier head\n",
      "(a fully connected layer) maps these features to the output\n",
      "classes (person attributes) [11].\n",
      "3. Training and Evaluation\n",
      "The train and evaluate function orchestrates the entire\n",
      "training and evaluation process. It includes:\n",
      "• Data loading and transformation using DataLoader [23].\n",
      "• Model initialization with a pre-trained ResNet50 back-\n",
      "bone [11].\n",
      "• Loading pre-trained weights for fine-tuning [7].\n",
      "• Defining the optimizer and learning rate scheduler [14].\n",
      "• Training loop with model evaluation on the validation set\n",
      "[10].\n",
      "• Saving the best model based on validation accuracy [4].\n",
      "IV. CHALLENGES\n",
      "In the Smart City Surveillance project, designed to unveil In-\n",
      "dian person attributes in real time, we faced several significant\n",
      "challenges that affected the model’s performance. A primary\n",
      "issue was the scarcity of training images, which constrained\n",
      "the model’s ability to learn and generalize effectively across\n",
      "various attributes. This limitation was particularly evident in\n",
      "the model’s difficulty in recognizing specific attributes such as\n",
      "shoes and items carried by individuals. The inadequate dataset\n",
      "for these attributes meant the model struggled to identify and\n",
      "categorize them accurately.\n",
      "Moreover, there was a noticeable performance discrep-\n",
      "ancy between the static model and its real-time counterpart.\n",
      "While the static model performed admirably under controlled\n",
      "conditions, its accuracy and reliability significantly dropped\n",
      "when deployed in real-time scenarios. This gap highlighted\n",
      "the model’s struggle to adapt to the dynamic nature of real-\n",
      "time surveillance, which involves constantly changing lighting\n",
      "conditions, occlusions where parts of a person may be blocked\n",
      "from view, and rapid movements. These real-world complex-\n",
      "ities presented substantial challenges that the model was not\n",
      "fully equipped to handle.\n",
      "The deployment phase also revealed that the model robust-\n",
      "ness needed improvement to achieve consistent performance\n",
      "in real-time applications. This includes refining the model\n",
      "architecture, enhancing its ability to process and analyze\n",
      "live video feeds, and implementing strategies to handle the\n",
      "variability and unpredictability of real-world environments.\n",
      "In summary, while the project has made significant progress\n",
      "in developing a surveillance system capable of identifying\n",
      "person attributes in real time, it faces ongoing challenges that\n",
      "require further research and development. Addressing these\n",
      "challenges through data augmentation, advanced techniques,\n",
      "and model refinement will be key to achieving a more accurate\n",
      "and reliable real-time surveillance system.\n",
      "V. SCOPE OF IMPROVEMENT\n",
      "1) Dataset Expansion and Diversification:\n",
      "• Increase Quantity and Diversity: Acquiring a larger\n",
      "and more diverse set of training images is crucial. This\n",
      "includes capturing various scenarios, lighting condi-\n",
      "tions, and different types of footwear and carried items.\n",
      "A diverse dataset will help the model generalize better\n",
      "and improve accuracy across various attributes.\n",
      "• Synthetic Data Generation: Using techniques such\n",
      "as data augmentation and synthetic data generation\n",
      "can help simulate different scenarios and augment the\n",
      "existing dataset. This approach can provide the model\n",
      "with more examples to learn from without the need for\n",
      "extensive manual data collection.\n",
      "2) Advanced Machine Learning Techniques:\n",
      "• Transfer Learning: Implementing transfer learning\n",
      "can enhance the model’s performance by leveraging\n",
      "pre-trained models on large, diverse datasets. This can\n",
      "help the model learn more effectively from the limited\n",
      "data available and improve its ability to recognize\n",
      "specific attributes.\n",
      "• Fine-Tuning: Continuously fine-tuning the model with\n",
      "new data and incorporating feedback from real-time\n",
      "deployments can help adapt the model to changing\n",
      "conditions and improve its accuracy over time.\n",
      "3) Model Architecture and Optimization:\n",
      "• Improved Architecture: Experimenting with\n",
      "different model architectures and incorporating\n",
      "state-of-the-art techniques can enhance the model’s\n",
      "robustness and accuracy. Techniques like attention\n",
      "mechanisms and convolutional neural networks\n",
      "(CNNs) can be explored to better capture and\n",
      "recognize fine-grained details.\n",
      "• Optimization for Real-Time Performance:\n",
      "Optimizing the model for real-time performance\n",
      "involves reducing latency and improving\n",
      "computational efficiency. Techniques such as model\n",
      "pruning, quantization, and using efficient neural\n",
      "network architectures can help achieve faster and\n",
      "more reliable real-time processing.\n",
      "4) Real-Time Adaptability and Robustness:\n",
      "• Handling Variability: Developing algorithms that\n",
      "can handle variability in real-time conditions, such as\n",
      "changes in lighting, occlusions, and rapid movements,\n",
      "is essential. This may involve incorporating adaptive\n",
      "algorithms that can dynamically adjust to changing\n",
      "environments.\n",
      "• Continuous Learning: Implementing continuous\n",
      "learning frameworks where the model can learn from\n",
      "new data and experiences in real-time can help\n",
      "improve its adaptability and robustness. This includes\n",
      "using techniques like online learning and\n",
      "reinforcement learning.\n",
      "5) Integration with Additional Sensors and Data Sources:\n",
      "• Multi-Modal Data Integration: Integrating data\n",
      "from additional sensors, such as depth cameras,\n",
      "thermal cameras, and audio sensors, can provide\n",
      "complementary information that enhances the model’s\n",
      "ability to recognize and understand person attributes.\n",
      "• Contextual Information: Incorporating contextual\n",
      "information, such as location data, time of day, and\n",
      "historical patterns, can help improve the model’s\n",
      "accuracy and provide more meaningful insights.\n",
      "6) User Interface and Experience:\n",
      "• Enhanced GUI: Improving the graphical user\n",
      "interface (GUI) to be more intuitive and user-friendly\n",
      "can facilitate easier interaction with the system.\n",
      "Features like real-time alerts, detailed analytics, and\n",
      "customization options can enhance the user\n",
      "experience.\n",
      "• Feedback Mechanisms: Implementing feedback\n",
      "mechanisms where users can provide input on the\n",
      "model’s performance and flag inaccuracies can help\n",
      "continuously refine and improve the model.\n",
      "VI. EXPERIMENTAL RESULTS\n",
      "Sr.No\n",
      "Model\n",
      "mA Val\n",
      "Train Loss\n",
      "Val Loss\n",
      "Epoch\n",
      "1\n",
      "Model 1\n",
      "0.91\n",
      "0.008\n",
      "0.32\n",
      "15\n",
      "2\n",
      "Model 2\n",
      "0.91\n",
      "0.003\n",
      "0.38\n",
      "15\n",
      "3\n",
      "Model 3\n",
      "0.86\n",
      "0.14\n",
      "0.17\n",
      "15\n",
      "TABLE II\n",
      "COMPARISON OF DIFFERENT MODELS\n",
      "VII. RESOURCE UTILIZATION\n",
      "Model\n",
      "Computation Time (seconds)\n",
      "Device Type\n",
      "Model 1\n",
      "52214.74\n",
      "GPU P100\n",
      "Model 2\n",
      "52327.79\n",
      "GPU P100\n",
      "Model 3\n",
      "1020.65\n",
      "GPU P100\n",
      "TABLE III\n",
      "RESOURCE UTILIZATION BY DIFFERENT MODELS\n",
      "VIII. VISUALIZATION\n",
      "Fig. 2. Train & Validation Loss Vs Epoch model 1\n",
      "Fig. 3. Train & Validation Loss Vs Epoch model 2\n",
      "Fig. 4. Train & Validation Loss Vs Epoch model 3\n",
      "IX. FUTURE WORK\n",
      "In the landscape of urban development, smart city surveil-\n",
      "lance stands poised at the forefront of innovation, promising\n",
      "transformative advancements in safety, efficiency, and com-\n",
      "munity participation. As cities embrace interconnected tech-\n",
      "nologies and data-driven solutions, the future of work within\n",
      "smart city surveillance unfolds with distinct implications and\n",
      "opportunities.\n",
      "• Public Health Monitoring:\n",
      "– Epidemic Surveillance: In public areas, surveillance\n",
      "devices can keep an eye on things like body tem-\n",
      "perature, movement patterns, and crowd density. This\n",
      "data can be analyzed to detect early signs of disease\n",
      "outbreaks, allowing for prompt public health responses\n",
      "and containment measures.\n",
      "– Behavioral Analysis: Authorities can act swiftly and\n",
      "efficiently by recognising odd behaviours or trends that\n",
      "may point to emergencies or health threats by analysing\n",
      "data from monitoring systems.\n",
      "• Community Engagement:\n",
      "– Transparency: In order to interact with the public, cities\n",
      "should be transparent about the data that is gathered,\n",
      "how it is used, and how surveillance technologies work.\n",
      "Policies that are transparent foster confidence and\n",
      "reassure the public about the appropriate application\n",
      "of surveillance technologies.\n",
      "– Education and Participation: Community support and\n",
      "cooperation are fostered by informing the public about\n",
      "the advantages of smart city surveillance, such as\n",
      "increased safety and better urban services, and by\n",
      "offering channels for public feedback and involvement\n",
      "in decision-making processes.\n",
      "• Crisis Response and Management:\n",
      "– Social Media Integration: By combining social media\n",
      "analytics with surveillance data, authorities can better\n",
      "assess public opinion during emergencies and respond\n",
      "to community needs.\n",
      "– Drone Surveillance: Deploying drones with surveil-\n",
      "lance capabilities to quickly analyze the situation from\n",
      "the air during crises like fires, natural catastrophes, or\n",
      "search and rescue missions.\n",
      "X. MODEL DEPLOYMENT\n",
      "We have created a GUI using the model in our third\n",
      "approach -\n",
      "1) Live camera prediction: Created a GUI using Tkinter\n",
      "which uses the device cam to give live predictions for\n",
      "the labels Figure 5 & 6. The demo is shown below. The\n",
      "image shown in figure 7 depicts our setup Live demo\n",
      "link..\n",
      "Fig. 5. Live Prediction 1\n",
      "Fig. 6. Live Prediction 2\n",
      "2) Static Prediction: We deployed our model on Hugging\n",
      "Face and developed a GUI that enables users to upload\n",
      "Fig. 7. Our setup for live prediction\n",
      "images and receive predictions from our model in figure\n",
      "8. Here is the demo link.\n",
      "Fig. 8. Static Prediction 1\n",
      "XI. CONCLUSION\n",
      "In this study, we developed a comprehensive pipeline\n",
      "for person attribute recognition, evaluating three distinct ap-\n",
      "proaches: BEiT, SWIN, and FeatClassifier. Our initial ex-\n",
      "periments with the BEiT model incorporated advanced data\n",
      "augmentation techniques and a novel ScaledBCELoss function\n",
      "to address class imbalance. We then explored the SWIN model,\n",
      "a state-of-the-art architecture renowned for its performance\n",
      "in various vision tasks. However, the FeatClassifier, which\n",
      "integrates a pre-trained ResNet50 backbone with a custom\n",
      "classifier head, emerged as the most effective model. Its\n",
      "superior performance can be attributed to dropout regulariza-\n",
      "tion that successfully mitigates overfitting and its pre-training\n",
      "on the RAPv2 dataset, which is specifically comprised of\n",
      "pedestrian images. In contrast, the BEiT and SWIN models\n",
      "were pre-trained on the ImageNet dataset, which contains a\n",
      "diverse range of images.\n",
      "Our experimental results validate the FeatClassifier ap-\n",
      "proach, demonstrating its strong potential for real-world ap-\n",
      "plications in person attribute recognition. Moving forward, we\n",
      "are excited about further enhancing our pipeline. Future work\n",
      "will involve implementing more sophisticated augmentation\n",
      "strategies, experimenting with various backbone architectures,\n",
      "and extending our model to recognize a wider array of\n",
      "attributes. Additionally, we plan to test our approach on larger\n",
      "and more diverse datasets to further confirm its robustness\n",
      "and scalability. We are committed to advancing this project\n",
      "and look forward to achieving even greater milestones in the\n",
      "field of person attribute recognition.\n",
      "REFERENCES\n",
      "[1] S Agarwal. Anomaly detection in surveillance. Security Journal, pages\n",
      "98–108, 2020.\n",
      "[2] H Bao, L Dong, and F Wei. Beit: Bert pre-training of image transform-\n",
      "ers. arXiv preprint arXiv:2106.08254, pages 1–13, 2021.\n",
      "[3] R Bhargava and S Jain. Ml techniques in urban planning. Urban Science,\n",
      "pages 112–121, 2019.\n",
      "[4] J Brownlee. Accuracy and performance in deep learning. Deep Learning\n",
      "Journal, pages 15–25, 2019.\n",
      "[5] S Das and V Rao. Legal regulations in ai surveillance. Law Journal,\n",
      "pages 40–50, 2021.\n",
      "[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-\n",
      "Fei. Imagenet: A large-scale hierarchical image database. 2009 IEEE\n",
      "Conference on Computer Vision and Pattern Recognition, pages 248–\n",
      "255, 2009.\n",
      "[7] I Goodfellow, Y Bengio, and A Courville. Deep learning. MIT Press,\n",
      "pages 1–775, 2016.\n",
      "[8] N Gupta and A Patel. Use of accessories in urban areas. Accessory\n",
      "Review, pages 23–34, 2019.\n",
      "[9] Song Han, Jeff Pool, John Tran, and William Dally.\n",
      "Deep learning.\n",
      "arXiv preprint arXiv:1506.02626, pages 1–10, 2015.\n",
      "[10] T Hastie, R Tibshirani, and J Friedman.\n",
      "The elements of statistical\n",
      "learning. Springer, pages 1–745, 2009.\n",
      "[11] K He, X Zhang, S Ren, and J Sun. Deep residual learning for image\n",
      "recognition. Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pages 770–778, 2016.\n",
      "[12] A Jain. Community trust in ai. Community Journal, pages 34–45, 2019.\n",
      "[13] M Khan. Diversity in indian cities. Cultural Studies, pages 15–26, 2021.\n",
      "[14] DP Kingma and J Ba. Adam: A method for stochastic optimization.\n",
      "arXiv preprint arXiv:1412.6980, pages 1–15, 2014.\n",
      "[15] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\n",
      "Imagenet\n",
      "classification with deep convolutional neural networks.\n",
      "Advances in\n",
      "neural information processing systems, 25:1097–1105, 2012.\n",
      "[16] P Kulkarni. Privacy concerns in ai. AI Ethics, pages 12–23, 2022.\n",
      "[17] A Kumar and D Patel. Traffic patterns in cities. Traffic Journal, pages\n",
      "45–55, 2019.\n",
      "[18] R Kumar and P Mehta. Image data augmentation approaches. Journal\n",
      "of Computer Vision, pages 78–89, 2023.\n",
      "[19] Vehant Research Lab. Vehant research lab challenge dataset. Research\n",
      "Lab Data, pages 10–20, 2021.\n",
      "[20] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,\n",
      "Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision\n",
      "transformer using shifted windows.\n",
      "Proceedings of the IEEE/CVF\n",
      "International Conference on Computer Vision (ICCV), pages 10012–\n",
      "10022, 2021.\n",
      "[21] K Mehta and P Shah. Transparency in ai systems. Tech Ethics, pages\n",
      "30–42, 2021.\n",
      "[22] S Mir. Data augmentation techniques for computer vision. Journal of\n",
      "AI Research, pages 123–134, 2022.\n",
      "[23] A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen,\n",
      "Z Lin, N Gimelshein, L Antiga, et al. Pytorch: An imperative style,\n",
      "high-performance deep learning library. Advances in Neural Information\n",
      "Processing Systems, pages 1–12, 2019.\n",
      "[24] V Patil and P Deshmukh. Surveillance in smart cities. Surveillance\n",
      "Review, pages 78–88, 2021.\n",
      "[25] L Perez and J Wang.\n",
      "The effectiveness of data augmentation in\n",
      "image classification using deep learning. Journal of Machine Learning\n",
      "Research, pages 1–20, 2017.\n",
      "[26] R Ramesh and A Verma. Public engagement in smart cities. Urban\n",
      "Affairs, pages 78–89, 2020.\n",
      "[27] A Rao and M Srivastava.\n",
      "Ai in smart cities.\n",
      "Journal of Urban\n",
      "Technology, pages 35–45, 2018.\n",
      "[28] B Reddy. Ai in urban development. Urban AI, pages 33–44, 2021.\n",
      "[29] M Shah and K Desai. Smart city technologies. Smart City Journal,\n",
      "pages 15–25, 2022.\n",
      "[30] A Sharma and R Gupta. Fashion trends in urban india. Fashion Journal,\n",
      "pages 50–60, 2020.\n",
      "[31] N Sharma.\n",
      "Crowd dynamics in urban areas.\n",
      "Crowd Science, pages\n",
      "20–30, 2018.\n",
      "[32] C Shorten and TM Khoshgoftaar. A survey on image data augmentation\n",
      "for deep learning. Journal of Big Data, pages 1–48, 2019.\n",
      "[33] P Singh. Urban resource management. Resource Management, pages\n",
      "55–65, 2020.\n",
      "[34] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever,\n",
      "and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural\n",
      "networks from overfitting. Journal of Machine Learning Research, pages\n",
      "1929–1958, 2014.\n",
      "[35] R Venkat. Data analytics in urban planning. Analytics Journal, pages\n",
      "89–99, 2022.\n",
      "[36] T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac,\n",
      "T Rault, R Louf, M Funtowicz, et al.\n",
      "Transformers: State-of-the-art\n",
      "natural language processing. Proceedings of the 2020 Conference on\n",
      "Empirical Methods in Natural Language Processing: System Demon-\n",
      "strations, pages 38–45, 2020.\n",
      "[37] S Yun, D Han, S Oh, S Chun, J Choe, and Y Yoo. Cutmix: Regu-\n",
      "larization strategy to train strong classifiers with localizable features.\n",
      "International Conference on Computer Vision, pages 1–10, 2019.\n",
      "[38] H Zhang, M Cisse, Y Dauphin, and D Lopez-Paz.\n",
      "Mixup: Beyond\n",
      "empirical risk minimization.\n",
      "International Conference on Learning\n",
      "Representations, pages 1–13, 2018.\n",
      "[39] Z Zhang.\n",
      "Imbalanced classification of images.\n",
      "Journal of Machine\n",
      "Learning Research, pages 45–60, 2018.\n",
      "[40] Y Zhu and L Wang. Data augmentation for object detection. IEEE\n",
      "Transactions on Image Processing, pages 10–20, 2020.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# URL of the webpage containing PDF links\n",
    "webpage_url = 'https://arxiv.org/list/cs.CV/recent?skip=2&show=1'# i just giving only one paper to extract we can alter show=\n",
    "\n",
    "# Fetch webpage content\n",
    "response = requests.get(webpage_url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract the publication date\n",
    "date_element = soup.find('h3')\n",
    "publication_date = date_element.get_text(strip=True) if date_element else 'Unknown date'\n",
    "\n",
    "# Extract all titles, authors, and PDF URLs\n",
    "base_url = 'https://arxiv.org'\n",
    "papers = []\n",
    "\n",
    "# Find all entries\n",
    "entries = soup.find_all('dl')\n",
    "\n",
    "for entry in entries:\n",
    "    titles = entry.find_all('div', class_='list-title mathjax')\n",
    "    authors = entry.find_all('div', class_='list-authors')\n",
    "    pdf_links = entry.find_all('a', title='Download PDF')\n",
    "\n",
    "    for title, author, pdf_link in zip(titles, authors, pdf_links):\n",
    "        paper_title = title.get_text(strip=True).replace('Title:', '').strip()\n",
    "        paper_authors = [a.get_text(strip=True) for a in author.find_all('a')]\n",
    "        paper_pdf_url = base_url + pdf_link['href']\n",
    "        papers.append({\n",
    "            'title': paper_title,\n",
    "            'authors': paper_authors,\n",
    "            'pdf_url': paper_pdf_url,\n",
    "            'date': publication_date\n",
    "        })\n",
    "\n",
    "def extract_text_from_pdf(paper):\n",
    "    text = f\"publication date of {paper['title']}: {paper['date']}\\nTitle: {paper['title']}\\nAuthors of {paper['title']}: {', '.join(paper['authors'])}\\n\\n\"\n",
    "    try:\n",
    "        # Download PDF or directly process it\n",
    "        response = requests.get(paper['pdf_url'], stream=True)\n",
    "        document = fitz.open(stream=response.content, filetype=\"pdf\")\n",
    "\n",
    "        for page_num in range(len(document)):\n",
    "            page = document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "\n",
    "    except Exception as e:\n",
    "        text += f\"Error processing PDF at {paper['pdf_url']}: {e}\"\n",
    "\n",
    "    return text\n",
    "\n",
    "# Extract text from each PDF and print it along with title, authors, and date\n",
    "for paper in papers:\n",
    "    extracted_text = extract_text_from_pdf(paper)\n",
    "    print(extracted_text)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "extracted_texts=[extracted_text]\n",
    "# Split text into manageable chunks/documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    \n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=200, # Adjust overlap size based on requirements\n",
    ")\n",
    "\n",
    "documents = []\n",
    "for text in extracted_texts:\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    documents.extend(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "# Initialize the embeddings model from HuggingFace\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "# Create a FAISS vector store from documents and embeddings\n",
    "vector = FAISS.from_texts(documents, embeddings)\n",
    "# Create a retriever from the vector store\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "# Initialize the Ollama language model\n",
    "llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# Initialize the output parser for string outputs\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# Define the system instruction for reformulating the user's question\n",
    "\n",
    "instruction_to_system = \"\"\"\n",
    "Given a chat history and the latest user question \n",
    "which might reference context in the chat history, formulate a standalone question \n",
    "which can be understood without the chat history. Do NOT answer the question, \n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "# Create a prompt template for reformulating questions\n",
    "question_maker_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", instruction_to_system),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Define a chain that reformulates the question if needed\n",
    "question_chain = question_maker_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine if question needs reformulation based on chat history\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return question_chain\n",
    "    else:\n",
    "        return input[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# Create a retriever chain to fetch relevant context for the question\n",
    "retriever_chain = RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever #| format_docs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the question-answering assistant\n",
    "qa_system_prompt =  \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer,dont hallicuniate i need concise answer . Do not generate your answer.\\\n",
    "{context}\"\"\"\n",
    "\n",
    "# Create a prompt template for the question-answering task\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Retrieval-Augmented Generation (RAG) chain\n",
    "rag_chain = (\n",
    "    retriever_chain\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it appears that the paper discusses the concept of smart city surveillance and its various aspects, including community engagement, crisis response, public health monitoring, and transformative advancements in safety, efficiency, and community participation. The paper emphasizes the importance of transparency, education, and participation in implementing smart city surveillance systems, as well as the use of advanced data analytics and predictive modeling to optimize urban planning and resource management.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = \"breif the paper what it is all about ??  \"\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "#Invoke the RAG chain with the question and chat history, and update chat history with responses\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "###code to automate the scraping process everyday\n",
    "#import schedule\n",
    "#import time\n",
    "#import subprocess\n",
    "\n",
    "#def job():\n",
    "#    subprocess.run([\"python\", \"C:\\\\Path\\\\To\\\\YourScript\\\\scrape.py\"])\n",
    "\n",
    "#schedule.every().day.at(\"12:00\").do(job)\n",
    "\n",
    "#while True:\n",
    "#    schedule.run_pending()\n",
    "#    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Function to handle the chat interaction\n",
    "def chat_complete(message, state):\n",
    "    if state is None:\n",
    "        state = []\n",
    "    ai_msg = rag_chain.invoke({\"question\": message, \"chat_history\": state})\n",
    "    state.append({\"role\": \"user\", \"content\": message})\n",
    "    state.append({\"role\": \"assistant\", \"content\": ai_msg})\n",
    "    response = [(msg[\"content\"], state[i+1][\"content\"]) for i, msg in enumerate(state) if msg[\"role\"] == \"user\"]\n",
    "    return response, state\n",
    "\n",
    "# Define the Gradio interface\n",
    "with gr.Blocks() as block:\n",
    "    gr.Markdown(\"\"\"<h1><center> EduVisionBot </center></h1>\"\"\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    message = gr.Textbox(placeholder=\"Type your Message.........\")\n",
    "    state = gr.State([])\n",
    "    submit = gr.Button(\"SEND\")\n",
    "    \n",
    "    submit.click(chat_complete, inputs=[message, state], outputs=[chatbot, state])\n",
    "block.launch(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
